{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Programming\n",
    "## Introduction\n",
    "\n",
    "So far you've been doing all your calculations by hand. As you have probably learned, this is timeconsuming and error prone. In this minicourse, we are going to automate some of this labour. The framework of defining a probabilistic model and automatically inferring variables of interest is called _Probabilistic Programming_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with probabilistic models and Bayesian inference in general, is that one cannot perform certain integrations required to form the posterior distribution. To avoid this, we can employ approximate Bayesian inference.\n",
    "\n",
    "Within probabilistic programming, there are two schools of thought: Monte Carlo Sampling and Variational Inference. We will briefly discuss both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Sampling\n",
    "\n",
    "We are often interested in expected values of posterior distributions. This need not be the mean, but can also be the expected value with respect to some function $g$ applied to the data: $\\int g(\\theta) p(\\theta | x) d\\theta$. However, in most cases, one cannot integrate with respect to the posterior distribution. Monte Carlo sampling is a numerical approximation method. It consists of drawing samples from the posterior distribution and approximate the expected value with the sample average. This is motivated by the [Law of Large Numbers](https://www.statlect.com/asymptotic-theory/law-of-large-numbers), stating that sample averages converge to expected values as the sample size goes to infinity.\n",
    "\n",
    "Let's start with an example. Say that I would like to know the expected value of the following distribution: \n",
    "\n",
    "$$\\mathcal{N}(x \\mid 1, 1)$$. \n",
    "\n",
    "This is, of course, the Gaussian distribution and we know its expected value analytically: $\\int x \\mathcal{N}(x \\mid \\mu, 1) dx = \\mu$. However, we are going to approximate it anyway, just to show how approximation via MC sampling works. So, I draw 10 samples from the Gaussian distribution and approximate the expected value via the sample average:\n",
    "\n",
    "$$ \\int x p(x | \\theta) dx \\approx \\frac{1}{n} \\sum_{i=1}^n x_i \\quad \\text{for} \\ x_i \\sim p(x | \\theta)$$\n",
    "\n",
    "I can draw samples from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.619867910862402"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using StatsPlots\n",
    "using Distributions\n",
    "\n",
    "# Data points\n",
    "X = [1. 0.9 1.12 0.89]\n",
    "\n",
    "# Likelihood\n",
    "px(μ) = prod([pdf.(Normal(μ,1), x) for x in X])\n",
    "\n",
    "# Draw samples from prior\n",
    "μ_ = rand(Normal(1,1), 1000)\n",
    "\n",
    "# Push drawn samples through likelihood\n",
    "lμ = [px(m) for m in μ_]\n",
    "\n",
    "# x = -3:.1:3\n",
    "# plot(x, lμ(x), color=\"black\", label=\"\")\n",
    "sum(lμ .* μ_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline mini-course PP\n",
    "\n",
    "We will have four blocks of 2 hours, in wich we work through the following types of models:\n",
    "\n",
    "1. Regression & Classification\n",
    "2. Mixture models\n",
    "3. Hidden Markov models\n",
    "4. Kalman filters\n",
    "\n",
    "In each block, we fit the particular probabilistic model using either of the two schools of thought. This results in two notebooks per block: one using MC sampling (e.g. PP-1-sampling) and one using VI (e.g. PP-1-variational). The headers and data generation are equal in both notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Critiquing & Improvement\n",
    "It is important to continuously critique and improve your model design. This is daily practice for most data scientists and machine learning engineers. How to critique models is a skill often expected to be obtained through experience; practice makes perfect. However, there are quite a few heuristics that can serve as useful tools in your toolbelt. Today we will be going over a few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Materials\n",
    "\n",
    "#### Reading\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_programming)\n",
    "- [Variational inference: a review for statisticians (Blei et al., 2018)](https://arxiv.org/pdf/1601.00670.pdf)\n",
    "\n",
    "#### Videos\n",
    "- [Intro to programming in Julia](https://youtu.be/8h8rQyEpiZA?t=233).\n",
    "\n",
    "#### Software\n",
    "- Multi-language\n",
    "    - [Stan](https://mc-stan.org/)\n",
    "- Julia\n",
    "    - [Turing.jl](https://turing.ml/dev/tutorials/0-introduction/)\n",
    "    - [ForneyLab.jl](https://biaslab.github.io/forneylab/docs/getting-started/)\n",
    "- Python\n",
    "    - [Pyro](http://pyro.ai/)\n",
    "    - [TensorFlow Probability](https://www.tensorflow.org/probability/)\n",
    "- MATLAB\n",
    "    - [Stat & ML Toolbox](https://nl.mathworks.com/products/statistics.html)\n",
    "    - [dimple](https://github.com/analog-garage/dimple)\n",
    "- .NET\n",
    "    - [Infer.NET](https://www.microsoft.com/en-us/research/project/infernet/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
