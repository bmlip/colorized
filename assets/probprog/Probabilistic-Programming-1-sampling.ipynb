{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Programming - 1: Regression & Classification\n",
    "## Monte Carlo sampling\n",
    "\n",
    "In this lesson we are going to use Monte Carlo sampling for regression & classification. We will use Turing.jl to specify a model and run a sampling procedure. While Probabilistic Programming requires some specialised knowledge in terms of probability theory and Bayesian statistics, implementing an inference procedure is straightforward once you have the right tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Learn to estimate regression parameters using Monte Carlo sampling.\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes.\n",
    "    - [Intro to programming in Julia](https://youtu.be/8h8rQyEpiZA?t=233).\n",
    "  - Optional\n",
    "    - Tutorials using [Turing.jl](https://turing.ml/dev/tutorials/0-introduction/)\n",
    "    - Cheatsheets: [how does Julia differ from Matlab / Python](https://docs.julialang.org/en/v1/manual/noteworthy-differences/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Turing.jl\n",
    "[Turing.jl](https://turing.ml/dev/) is a Probabilistic Programming library written in Julia. Turing relies on MC sampling based procedures to perform inference. It is easy to learn but difficult to master, since optimizing the sampling procedure for a particular problem can require quite some expertise.\n",
    "\n",
    "Things to keep in mind when using Turing:\n",
    "1. Sampling based inference always runs. This means Turing can handle a wider class of problems than variational inference.\n",
    "2. Sampling based inference is stochastic. Your results will vary between runs.\n",
    "3. Sampling based inference is slow. \n",
    "\n",
    "Now, let's proceed with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package managing\n",
    "using Pkg\n",
    "Pkg.activate(\"workspace\")\n",
    "Pkg.instantiate()\n",
    "\n",
    "using Logging; disable_logging(LogLevel(0))\n",
    "using StatsPlots\n",
    "using Random\n",
    "using Distributions\n",
    "using Turing\n",
    "using MCMCChains\n",
    "include(\"../scripts/pp-1.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "\n",
    "We'll generate a simple regression problem setting. Given a data set $x = (x_1, \\dots, x_N)$ of $N$ samples, we generate the set of observations $y = (y_1, \\dots, y_N)$ via: \n",
    "\n",
    "$$ y = f(x) + \\epsilon$$ \n",
    "\n",
    "where $f(x) = x*w_1 + w_2$ and $\\epsilon \\sim \\mathcal{N}(0, \\sigma)$. We summarize the slope coefficient and intercept into a single weight vector, $w = [w_1\\ w_2]$. We will use the name \"covariates\" for $x$ and \"responses\" for $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "N = 20\n",
    "\n",
    "# Parameters\n",
    "true_w = [4.0; 1.0]\n",
    "true_σ = 1/2\n",
    "\n",
    "function generate_data(true_w, true_σ; num_samples=10)\n",
    "    \"Generate data according to y = x⋅w + ϵ\"\n",
    "    \n",
    "    # Covariates\n",
    "    x = rand(Uniform(0,1), num_samples)\n",
    "    \n",
    "    # Linear function of covariates\n",
    "    fx = x.* true_w[1] .+ true_w[2] \n",
    "    \n",
    "    # Generate Gaussian noise\n",
    "    ϵ = rand(Normal(0, true_σ), num_samples)\n",
    "    \n",
    "    # Responses consist of the linear mapping plus noise\n",
    "    y = fx + ϵ\n",
    "    \n",
    "    # Return covariates and responses\n",
    "    return y, x\n",
    "end\n",
    "\n",
    "# Generate data\n",
    "responses, covariates = generate_data(true_w, true_σ, num_samples=N)\n",
    "\n",
    "# True regression of the covariates\n",
    "fx = covariates*true_w[1] .+ true_w[2]\n",
    "\n",
    "# Visualize data\n",
    "scatter(covariates, responses, color=\"blue\", label=\"responses\", legend=:topleft)\n",
    "plot!(covariates, fx, color=\"red\", label=\"true regression\", linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification\n",
    "\n",
    "Turing has an intuitive means of specifying a model. It follows the math quite closely, and your final model will resemble a system of equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model linear_regression(x, y, true_σ) = begin \n",
    "    \n",
    "    # Extract number of samples\n",
    "    num_samples = size(x, 1)\n",
    "    \n",
    "    # Weights prior parameters\n",
    "    μ_w = [0.5; 0.]\n",
    "    σ_w = 10*[1. 0.; 0. 1.]\n",
    "    \n",
    "    # Define a Gaussian prior distribution for the weights \n",
    "    w ~ MvNormal(μ_w, σ_w)\n",
    "\n",
    "    # Loop over data points\n",
    "    for i in 1:num_samples\n",
    "        \n",
    "        # Specify that responses are a Gaussian perturbation\n",
    "        y[i] ~ Normal(x[i]*w[1] + w[2], true_σ) \n",
    "        \n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to infer the weights. For this problem setting, we will use the No U-Turn Sampler (NUTS). As stated in the introduction notebook, don't worry too much about the details of the sampler. But if you are feeling adventurous, feel free to try out the other ones or try out different sampler parameter settings. Can you get better results? What happens to the runtime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide the progress prompt while sampling.\n",
    "Turing.turnprogress(false);\n",
    "\n",
    "# Construct a Markov chain and draw samples\n",
    "chain = Turing.sample(linear_regression(covariates, responses, true_σ), NUTS(500, 0.6), 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inference procedure has introduced a new object: the MCMCChain. It holds the samples as well as some diagnostic information to assess convergence of the sampler. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table summarizes properties of the Markov chain. A lot of the entries are not important at the moment, so we will focus on the posterior distribution over the weights $w_1$ and $w_2$. Did the sampler succesfully manage to recover the parameters of the data generating process? \n",
    "\n",
    "We can also plot the chain. That will give us a visual clue of whether it has converged and what the approximate posterior looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make predictions using the expected value of the weight posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract mean weights\n",
    "w_hat = mean_chain(chain, :w)\n",
    "\n",
    "# Make predictions\n",
    "y_hat = covariates * w_hat[1] .+ w_hat[2]\n",
    "\n",
    "# Visualize true data and observations\n",
    "scatter(covariates, responses, color=\"blue\", label=\"observations\", legend=:topleft)\n",
    "plot!(covariates, fx, color=\"red\", label=\"true regression\", linewidth=2)\n",
    "\n",
    "# Overlay predictions\n",
    "plot!(covariates, y_hat, color=\"green\", label=\"prediction\", linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. Below are a few assignments to hone your skills with Probabilistic Programming in Turing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional assignment 1: classification\n",
    "The code below generates a dataset of 0's and 1's. Your task is to turn the Turing model above into a binary classifier. The probabilistic model looks like this:\n",
    "\n",
    "$$ \\begin{align} w \\sim&\\ \\mathcal{N}(\\mathbf{\\mu_w},\\mathbf{\\sigma_w})\\\\\n",
    "y^\\prime{} \\sim&\\ \\mathcal{N}(f(x),\\sigma_y) \\\\\n",
    "p =&\\ \\sigma(y^\\prime{}) \\\\\n",
    "y \\sim&\\ \\mathcal{B}er(p)) \\end{align} $$\n",
    "\n",
    "As you can see, it is pretty similar to the linear regression covered so far. The main difference is that $y$ is now a Bernoulli distribution since the data consists of 0's and 1's. We call $x$ \"features\" now and $y$ \"labels\". The Bernoulli distribution takes a single parameter $p$ in the range [0,1]. Hence we need to squash the output of the regression $y^\\prime{}$ using a logistic function $\\sigma$. To that end we have provided a function for you to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "N = 10\n",
    "\n",
    "# Sigmoid function\n",
    "σ(x) = 1/ (1 + exp(-x))\n",
    "\n",
    "# Parameters for generating data\n",
    "true_w = [1.0; 0.5]\n",
    "true_σ = 1.\n",
    "\n",
    "function generate_binary_data(true_w, true_σ; num_samples=10)\n",
    "    \n",
    "    # Generate data\n",
    "    x = rand(Normal(0, true_σ), num_samples,)\n",
    "    \n",
    "    # Linear mapping of covariates \n",
    "    fx = [x ones(num_samples,)]*true_w\n",
    "    \n",
    "    # Observations\n",
    "    y = round.(σ.(fx))\n",
    "    \n",
    "    return y, x\n",
    "end\n",
    "\n",
    "# Binary dataset\n",
    "labels, features = generate_binary_data(true_w, true_σ, num_samples=N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model binary_classifier(x, y, n) = begin\n",
    "    \n",
    "    # Parameters for priors\n",
    "    μ_w = # YOUR CODE HERE\n",
    "    σ_w = # YOUR CODE HERE\n",
    "    σ_y = # YOUR CODE HERE\n",
    "    w ~ # YOUR CODE HERE\n",
    "\n",
    "    y_prime = Vector(undef,n)\n",
    "    for i in 1:n # Loop over data points\n",
    "        \n",
    "        y_prime[i] ~ #YOUR CODE HERE\n",
    "        y[i] ~ # YOUR CODE HERE\n",
    "        \n",
    "    end\n",
    "end\n",
    "\n",
    "chain = Turing.sample(binary_classifier(features, labels, N), NUTS(500,0.65), 5000);\n",
    "describe(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional assignment 2: Markov chain convergence\n",
    "\n",
    "[MCMCChains.jl](https://github.com/TuringLang/MCMCChains.jl) has a number of functions to help you examine your chain's convergence. Check the documentation and argue for why you believe your chain has converged (or not). Play around with the parameters of the sampler until you get a chain that keeps drifting. Why is this happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional assignment 3: Regressing real data\n",
    "Download the Titanic dataset from https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv. It contains passenger information of the Titanic. Build a classifier that predicts whether a passenger survives or not, based on the available information. You will have to do your own data wrangling to get the data set into the right shape, as well as come up with your own model specification and parameterisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia-1.3 1.3.1",
   "language": "julia",
   "name": "julia-1.3-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
