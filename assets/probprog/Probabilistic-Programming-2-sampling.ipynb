{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Programming - 2: Mixture modeling\n",
    "\n",
    "## Monte Carlo sampling\n",
    "\n",
    "In this notebook, we are exploring a more complex problem set using Monte Carlo sampling, namely mixture modeling. It will show you how to extend the sampling procedure to the multivariate case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Write a Monte Carlo sampling procedure for a 2-dimensional mixture model.\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes.\n",
    "  - Optional\n",
    "    - Tutorials using [Turing.jl](https://turing.ml/dev/tutorials/0-introduction/)\n",
    "    - Cheatsheets: [how does Julia differ from Matlab / Python](https://docs.julialang.org/en/v1/manual/noteworthy-differences/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package managing\n",
    "using Pkg\n",
    "Pkg.activate(\"workspace\")\n",
    "Pkg.instantiate()\n",
    "\n",
    "using Logging; disable_logging(LogLevel(0))\n",
    "using Distributions\n",
    "using StatsPlots\n",
    "using LaTeXStrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "Consider a multivariate data set $X = (X_1, \\dots, X_d)$ that we capture with a $K$-component Gaussian mixture model. Each component is a weighted Gaussian distribution,\n",
    "\n",
    "$$ p_k(x \\mid \\phi_k, \\mu_k, \\Sigma_k) \\triangleq \\phi_k \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) \\, ,$$\n",
    "\n",
    "where the weight $\\phi_k$ models the relative proportion of samples that belong to the $k$-th component. In total, we model $N$ samples:\n",
    "\n",
    "$$ p(X \\mid \\phi, \\mu, \\Sigma) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} p_k(x \\mid \\phi_k, \\mu_k, \\Sigma_k)^{z_i} \\, .$$\n",
    "\n",
    "Note that we summarize parameters as follows: $\\phi = (\\phi_1, \\dots, \\phi_K), \\mu = (\\mu_1, \\dots, \\mu_K), \\Sigma = (\\Sigma_1, \\dots, \\Sigma_K)$. The variable $z_i$ is the assignment of sample $i$ to class $k$ ($p(z_{i} = k)$ is also sometimes called the _responsibility_ of component $k$ for the $i$-th sample). Each $z_i$ follows a Categorical distribution:\n",
    "\n",
    "$$ p(z_i) \\sim \\text{Cat}(\\beta_i) \\, .$$\n",
    "\n",
    "Being Bayesians, we put priors on all unknown variables ($\\beta, \\phi, \\mu, \\Sigma$):\n",
    "\n",
    "$$ \\begin{align}\n",
    "p(\\beta_i) \\sim&\\ \\text{Dir}(b_0) \\quad \\forall i \\\\\n",
    "p(\\phi) \\sim&\\ \\text{Dir}(a_0) \\\\\n",
    "p(\\mu, \\Sigma) \\sim&\\ \\mathcal{NW}(m_0, l_0, W_0, v_0) \\end{align} \\, .$$\n",
    "\n",
    "The prior for $z$, $p(\\beta)$, follows a Dirichlet distribution because the elements of the vector should sum to one. The distribution for the mixture weights, $p(\\phi)$, also follows a Dirichlet distribution because it also is a vector that should sum to one. The prior distribution for the component parameters, $p(\\mu, \\Sigma)$, is a Normal-Wishart distribution.\n",
    "\n",
    "We are mostly interested in the posterior distribution over $z$, which can assign samples to clusters via $\\hat{k}_i = \\arg \\max_{k} p(z_i = k)$. For example, if the posterior distribution for $z_i$ is $[0.32\\ 0.21\\ 0.47]$, then $\\hat{k}_i$ is 3 and we say that $x_i$ probably belongs to the component 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size\n",
    "N = 40\n",
    "\n",
    "# Parameters for each cluster\n",
    "ϕ = [0.45, .55]\n",
    "μ1 = [-2., -2.]\n",
    "Σ1 = [1.0 0.0; 0.0 3.0]\n",
    "μ2 = [+2., +2.]\n",
    "Σ2 = [3.0 0.0; 0.0 1.0]\n",
    "\n",
    "function generate_data(μ_1::Array{Float64,1}, μ_2::Array{Float64,1}, Σ_1::Array{Float64,2}, Σ_2::Array{Float64,2}, ϕ::Array{Float64,1}; num_samples=10)\n",
    "    \"Generate data from a 2-dimensional mixture model\"\n",
    "    \n",
    "    # Sample sizes in each cluster\n",
    "    N1 = Int(round(num_samples * ϕ[1]))\n",
    "    N2 = num_samples - N1\n",
    "    \n",
    "    # Cluster 1\n",
    "    X1 = rand(MvNormal(μ_1, Σ_1), N1)\n",
    "    \n",
    "    # Cluster 2\n",
    "    X2 = rand(MvNormal(μ_2, Σ_2), N2)\n",
    "    \n",
    "    # Concatenate clusters\n",
    "    X = [X1'; X2']\n",
    "    \n",
    "    # Keep track of cluster membership\n",
    "    y = [ones(N1,); 2*ones(N2,)]\n",
    "    \n",
    "    return X, y\n",
    "end\n",
    "\n",
    "# Generate data\n",
    "observations, cluster = generate_data(μ1, μ2, Σ1, Σ2, ϕ, num_samples=N)\n",
    "\n",
    "# Visualization\n",
    "scatter(observations[cluster .== 1, 1], observations[cluster .== 1, 2], marker=:circle, color=\"black\", label=\"C1\")\n",
    "scatter!(observations[cluster .== 2, 1], observations[cluster .== 2, 2], \n",
    "         marker=:diamond,\n",
    "         color=\"black\", \n",
    "         label=\"C2\", \n",
    "         xlabel=L\"$x_1$\", \n",
    "         ylabel=L\"$x_2$\", \n",
    "         xlim=[-6, 6], \n",
    "         ylim=[-6, 6], \n",
    "         size=(500,500),\n",
    "         grid=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification\n",
    "\n",
    "We are going to start with a simple model and gradually increase its complexity. First, we important Turing, MCMCChains and call the utility functions for this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing\n",
    "using MCMCChains\n",
    "Turing.setadbackend(:forward_diff)\n",
    "include(\"../scripts/pp-2.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first Gaussian Mixture Model will assume 2 classes, equal class proportions, $\\pi = [0.5 \\ 0.5]$, and spherical covariance matrices, $\\Sigma_k = \\mathcal{I}$ for all $k$. Using these assumptions, we estimate the means $\\mu$ and compute the posterior of $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model GaussianMixtureModel(X, m1, m2) = begin\n",
    "    \"2-dimensional Gaussian mixture model with spherical covariances\"\n",
    "    \n",
    "    # Data shape\n",
    "    N, D = size(X)\n",
    "\n",
    "    # Draw the mean for cluster 1 [2x1 vector]\n",
    "    μ1 ~ MvNormal(m1, 10*[1. 0.; 0. 1.])\n",
    "    \n",
    "    # Draw the mean for cluster 2 [2x1 vector]\n",
    "    μ2 ~ MvNormal(m2, 10*[1. 0; 0. 1.])\n",
    "    \n",
    "    # Concatenate means [2x2 matrix]\n",
    "    μ = [μ1 μ2]\n",
    "    \n",
    "    # Probability of belonging to each cluster [2x1 vector]\n",
    "    w = [0.5, 0.5]\n",
    "    \n",
    "    # Preallocate assignment vector [Nx1 vector]\n",
    "    k = Vector{Int}(undef, N)\n",
    "    \n",
    "    # Loop over observations\n",
    "    for i in 1:N\n",
    "        \n",
    "        # Draw assignment for i-th sample from a Categorical distribution parameterized by w\n",
    "        k[i] ~ Categorical(w)\n",
    "        \n",
    "        # Data point x[i] follows a normal distribution, with mean indexed by the assignment variable k[i]\n",
    "        X[i,:] ~ MvNormal(μ[:, k[i]], [1. 0; 0. 1.])\n",
    "    end\n",
    "    \n",
    "    # Return the assignments\n",
    "    return k\n",
    "end\n",
    "\n",
    "# Set priors\n",
    "m1 = [-2, -2]\n",
    "m2 = [+2, +2]\n",
    "\n",
    "# Create an instance of a model, using the observations\n",
    "model = GaussianMixtureModel(observations, m1, m2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a data set and a model. To arrive at class assignments for each data point, we need to perform inference. As stated before, there are two main schools of thought in Bayesian inference: Monte Carlo sampling and variational approximations. Below, we will use a particular form of Monte Carlo sampling, called Gibbs sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Gibbs sampler\n",
    "sampler = Gibbs(PG(20, :k), HMC(0.1, 2, :μ1, :μ2))\n",
    "\n",
    "# Start sampling\n",
    "chain = sample(model, sampler, 200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the means over the MCMC chains\n",
    "μ1_hat = mean_chain(chain, :μ1)\n",
    "μ2_hat = mean_chain(chain, :μ2)\n",
    "\n",
    "# Plot data and overlay estimated posterior probabilities\n",
    "plot_posteriors(observations, μ1_hat, μ2_hat, x1=range(-5, stop=5), x2=range(-5, stop=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe(chain[[:μ1, :μ2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(chain[[:μ1, :μ2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model improvement: covariance matrices\n",
    "\n",
    "We modeled the data with spherical Gaussians. But the data doesn't really look spherical; the cluster plot showed that quite a few samples are not covered by the two Gaussians. This suggests that we should estimate the covariance matrices as well. \n",
    "\n",
    "A variance parameter is a strictly positive number and is captured with a Gamma distrbution. A covariance matrix is a positive-definite matrix and can be captured with a Wishart distribution. The [Wishart distribution](https://en.wikipedia.org/wiki/Wishart_distribution) is the natural multivariate extension of the Gamma distribution:\n",
    "\n",
    "$$ \\mathcal{W}_p(C \\mid V, n) = \\frac{1}{2^{np/2} \\ |V|^{n/2} \\ \\Gamma_p(n/2)} |C|^{(n-p-1)/2} \\exp(-\\text{tr}(V^{-1}C)/2)$$\n",
    "\n",
    "where $p$ is the dimensionality of the matrix-variate, $n$ is the degrees of freedom and $V$ is a scale matrix. $\\Gamma_p$ is a multivariate gamma function (continuous version of the factorial function).\n",
    "\n",
    "It is relatively straightforward to add two Wishart priors to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model GaussianMixtureModel(X, m1, m2, S1, S2) = begin\n",
    "    \"2-dimensional Gaussian mixture model with free covariances\"\n",
    "    \n",
    "    # Data shape\n",
    "    N, D = size(X)\n",
    "\n",
    "    # Draw means for each cluster [2x1 vector]\n",
    "    μ1 ~ MvNormal(m1, 10*[1. 0.; 0. 1.])\n",
    "    μ2 ~ MvNormal(m2, 10*[1. 0.; 0. 1.])\n",
    "    \n",
    "    # Draw a covariance matrix for each cluster [2x2 matrix]\n",
    "    Σ1 ~ Wishart(2, S1)\n",
    "    Σ2 ~ Wishart(2, S2)\n",
    "    \n",
    "    # Concatenate parameters to allow for indexing\n",
    "    μ = [μ1 μ2]\n",
    "    Σ = cat(Σ1, Σ2, dims=3)\n",
    "    \n",
    "    # Probability of belonging to each cluster [2x1 vector]\n",
    "    w = [0.5, 0.5]\n",
    "    \n",
    "    # Preallocate assignment vector [Nx1 vector]\n",
    "    k = Vector{Int}(undef, N)\n",
    "    \n",
    "    # Loop over observations\n",
    "    for i in 1:N\n",
    "        \n",
    "        # Draw assignment for i-th sample from a Categorical distribution parameterized by w\n",
    "        k[i] ~ Categorical(w)\n",
    "        \n",
    "        # Data point x[i] follows a normal distribution, with mean and covariance indexed by k[i]\n",
    "        X[i,:] ~ MvNormal(μ[:, k[i]], Σ[:,:, k[i]])\n",
    "    end\n",
    "    \n",
    "    # Return the assignments\n",
    "    return k\n",
    "end\n",
    "\n",
    "# Set prior means\n",
    "m1 = [-1, -1]\n",
    "m2 = [+1, +1]\n",
    "\n",
    "# Set prior covariances\n",
    "S1 = [1. 0.;0. 1.]\n",
    "S2 = [1. 0.;0. 1.]\n",
    "\n",
    "# Create an instance of a model, using the observations\n",
    "model = GaussianMixtureModel(observations, m1, m2, S1, S2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to extend the sampler as well. For now, we'll include $\\Sigma1$ and $\\Sigma2$ in the Hamiltonian MC procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Gibbs sampler\n",
    "sampler = Gibbs(PG(20, :k), HMC(0.1, 2, :μ1, :μ2, :Σ1, :Σ2))\n",
    "\n",
    "# Start sampling\n",
    "chain = sample(model, sampler, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the sample averages for the means and covariances of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the means over the MCMC chains\n",
    "μ1_hat = mean_chain(chain, :μ1)\n",
    "μ2_hat = mean_chain(chain, :μ2)\n",
    "Σ1_hat = reshape(mean_chain(chain, :Σ1), (2,2))\n",
    "Σ2_hat = reshape(mean_chain(chain, :Σ2), (2,2))\n",
    "\n",
    "# Plot data and overlay estimated posterior probabilities\n",
    "plot_posteriors(observations, μ1_hat, μ2_hat, Σ1=Σ1_hat, Σ2=Σ2_hat, x1=range(-8, stop=8), x2=range(-10, stop=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we've improved coverage of all the points, but the fit is still not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(chain[[:μ1, :Σ1, :μ2, :Σ2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model improvement: cluster proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model GaussianMixtureModel(X, m1, m2, S1, S2, a) = begin\n",
    "    \"2-dimensional Gaussian mixture model with free covariances\"\n",
    "    \n",
    "    # Data shape\n",
    "    N, D = size(X)\n",
    "\n",
    "    # Draw means for each cluster [2x1 vector]\n",
    "    μ1 ~ MvNormal(m1, 10*[1. 0.; 0. 1.])\n",
    "    μ2 ~ MvNormal(m2, 10*[1. 0.; 0. 1.])\n",
    "    \n",
    "    # Draw a covariance matrix for each cluster [2x2 matrix]\n",
    "    Σ1 ~ Wishart(2, S1)\n",
    "    Σ2 ~ Wishart(2, S2)\n",
    "    \n",
    "    # Draw a proportion \n",
    "    π ~ Dirichlet(a)\n",
    "    \n",
    "    # Concatenate parameters to allow for indexing\n",
    "    μ = [μ1 μ2]\n",
    "    Σ = cat(Σ1, Σ2, dims=3)\n",
    "    \n",
    "    # Preallocate assignment vector [Nx1 vector]\n",
    "    k = Vector{Int}(undef, N)\n",
    "    \n",
    "    # Loop over observations\n",
    "    for i in 1:N\n",
    "        \n",
    "        # Draw assignment for i-th sample from a Categorical distribution parameterized by w\n",
    "        k[i] ~ Categorical(π)\n",
    "        \n",
    "        # Data point x[i] follows a normal distribution, with mean and covariance indexed by k[i]\n",
    "        X[i,:] ~ MvNormal(μ[:, k[i]], Σ[:,:, k[i]])\n",
    "    end\n",
    "    \n",
    "    # Return the assignments\n",
    "    return k\n",
    "end\n",
    "\n",
    "# Set prior means\n",
    "m1 = [-1, -1]\n",
    "m2 = [+1, +1]\n",
    "\n",
    "# Set prior covariances\n",
    "S1 = [1. 0.;0. 1.]\n",
    "S2 = [1. 0.;0. 1.]\n",
    "\n",
    "# Set prior proportions\n",
    "a = [1., 1.]\n",
    "\n",
    "# Create an instance of a model, using the observations\n",
    "model = GaussianMixtureModel(observations, m1, m2, S1, S2, a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start sampler, with pi under the Hamiltonian MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Gibbs sampler\n",
    "sampler = Gibbs(PG(20, :k), HMC(0.1, 2, :μ1, :μ2, :Σ1, :Σ2, :π))\n",
    "\n",
    "# Start sampling\n",
    "chain = sample(model, sampler, 1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract parameters from the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the means over the MCMC chains\n",
    "π_hat = mean_chain(chain, :π)\n",
    "μ1_hat = mean_chain(chain, :μ1)\n",
    "μ2_hat = mean_chain(chain, :μ2)\n",
    "Σ1_hat = reshape(mean_chain(chain, :Σ1), (2,2))\n",
    "Σ2_hat = reshape(mean_chain(chain, :Σ2), (2,2))\n",
    "\n",
    "# Plot data and overlay estimated posterior probabilities\n",
    "plot_posteriors(observations, μ1_hat, μ2_hat, Σ1=Σ1_hat, Σ2=Σ2_hat, π=π_hat, x1=range(-8, stop=8), x2=range(-10, stop=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(chain[:π])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: 3 clusters\n",
    "\n",
    "Can you change the model to work with 3 clusters instead of 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Assignment: Dirichlet Process\n",
    "\n",
    "Are we sure we need 3 clusters to model this data? Maybe we need 4? Or 5? \n",
    "\n",
    "There is a way to automatically determine the number of clusters in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
