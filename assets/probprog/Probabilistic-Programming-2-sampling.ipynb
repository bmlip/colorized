{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Programming - 2\n",
    "\n",
    "## Monte Carlo sampling\n",
    "\n",
    "In this notebook, we are exploring a more complex problem set using Monte Carlo sampling, namely mixture modeling. It will show you how to extend the sampling procedure to the multivariate case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Write a Monte Carlo sampling procedure for a 2-dimensional mixture model.\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes.\n",
    "  - Optional\n",
    "    - Cheatsheets: [how does Julia differ from Matlab / Python](https://docs.julialang.org/en/v1/manual/noteworthy-differences/index.html).\n",
    "    - Tutorials using [Turing.jl](https://turing.ml/dev/tutorials/0-introduction/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture modeling\n",
    "\n",
    "We will first generate a synthetic data set to play around. This will let us focus on the data-generating process and on the point that a good model should follow the data-generating process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using Random\n",
    "using Distributions\n",
    "using StatsPlots\n",
    "using LaTeXStrings\n",
    "\n",
    "# Set a random seed.\n",
    "Random.seed!(3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct 10 data points for each cluster.\n",
    "N = 10\n",
    "\n",
    "# Means for each cluster, assuming each cluster is Gaussian distributed\n",
    "μs = [[-2 +2]; \n",
    "      [+2 -2]]\n",
    "\n",
    "# Cluster 1\n",
    "X1 = randn(N,2)\n",
    "for i = 1:N\n",
    "    X1[i,:] += μs[1,:]\n",
    "end \n",
    "    \n",
    "# Cluster 2\n",
    "X2 = randn(N,2)\n",
    "for i = 1:N\n",
    "    X2[i,:] += μs[2,:]\n",
    "end \n",
    "    \n",
    "# Concatenate clusters\n",
    "X = [X1; X2]\n",
    "\n",
    "# ForneyLab needs every vector to be a row\n",
    "X_ = [X[i,:] for i = 1:N*2]\n",
    "\n",
    "# Visualization.\n",
    "scatter(X[:,1], X[:,2], legend = false, title = \"Synthetic Dataset\")\n",
    "xlabel!(L\"$x_1$\")\n",
    "ylabel!(L\"$x_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Sampling via Turing.jl\n",
    "\n",
    "With MC sampling, we can obtain samples from the posterior distribution. These are used to approximate the expected value, variance, etc. Below, we will draw samples from the posterior distribution of cluster assignment. Those samples are averaged to approximate the _expected_ cluster assignment of each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import Turing.jl and the package containing the sampler. The script being included contains utility functions, important for plotting and diagnosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Turing\n",
    "using MCMCChains\n",
    "Turing.setadbackend(:forward_diff)\n",
    "include(\"../scripts/pp-2.jl\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the Gaussian Mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@model GaussianMixtureModel(x, m1, m2) = begin\n",
    "    \n",
    "    D, N = size(x)\n",
    "\n",
    "    # Draw the mean for cluster 1 [2x1 vector]\n",
    "    μ1 ~ MvNormal(m1, 1.)\n",
    "    \n",
    "    # Draw the mean for cluster 2 [2x1 vector]\n",
    "    μ2 ~ MvNormal(m2, 1.)\n",
    "    \n",
    "    # Concatenate means [2x2 matrix]\n",
    "    μ = [μ1 μ2]\n",
    "    \n",
    "    # Probability of belonging to each cluster [2x1 vector]\n",
    "    w = [0.5, 0.5]\n",
    "    \n",
    "    # Preallocate assignment vector [Nx1 vector]\n",
    "    k = Vector{Int}(undef, N)\n",
    "    \n",
    "    # Loop over observations\n",
    "    for i in 1:N\n",
    "        \n",
    "        # Draw assignment for i-th sample from a Categorical distribution parameterized by w\n",
    "        k[i] ~ Categorical(w)\n",
    "        \n",
    "        # Data point x[i] follows a normal distribution, with mean indexed by the assignment variable k[i]\n",
    "        x[:,i] ~ MvNormal(μ[:, k[i]], 1.)\n",
    "    end\n",
    "    \n",
    "    # Return the assignments\n",
    "    return k\n",
    "end\n",
    "\n",
    "\"Call an instance of the defined Gaussian Mixture model\"\n",
    "\n",
    "# Set priors\n",
    "m1 = [-2, -2]\n",
    "m2 = [0, 0]\n",
    "\n",
    "# Create an instance of a model, using the synthetic data set X\n",
    "gmm_model = GaussianMixtureModel(X, m1, m2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a data set and a model. To arrive at class assignments for each data point, we need to perform inference. As stated before, there are two main schools of thought in Bayesian inference: Monte Carlo sampling and variational approximations. Below, we will use a particular form of Monte Carlo sampling, called Gibbs sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Gibbs sampler\n",
    "gmm_sampler = Gibbs(PG(50, :k), HMC(0.05, 10, :μ1, :μ2))\n",
    "\n",
    "# Start sampling\n",
    "tchain = mapreduce(c -> sample(gmm_model, gmm_sampler, 100), chainscat, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the means over the MCMC chains for the posterior means\n",
    "μ1_estimated = vec(convert(Array{Float64,2}, mean(tchain[:μ1].value.data[:,:,1]; dims=1)));\n",
    "μ2_estimated = vec(convert(Array{Float64,2}, mean(tchain[:μ2].value.data[:,:,1]; dims=1)));\n",
    "\n",
    "# Report differences between estimated mean and true means\n",
    "println(\"Difference between μ1 and μ1_estimated = \"*string(abs.(μ1 - μ1_estimated)))\n",
    "println(\"Difference between μ2 and μ2_estimated = \"*string(abs.(μ2 - μ2_estimated)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data and overlay estimated posterior probabilities\n",
    "plot_posteriors(X, μ1_estimated, μ2_estimated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the means within the chain\n",
    "ids = findall(map(name -> occursin(\"μ\", name), names(tchain)));\n",
    "\n",
    "# Plot the course of the chains\n",
    "p=plot(tchain[:, ids, :], labels = [\"mu_11\" \"mu_12\" \"mu_21\" \"mu_22\"], colordim=:parameter)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
