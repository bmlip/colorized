{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Programming 3: Hidden Markov modeling\n",
    "## Variational inference\n",
    "\n",
    "In this notebook, we are looking at inference in a dynamical system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Learn to apply ForneyLab to a dynamical system.\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes.\n",
    "  - Optional\n",
    "    - Cheatsheets: [how does Julia differ from Matlab / Python](https://docs.julialang.org/en/v1/manual/noteworthy-differences/index.html).\n",
    "    - Getting started with [ForneyLab](https://biaslab.github.io/forneylab/docs/getting-started/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1mActivating\u001b[22m\u001b[39m environment at `~/Documents/biaslab/repos/BMLIP/lessons/notebooks/probprog/workspace/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg;Pkg.activate(\"workspace\");Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "using Distributions\n",
    "using Plots\n",
    "pyplot()\n",
    "include(\"../scripts/pp-3.jl\") \n",
    "\n",
    "Random.seed!(1234);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 3 possible states and each variable is in one of those (one-hot encoding)\n",
    "K = 3\n",
    "\n",
    "# Length of time-series\n",
    "T = 50\n",
    "\n",
    "# Transition matrix of latent variables\n",
    "transition = [0.3 0.6 0.1; \n",
    "              0.5 0.2 0.3; \n",
    "              0.2 0.8 0.1]\n",
    "\n",
    "# Emission matrix for observed variables\n",
    "emission = [0.7 0.3 0.0; \n",
    "            0.2 0.6 0.2; \n",
    "            0.0 0.3 0.7]\n",
    "\n",
    "# Preallocate data arrays\n",
    "X = zeros(T+1, K)\n",
    "Y = zeros(T, K)\n",
    "\n",
    "# Initial state\n",
    "X[1,:] = [0.0, 1.0, 0.0] \n",
    "\n",
    "# Generate data for entire time-series\n",
    "for t = 2:T\n",
    "    \n",
    "    # Transition from previous state\n",
    "    A = transition * X[t-1,:]\n",
    "    \n",
    "    # Sample from Categorical distribution\n",
    "    X[t,:] = one_hot(rand(Categorical(A ./ sum(A)), 1)[1], K)\n",
    "    \n",
    "    # Emission of current state\n",
    "    B = emission * X[t,:]\n",
    "    \n",
    "    # Sample from Categorical distribution\n",
    "    Y[t-1,:] = one_hot(rand(Categorical(B ./ sum(B)), 1)[1], K)\n",
    "    \n",
    "end\n",
    "\n",
    "# For visualization, we collapse the data from a one-hot to a numerical encoding\n",
    "states = argmax.(eachrow(X))\n",
    "observations = argmax.(eachrow(Y))\n",
    "\n",
    "# Visualization.\n",
    "plot(1:T, states[2:end], color=\"red\", label=\"states\", ylim=(0, 4), grid=false)\n",
    "scatter!(1:T, observations, color=\"blue\", label=\"observations\")\n",
    "xlabel!(\"time (t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model specification\n",
    "\n",
    "Let's quickly recap the $K$-component Gaussian mixture model. \n",
    "\n",
    "We have a data set of $X$ of $N$ samples by $D$ features. Each component is a Gaussian distribution, $ p_k(x \\mid \\mu_k, \\Sigma_k) \\triangleq \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)$. In total, we model $N$ samples:\n",
    "\n",
    "$$ p(X \\mid z, \\mu, \\Sigma) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} p_k(x_i \\mid \\mu_k, \\Sigma_k)^{z_i=k} \\, .$$\n",
    "\n",
    "<!-- Note that we summarize parameters as follows: $\\phi = (\\phi_1, \\dots, \\phi_K), \\mu = (\\mu_1, \\dots, \\mu_K), \\Sigma = (\\Sigma_1, \\dots, \\Sigma_K)$.  -->\n",
    "The variable $z_i$ is the assignment of sample $i$ to class $k$. The probability $p(z_{i} = k)$ is also called the _responsibility_ of component $k$ for the $i$-th sample. All $z_i$ follow a Categorical distribution with mixture weights as parameters:\n",
    "\n",
    "$$ p(z_i) \\sim \\text{Cat}(\\phi) \\, ,$$\n",
    "\n",
    "Being Bayesians, we put priors on all unknown parameters ($\\phi, \\mu, \\Sigma$):\n",
    "\n",
    "$$ \\begin{align}\n",
    "p(\\phi) \\sim&\\ \\text{Dir}(a_0) \\\\\n",
    "p(\\mu) \\sim&\\ \\mathcal{N}(m_0, l_0^{-1}\\Sigma) \\\\\n",
    "p(\\Sigma) \\sim&\\ \\mathcal{W}(W_0, v_0) \\end{align} \\, .$$\n",
    "\n",
    "The distribution for the mixture weights, $p(\\phi)$, follows a Dirichlet distribution because it is a vector that should sum to one. The component parameter priors are a Wishart distribution for the covariance matrix and a Gaussian distribution for the mean, conditioned on the sampled covariance matrix. Together, these two priors form a [Normal-Inverse-Wishart distribution](https://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution), $p(\\mu, \\Sigma)$, which is the conjugate prior for the Gaussian components. The full joint distribution is therefore:\n",
    "\n",
    "$$ \\begin{align} p(X, z, \\phi, \\mu, \\Sigma) =&\\ p(X \\mid z, \\phi, \\mu, \\Sigma) p(z \\mid \\phi) p(\\mu, \\Sigma) p(\\phi) \\\\\n",
    "=&\\ \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)^{z_i=k}\\ \\text{Cat}(z_i=k \\mid \\phi_k)\\ \\mathcal{NIW}(\\mu_k, \\Sigma_k \\mid m_{0k}, l_{0k}, W_{0k}, v_{0k})\\ \\text{Dir}(\\phi_k \\mid a_{0k}) \\end{align}$$\n",
    "\n",
    "We are interested in the posterior distribution over $z$, which assigns the current samples to clusters via $\\arg \\max_{k} p(z_i = k)$. For example, if the posterior distribution for $z_i$ is $[0.32\\ 0.21\\ 0.47]$, then $\\hat{k}_i$ is 3 and we say that $x_i$ belongs to the component 3. We are also interested in the posterior distributions for the component parameters, because these allow us to assign future samples to the current clusters. The posterior for $z, \\phi, \\mu, \\Sigma$ has the form:\n",
    "\n",
    "$$p(z, \\phi, \\mu, \\Sigma \\mid X) = \\frac{p(z, \\phi, \\mu, \\Sigma \\mid X)}{\\int p(X, z, \\phi, \\mu, \\Sigma)\\ \\mathrm{d}X} \\, .$$\n",
    "\n",
    "We cannot evaluate the integral in the denominator, but because it only serves as a normalization factor we have a workaround. Note that the numerator is the full joint distribution and that if you fix the parameters $\\phi, \\mu, \\Sigma$ to certain values, it simplifies to:\n",
    "\n",
    "$$p(z_i \\mid X) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\rho_k^{z_i = k} \\, ,$$\n",
    "\n",
    "where $\\rho_k = \\phi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ for all $k$. This is a Categorical distribution! The only problem is that $\\rho$ is not normalized correctly. But we know that it _should_ be correctly normalized, after division by the denominator term above, $\\int p(X, z, \\phi, \\mu, \\Sigma) \\ \\mathrm{d}X$. We can therefore perform the following re-parameterization:\n",
    "\n",
    "$$r_k = \\frac{\\rho_k}{\\sum_{k} \\rho_k} \\, .$$\n",
    "\n",
    "Now, $r_k$ are valid responsibilities for the assignment variable $z$. In other words, $p(z_i \\mid X) \\sim \\text{Cat}(r_k)$. But how should we fix $\\phi, \\mu, \\Sigma$? <br>\n",
    "Note that if we fix $z_i$ to $k=2$ for instance, the full posterior simplifies to:\n",
    "\n",
    "$$p(\\phi_2, \\mu_2, \\Sigma_2 \\mid X) = \\sum_{i=1}^{N} \\mathcal{N}(x_i \\mid \\mu_2, \\Sigma_2) \\ \\text{Cat}(z_i=2 \\mid \\phi_2)\\ \\mathcal{NIW}(\\mu_2, \\Sigma_2 \\mid m_{02}, l_{02}, W_{02}, v_{02}) \\ \\text{Dir}(\\phi_2 \\mid a_{02}) \\, .$$\n",
    "\n",
    "This can be split according to $p(\\phi) p(\\mu, \\Sigma)$:\n",
    "\n",
    "$$\\begin{align} \n",
    "p(\\phi_k \\mid z) =&\\ \\sum_{i=1}^{N} \\text{Cat}(z_i \\mid \\phi_k)\\ \\text{Dir}(\\phi_k \\mid a_{0}) \\\\ \n",
    "p(\\mu_k, \\Sigma_k \\mid X) =&\\ \\sum_{i=1}^{N} \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\ \\mathcal{NIW}(\\mu_k, \\Sigma_k \\mid m_{0k}, l_{0k}, W_{0k}, v_{0k}) \n",
    "\\end{align}$$\n",
    "\n",
    "which are Dirichlet and a Normal-Inverse-Wishart distributions. So, we have multiple marginal posteriors with known forms. We can define a recognition distribution that takes a similar form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForneyLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = FactorGraph()\n",
    "\n",
    "@RV A ~ Dirichlet(ones(3,3)) # Vague prior on transition model\n",
    "@RV B ~ Dirichlet([10.0 1.0 1.0; 1.0 10.0 1.0; 1.0 1.0 10.0]) # Stronger prior on observation model\n",
    "@RV s_0 ~ Categorical(1/3*ones(3))\n",
    "\n",
    "s = Vector{Variable}(undef, n_samples) # one-hot coding\n",
    "x = Vector{Variable}(undef, n_samples) # one-hot coding\n",
    "s_t_min = s_0\n",
    "for t = 1:n_samples\n",
    "    @RV s[t] ~ Transition(s_t_min, A)\n",
    "    @RV x[t] ~ Transition(s[t], B)\n",
    "    \n",
    "    s_t_min = s[t]\n",
    "    \n",
    "    placeholder(x[t], :x, index=t, dims=(3,))\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the recognition factorization\n",
    "q = RecognitionFactorization(A, B, [s_0; s], ids=[:A, :B, :S])\n",
    "\n",
    "# Generate VMP algorithm\n",
    "algo = variationalAlgorithm(q)\n",
    "\n",
    "# Construct variational free energy evaluation code\n",
    "algo_F = freeEnergyAlgorithm(q);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load algorithms\n",
    "eval(Meta.parse(algo))\n",
    "eval(Meta.parse(algo_F))\n",
    "\n",
    "# Initial recognition distributions\n",
    "marginals = Dict{Symbol, ProbabilityDistribution}(\n",
    "    :A => vague(Dirichlet, (3,3)),\n",
    "    :B => vague(Dirichlet, (3,3)))\n",
    "\n",
    "# Initialize data\n",
    "data = Dict(:x => x_data)\n",
    "n_its = 20\n",
    "\n",
    "# Run algorithm\n",
    "F = Vector{Float64}(undef, n_its)\n",
    "for i = 1:n_its\n",
    "    stepS!(data, marginals)\n",
    "    stepB!(data, marginals)\n",
    "    stepA!(data, marginals)\n",
    "\n",
    "    F[i] = freeEnergy(data, marginals)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "\n",
    "# Plot free energy\n",
    "plot(1:n_its, F, color=\"black\")\n",
    "\n",
    "grid(\"on\")\n",
    "xlabel(\"Iteration\")\n",
    "ylabel(\"Free Energy\")\n",
    "xlim(0,n_its);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(10,5))\n",
    "\n",
    "# Collect state estimates\n",
    "x_obs = [findfirst(x_i.==1.0) for x_i in x_data]\n",
    "s_true = [findfirst(s_i.==1.0) for s_i in s_data]\n",
    "\n",
    "# Plot simulated state trajectory and observations\n",
    "subplot(121)\n",
    "plot(1:n_samples, x_obs, \"k*\", label=\"Observations x\", markersize=7)\n",
    "plot(1:n_samples, s_true, \"k--\", label=\"True state s\")\n",
    "yticks([1.0, 2.0, 3.0], [\"Red\", \"Green\", \"Blue\"])\n",
    "grid(\"on\")\n",
    "xlabel(\"Time\")\n",
    "legend(loc=\"upper left\")\n",
    "xlim(0,n_samples)\n",
    "ylim(0.9,3.1)\n",
    "title(\"Data set and true state trajectory\")\n",
    "\n",
    "# Plot inferred state sequence\n",
    "subplot(122)\n",
    "m_s = [mean(marginals[:s_*t]) for t=1:n_samples]\n",
    "m_s_1 = [m_s_t[1] for m_s_t in m_s]\n",
    "m_s_2 = [m_s_t[2] for m_s_t in m_s]\n",
    "m_s_3 = [m_s_t[3] for m_s_t in m_s]\n",
    "\n",
    "fill_between(1:n_samples, zeros(n_samples), m_s_1, color=\"red\")\n",
    "fill_between(1:n_samples, m_s_1, m_s_1 + m_s_2, color=\"green\")\n",
    "fill_between(1:n_samples, m_s_1 + m_s_2, ones(n_samples), color=\"blue\")\n",
    "xlabel(\"Time\")\n",
    "ylabel(\"State belief\")\n",
    "grid(\"on\")\n",
    "title(\"Inferred state trajectory\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True state transition probabilities\n",
    "PyPlot.plt.matshow(A_data, cmap=\"bone\", vmin=0.0, vmax=1.0)\n",
    "ttl = title(\"True state transition probabilities\")\n",
    "ttl.set_position([.5, 1.15])\n",
    "yticks([0, 1, 2], [\"Red\", \"Green\", \"Blue\"])\n",
    "xticks([0, 1, 2], [\"Red\", \"Green\", \"Blue\"], rotation=\"vertical\")\n",
    "colorbar()\n",
    "\n",
    "# Inferred state transition probabilities\n",
    "PyPlot.plt.matshow(mean(marginals[:A]), cmap=\"bone\", vmin=0.0, vmax=1.0)\n",
    "ttl = title(\"Inferred state transition probabilities\")\n",
    "ttl.set_position([.5, 1.15])\n",
    "yticks([0, 1, 2], [\"Red\", \"Green\", \"Blue\"])\n",
    "xticks([0, 1, 2], [\"Red\", \"Green\", \"Blue\"], rotation=\"vertical\")\n",
    "colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
