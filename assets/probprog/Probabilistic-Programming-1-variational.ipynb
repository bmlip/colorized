{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Programming - 1 \n",
    "## Variational inference\n",
    "\n",
    "So far you've been doing all your calculations by hand. As you have probably learned, this is timeconsuming and error prone. In this lesson we are going to introduce Probabilistic Programming as a method to automate this labour away. We will cover 2 packages - ForneyLab and Turing - and show you how to specify probabilistic models in both. Afterwards, you'll get your first taste of Probabilistic Programming by solving your first problem! The main takehome point is that while Probabilistic Programming requires some specialised mindware in terms of probability theory and correct model specification, implementation is straightforward once you have the right tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Learn to write a basic probabilistic program using a variational inference procedure.\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes.\n",
    "    - [Intro to programming in Julia](https://youtu.be/8h8rQyEpiZA?t=233).\n",
    "    - Getting started with [ForneyLab](https://biaslab.github.io/forneylab/docs/getting-started/).\n",
    "  - Optional\n",
    "    - Cheatsheets: [how does Julia differ from Matlab / Python](https://docs.julialang.org/en/v1/manual/noteworthy-differences/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to ForneyLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ForneyLab](https://github.com/biaslab/ForneyLab.jl) is a Probabilistic Programming toolbox developed inhouse at [BIASLab](https://biaslab.github.io/). ForneyLab utilises the FFG formalism that you are familiar with by now to describe models. Inference is performed through variational procedures or, if applicable, exact inference. This is the core feature of ForneyLab: A large library of analytical update rules similar to what you have derived so far. This provides fast, deterministic inference with convergence guarantees, provided you give it the right generative model.\n",
    "Things to keep in mind when using ForneyLab:\n",
    "1. It is optimised for Dynamical Systems and time series data. It'll be extra fast in these circumstances\n",
    "2. Relies on deterministic update rules - you will get the same result each time\n",
    "3. If you find a bug, please tell us :) \n",
    "\n",
    "Now, let's work through an example using ForneyLab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by generating some data\n",
    "using Plots\n",
    "true_W = [1.0;0.5]\n",
    "true_σ = 1.\n",
    "true_μ = 0.\n",
    "true_slope = 0.1\n",
    "n=42\n",
    "\n",
    "\n",
    "#function generate_data(true_W,true_σ,true_μ,sqrt_n)\n",
    "#    x_data = [[Float64(x1),Float64(x2)] for x1 in 0:sqrt_n for x2 in 0:sqrt_n]\n",
    "#    y_data = [true_W' * x + randn() * true_σ for x in x_data]\n",
    "#    return x_data,y_data\n",
    "#end\n",
    "            \n",
    "function generate_data(true_W, true_σ, true_μ, n)\n",
    "    x = randn(size(true_W)[1],n) .* true_σ .+ true_μ # Draw from a Gaussian with correct parameters\n",
    "    [x[:,i] .+= true_slope * i for i in 1:n] # Add a slope\n",
    "    y = true_W' * x # Compute y values\n",
    "    return [x[:,i] for i in 1:n],y # List comprehension formats data for easy access later\n",
    "end\n",
    "    \n",
    "x_data,y_data = generate_data(true_W,true_σ,true_μ,n)\n",
    "scatter(1:n,y_data[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model our data, we define a model using ForneyLab. We will try a bayesian linear regression of the form\n",
    "\n",
    "$$y \\sim \\mathcal{N}(W^Tx,\\sigma_y)$$\n",
    "With a Gaussian prior on the weights W\n",
    "$$ W \\sim \\mathcal{N}(\\mathbf{\\mu_w},\\mathbf{\\sigma_w})$$\n",
    "\n",
    "The first thing we need to do is to import ForneyLab and instantiate a Factor Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForneyLab\n",
    "g = FactorGraph();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a FactorGraph, we can start adding variables to it. This is the point where we specify our probabilistic model. When doing Probabilistic Programming, the core challenge is good model specification. To keep it simple, let's start by defining a model for a single x/y pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for priors\n",
    "μ_w = [0.,0.]\n",
    "σ_w = [1. 0. ; 0. 1.]\n",
    "σ_y = 1.\n",
    "\n",
    "# Let's create a random variable for our input data x. In ForneyLab we use the @RV macro to denote Random Variables\n",
    "@RV x \n",
    "\n",
    "# Now let's define a prior over the weights W. We use a 2 dimensional Gaussian since we have 2 weights\n",
    "@RV W ~ GaussianMeanVariance(μ_w,σ_w)\n",
    "\n",
    "# And finally our linear model for y\n",
    "@RV y ~ GaussianMeanVariance(dot(W,x), σ_y)\n",
    "\n",
    "\n",
    "# Visualise the graph\n",
    "ForneyLab.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at the graph, you will see that it has some open edges for x and y. This is where we want to feed in our data. To do so in ForneyLab, we designate them as placeholders. This means that we do not give them a value immediately but want them to take one on at a later time - such as when we want to feed in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder(x,:x,dims=(2,)) # The dims flag tells ForneyLab to expect 2 dimensional inputs here\n",
    "placeholder(y,:y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model, it is time to get our inference algorithm. ForneyLab includes the Belief Propagation algorithm for exact inference which we will utilise here. The details of the procedure are not important at this time, so feel free to treat it as a \"magic inference button\".\n",
    "ForneyLab works by directly generating new Julia code containing the inference algorithm. When we parse this code, we get a function (step!) which we can then run to execute the inference procedure.\n",
    "\n",
    "If you are feeling adventurous, feel free to experiment with some of the other inference tools available in ForneyLab. Do you get different results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = sumProductAlgorithm(W) # Generate Julia code for inference. The argument tells ForneyLab which variable we want the posterior for\n",
    "eval(Meta.parse(algo)) # Evaluate the generated code to get the step! function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a dictionary with our data. Since our model is only for a single data point, we create a dictionary holding just one random x/y pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dict(:x => x_data[10],\n",
    "            :y => y_data[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we feed our data dictionary into the step! function, do inference and observe the results! For clarity we have written the model specification below as well so you can get a sense of what a full ForneyLab program looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = FactorGraph(); # Instantiate Factor Graph\n",
    "\n",
    "# Parameters for priors\n",
    "μ_w = [0.,0.]\n",
    "σ_w = [1. 0. ; 0. 1.]\n",
    "σ_y = 1.\n",
    "\n",
    "\n",
    "# Specify probabilistic model\n",
    "@RV x \n",
    "@RV W ~ GaussianMeanVariance(μ_w,σ_w)\n",
    "@RV y ~ GaussianMeanVariance(dot(W,x), σ_y)\n",
    "\n",
    "# Denote placeholders\n",
    "placeholder(x,:x,dims=(2,)) \n",
    "placeholder(y,:y);\n",
    "\n",
    "# Generate inference algorithm\n",
    "algo = sumProductAlgorithm(W) \n",
    "eval(Meta.parse(algo)) \n",
    "\n",
    "# Set up data dictionary\n",
    "data = Dict(:x => x_data[10],\n",
    "            :y => y_data[10]);\n",
    "\n",
    "# Do inference\n",
    "marginals = step!(data)\n",
    "\n",
    "# Print results\n",
    "println(\"Posterior mean: \",mean(marginals[:W]))\n",
    "println(\"Posterior Variance: \",var(marginals[:W]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't particurlarly accurate because we only used a single data point. Let's create a new model that takes in the full data set instead and see if we get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = FactorGraph() # Instantiate new Factor Graph\n",
    "\n",
    "# Parameters for priors\n",
    "μ_w = [0.,0.]\n",
    "σ_w = [1. 0. ; 0. 1.]\n",
    "σ_y = 1.\n",
    "\n",
    "# Create vectors to hold variables for all data points\n",
    "x = Vector{Variable}(undef,n) \n",
    "y = Vector{Variable}(undef,n)\n",
    "\n",
    "# Define a prior over the weights. These are shared for all data points\n",
    "@RV W ~ GaussianMeanVariance(μ_w,σ_w)\n",
    "\n",
    "# Loop over the dataset\n",
    "for i in 1:n\n",
    "    @RV x[i] # We can use indexing to fill a vector with random variables\n",
    "    @RV y[i] ~ GaussianMeanVariance(dot(W,x[i]), σ_y) \n",
    "    placeholder(x[i],:x,index=i,dims=(2,)) # To define placeholders in vectors, we use the index flag\n",
    "    placeholder(y[i],:y,index=i)\n",
    "end\n",
    "\n",
    "# Generate a new inference algorithm\n",
    "algo = sumProductAlgorithm(W)\n",
    "eval(Meta.parse(algo)) \n",
    "\n",
    "# Now we can fit the entire data set in our model!\n",
    "data = Dict(:x => x_data,\n",
    "            :y => y_data)\n",
    "\n",
    "# Perform inference and observe the results\n",
    "marginals = step!(data)\n",
    "println(\"Posterior mean: \",mean(marginals[:W]))\n",
    "println(\"Posterior Variance: \",var(marginals[:W]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! The trick to note above is the use of vectors and indices to handle multiple data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigment 1. \n",
    "Generate some new data and change the value of true_$\\mu$ to something other than 0. How does the model fit now? Likely not particularly well, since it is missing an intercept term. Your task is to remedy this by extending the regression model to handle intercepts.\n",
    "\n",
    "Do this by extending the input data with an additional column of 1's and adding an extra weight to W. Can you explain why this works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
