{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Programming - 1 \n",
    "## Variational inference\n",
    "\n",
    "In this lesson we are going to use variational inference for regression and classification. We will use ForneyLab.jl to specify a model and run an inference procedure. While Probabilistic Programming requires some specialised knowledge in terms of probability theory and Bayesian inference, implementing an inference procedure is straightforward once you have the right tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Learn to estimate regression parameters using a variational inference procedure.\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes.\n",
    "    - [Intro to programming in Julia](https://youtu.be/8h8rQyEpiZA?t=233).\n",
    "  - Optional\n",
    "    - Getting started with [ForneyLab](https://biaslab.github.io/forneylab/docs/getting-started/).\n",
    "    - Cheatsheets: [how does Julia differ from Matlab / Python](https://docs.julialang.org/en/v1/manual/noteworthy-differences/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ForneyLab\n",
    "\n",
    "[ForneyLab](https://github.com/biaslab/ForneyLab.jl) is a Probabilistic Programming toolbox developed inhouse at [BIASLab](https://biaslab.github.io/). ForneyLab utilises the FFG formalism that you are familiar with by now to describe models. Inference is performed through variational procedures or, if applicable, exact inference. This is the core feature of ForneyLab: a large library of analytical update rules similar to what you have derived so far. This provides fast, deterministic inference with convergence guarantees, provided you supply the right generative model.\n",
    "Things to keep in mind when using ForneyLab:\n",
    "1. It is optimised for Dynamical Systems and time series data. It'll be fast in these circumstances.\n",
    "2. Relies on deterministic update rules - you will get the same result each time.\n",
    "3. If you find a bug, please tell us :) \n",
    "\n",
    "Now, let's work through an example using ForneyLab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: workspace instantiation\n",
    "# using Pkg\n",
    "# Pkg.activate(\"workspace\")\n",
    "# Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Logging; disable_logging(LogLevel(0))\n",
    "using StatsPlots\n",
    "using ForneyLab\n",
    "import Distributions: Uniform, Normal, rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "\n",
    "We'll generate a simple regression problem setting. Given a data set $x = (x_1, \\dots, x_N)$ of $N$ samples, we generate the set of observations $y = (y_1, \\dots, y_N)$ via: \n",
    "\n",
    "$$ y = f(x) + \\epsilon$$ \n",
    "\n",
    "where $f(x) = x*w_1 + w_2$ and $\\epsilon \\sim \\mathcal{N}(0, \\sigma)$. We summarize the slope coefficient and intercept into a single weight vector, $w = [w_1\\ w_2]$. We will use the name \"covariates\" for $x$ and \"responses\" for $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples\n",
    "N = 20\n",
    "\n",
    "# Parameters\n",
    "true_w = [4.0 1.0]\n",
    "true_σ = 1/2\n",
    "\n",
    "function generate_data(true_w, true_σ; num_samples=10)\n",
    "    \"Generate data according to y = w'*x + ϵ\"\n",
    "    \n",
    "    # Covariates\n",
    "    x = rand(Uniform(0,1), num_samples)\n",
    "    \n",
    "    # Linear function of covariates\n",
    "    fx = x.* true_w[1] .+ true_w[2] \n",
    "    \n",
    "    # Generate Gaussian noise\n",
    "    ϵ = rand(Normal(0, true_σ), num_samples)\n",
    "    \n",
    "    # Responses consist of the linear mapping plus noise\n",
    "    y = fx + ϵ\n",
    "    \n",
    "    # Return covariates and responses\n",
    "    return y, x\n",
    "end\n",
    "\n",
    "# Generate data\n",
    "responses, covariates = generate_data(true_w, true_σ, num_samples=N)\n",
    "\n",
    "# True regression of the covariates\n",
    "fx = covariates*true_w[1] .+ true_w[2]\n",
    "\n",
    "# Visualize data\n",
    "scatter(covariates, responses, color=\"blue\", label=\"responses\", legend=:topleft)\n",
    "plot!(covariates, fx, color=\"red\", label=\"true regression\", linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Specification\n",
    "\n",
    "We need to specify our model using ForneyLab. Firstly, we will formulate the linear function using a dot product:\n",
    "\n",
    "$$f(x) = [w_1\\ w_2] \\cdot [x\\ 1]^{\\top} \\, .$$\n",
    "\n",
    "We know that the responses are generated via a linear function of the covariates and Gaussian noise:\n",
    "\n",
    "$$y \\sim \\mathcal{N}(f(x),\\sigma_y)\\, .$$ \n",
    "\n",
    "We know that the weights are real numbers and that they can be negative. It makes sense therefore to put a Gaussian prior on them:\n",
    "\n",
    "$$ w \\sim \\mathcal{N}(\\mu_w, \\sigma_w) \\, .$$\n",
    "\n",
    "For now, this is all we need. We're going to specify these two equations within ForneyLab. First, we initialize a factor graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = FactorGraph();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a FactorGraph, we can start adding variables to it. This is the actual point where we specify our probabilistic model. To keep it simple, let's start by defining a model for a single x/y pair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for priors\n",
    "μ_w = [0. 0.]\n",
    "σ_w = [1. 0.; 0. 1.]\n",
    "σ_y = 1.\n",
    "\n",
    "# In ForneyLab we use the @RV macro to denote Random Variables\n",
    "@RV x\n",
    "\n",
    "# Define a prior over the weights\n",
    "@RV w ~ GaussianMeanVariance(μ_w, σ_w)\n",
    "\n",
    "# Response model is Gaussian function of the linear mapping between weights and covariates\n",
    "@RV y ~ GaussianMeanVariance(dot(w, x), σ_y)\n",
    "\n",
    "# Visualise the graph\n",
    "ForneyLab.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take a look at the graph, you will see that it has some open edges for x and y. This is where we want to feed in our data. To do so in ForneyLab, we designate them as placeholders. This means that we do not give them a value immediately but want them to take one a value later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We designate the covariates x to have two dimensions (the last one being all 1)\n",
    "placeholder(x, :x, dims=(2,))\n",
    "\n",
    "# Designate the observation variable\n",
    "placeholder(y, :y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model, it is time to infer parameters. ForneyLab includes Sum-Product as an exact inference algorithm. The details of the procedure are not important at this time, so feel free to treat it as a \"magic inference button\".\n",
    "ForneyLab works by directly generating new Julia code containing the inference algorithm. When we parse this code, we get a function (step!) which we can then run to update the recognition factors.\n",
    "\n",
    "If you are feeling adventurous, feel free to experiment with some of the other inference tools available in ForneyLab. Do you get different results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We specify a recognition distribution\n",
    "q = RecognitionFactorization(w, ids=[:w])\n",
    "\n",
    "# Call the variational algorithm\n",
    "algo = variationalAlgorithm(q) \n",
    "\n",
    "# Evaluate the generated code to get the step! function\n",
    "eval(Meta.parse(algo)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a dictionary with our data. Since our model is only for a single data point, we create a dictionary holding just one random x/y pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dict(:x => [covariates[10]; 1],\n",
    "            :y => responses[10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we feed our data dictionary into the step! function, do inference and observe the results! For clarity we have written the model specification below as well so you can get a sense of what a full ForneyLab program looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = FactorGraph()\n",
    "\n",
    "# Parameters for priors\n",
    "μ_w = [2.; 0.]\n",
    "σ_w = 10*[1. 0.; 0. 1.]\n",
    "σ_y = 1.\n",
    "\n",
    "# Specify probabilistic model\n",
    "@RV x \n",
    "@RV w ~ GaussianMeanVariance(μ_w, σ_w)\n",
    "@RV y ~ GaussianMeanVariance(dot(w, x), σ_y)\n",
    "\n",
    "# Denote placeholders\n",
    "placeholder(x, :x, dims=(2,)) \n",
    "placeholder(y, :y);\n",
    "\n",
    "# Specify recognition distribution\n",
    "q = RecognitionFactorization(w, ids=[:w])\n",
    "\n",
    "# Generate inference algorithm\n",
    "algo = variationalAlgorithm(q) \n",
    "eval(Meta.parse(algo)) \n",
    "\n",
    "# Set up data dictionary\n",
    "data = Dict(:x => [covariates[10]; 1],\n",
    "            :y => responses[10]);\n",
    "\n",
    "# Do inference\n",
    "marginals = stepw!(data)\n",
    "\n",
    "# Print results\n",
    "println(\"Posterior mean: \", mean(marginals[:w]))\n",
    "println(\"Posterior Variance: \", var(marginals[:w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That wasn't particurlarly accurate because we only used a single data point. Let's create a new model that takes in the full data set instead and see if we get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = FactorGraph()\n",
    "\n",
    "# Parameters for priors\n",
    "μ_w = [2.; 0.]\n",
    "σ_w = 10*[1. 0.; 0. 1.]\n",
    "σ_y = 1.\n",
    "\n",
    "# Create vectors to hold variables for all data points\n",
    "x = Vector{Variable}(undef, N) \n",
    "y = Vector{Variable}(undef, N)\n",
    "\n",
    "# Define a prior over the weights. These are shared for all data points\n",
    "@RV w ~ GaussianMeanVariance(μ_w, σ_w)\n",
    "\n",
    "# Loop over the dataset\n",
    "for i in 1:N\n",
    "    \n",
    "    # We can use indexing to fill a vector with random variables\n",
    "    @RV x[i] \n",
    "    @RV y[i] ~ GaussianMeanVariance(dot(w, x[i]), σ_y) \n",
    "    \n",
    "    # To define placeholders in vectors, we use the index flag\n",
    "    placeholder(x[i], :x, index=i, dims=(2,)) \n",
    "    placeholder(y[i], :y, index=i)\n",
    "end\n",
    "\n",
    "# Specify recognition distribution\n",
    "q = RecognitionFactorization(w, ids=[:w])\n",
    "\n",
    "# Generate inference algorithm\n",
    "algo = variationalAlgorithm(q) \n",
    "eval(Meta.parse(algo)) \n",
    "\n",
    "# Now we can fit the entire data set in our model!\n",
    "data = Dict(:x => [[covariates[i]; 1] for i in 1:N],\n",
    "            :y => responses)\n",
    "\n",
    "# Perform inference and observe the results\n",
    "marginals = stepw!(data)\n",
    "println(\"Posterior mean: \", mean(marginals[:w]))\n",
    "println(\"Posterior Variance: \", var(marginals[:w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! The trick to note above is the use of vectors and indices to handle multiple data points. We can make predictions and visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract estimated weights\n",
    "w_hat = mean(marginals[:w])\n",
    "\n",
    "# Make predictions\n",
    "y_hat = covariates * w_hat[1] .+ w_hat[2]\n",
    "\n",
    "# Visualize true data and observations\n",
    "scatter(covariates, responses, color=\"blue\", label=\"observations\", legend=:topleft)\n",
    "plot!(covariates, fx, color=\"red\", label=\"true regression\", linewidth=2)\n",
    "\n",
    "# Overlay predictions\n",
    "plot!(covariates, y_hat, color=\"green\", label=\"prediction\", linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional assignment 1: informativeness of priors\n",
    "\n",
    "We chose a prior distribution for the weights. As you've seen in the introduction notebook, the parameters of the final recognition distribution still depend on the parameters of the prior distribution. This means the result we've just gotten will be different for different priors. Play around with other choices of prior distribution parameters. \n",
    "\n",
    "Try a sharp prior on the wrong value (small variance, mean away from true_w). Where does that get you? And what if you use a very flat prior (super high variance)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia-1.3 1.3.1",
   "language": "julia",
   "name": "julia-1.3-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
