{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Programming - 2: Mixture modeling\n",
    "## Variational inference\n",
    "\n",
    "In this notebook, we are exploring a more complex problem set using variational inference, namely mixture modeling. It will show you how to deal with the multivariate case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Write a variational inference procedure for a 2-dimensional mixture model.\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes.\n",
    "  - Optional\n",
    "    - Cheatsheets: [how does Julia differ from Matlab / Python](https://docs.julialang.org/en/v1/manual/noteworthy-differences/index.html).\n",
    "    - Getting started with [ForneyLab](https://biaslab.github.io/forneylab/docs/getting-started/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture modeling\n",
    "\n",
    "We will first generate a synthetic data set to play around with. This will let us focus on the data-generating process and on the point that a good model should follow the data-generating process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "import Distributions: MvNormal, Normal, pdf\n",
    "using Plots\n",
    "pyplot()\n",
    "\n",
    "# Set a random seed.\n",
    "Random.seed!(3)\n",
    "\n",
    "# Construct 10 data points for each cluster.\n",
    "N = 20\n",
    "\n",
    "# Means for each cluster, assuming each cluster is Gaussian distributed\n",
    "μs = [[-2 +2]; \n",
    "      [+2 -2]]\n",
    "\n",
    "# Cluster 1\n",
    "X1 = randn(N,2)\n",
    "for i = 1:N\n",
    "    X1[i,:] += μs[1,:]\n",
    "end \n",
    "    \n",
    "# Cluster 2\n",
    "X2 = randn(N,2)\n",
    "for i = 1:N\n",
    "    X2[i,:] += μs[2,:]\n",
    "end \n",
    "    \n",
    "# Concatenate clusters\n",
    "X = [X1; X2]\n",
    "\n",
    "# ForneyLab needs every vector to be a row\n",
    "X_ = [X[i,:] for i = 1:N*2]\n",
    "\n",
    "# Visualization.\n",
    "scatter(X[:,1], X[:,2], legend = false, title = \"Synthetic Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForneyLab\n",
    "\n",
    "g = FactorGraph()\n",
    "\n",
    "# Specify generative model\n",
    "@RV _pi ~ Beta(1.0, 1.0)\n",
    "@RV m_1 ~ GaussianMeanVariance([0.0, 0.0], [1. 0.; 0. 1.])\n",
    "@RV W_1 ~ Wishart([1. 0.; 0. 1.], 2.)\n",
    "@RV m_2 ~ GaussianMeanVariance([0.0, 0.0], [1. 0.; 0. 1.])\n",
    "@RV W_2 ~ Wishart([1. 0.; 0. 1.], 2.)\n",
    "\n",
    "z = Vector{Variable}(undef, N)\n",
    "y = Vector{Variable}(undef, N)\n",
    "for i = 1:N\n",
    "    @RV z[i] ~ Bernoulli(_pi)\n",
    "    @RV y[i] ~ GaussianMixture(z[i], m_1, W_1, m_2, W_2)\n",
    "    \n",
    "    placeholder(y[i], :y, dims=(2,), index=i)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to specify a distribution with which you will approximate the posterior distribution that you would normally obtain through exact Bayesian inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify recognition factorization (mean-field)\n",
    "q = RecognitionFactorization(_pi, m_1, W_1, m_2, W_2, z, ids=[:PI, :M1, :W1, :M2, :W2, :Z])\n",
    "\n",
    "# Generate the algorithm\n",
    "algo = variationalAlgorithm(q)\n",
    "algo_F = freeEnergyAlgorithm(q);\n",
    "\n",
    "# Load algorithms\n",
    "eval(Meta.parse(algo))\n",
    "eval(Meta.parse(algo_F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we execute the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dict(:y => X_)\n",
    "\n",
    "# Prepare recognition distributions\n",
    "marginals = Dict(:_pi => ProbabilityDistribution(Beta, a=1.0, b=1.0),\n",
    "                 :m_1 => ProbabilityDistribution(Multivariate, GaussianMeanVariance, m=[-1.0, +1.0], v=1e4*[1. 0.;0. 1.]),\n",
    "                 :W_1 => ProbabilityDistribution(MatrixVariate, Wishart, v=10*[1. 0.;0. 1.], nu=2.),\n",
    "                 :m_2 => ProbabilityDistribution(Multivariate, GaussianMeanVariance, m=[+1.0, -1.0], v=1e4*[1. 0.;0. 1.]),\n",
    "                 :W_2 => ProbabilityDistribution(MatrixVariate, Wishart, v=10*[1. 0.;0. 1.], nu=2.)\n",
    "                 )\n",
    "for i = 1:N\n",
    "    marginals[:z_*i] = ProbabilityDistribution(Bernoulli, p=1/2.)\n",
    "end\n",
    "\n",
    "# Execute algorithm\n",
    "num_iterations = 20\n",
    "F = Float64[]\n",
    "for i = 1:num_iterations\n",
    "    stepZ!(data, marginals)\n",
    "    stepPI!(data, marginals)\n",
    "    stepM1!(data, marginals)\n",
    "    stepW1!(data, marginals)\n",
    "    stepM2!(data, marginals)\n",
    "    stepW2!(data, marginals)\n",
    "        \n",
    "    # Store variational free energy for visualization\n",
    "    push!(F, freeEnergy(data, marginals))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot free energy to check for convergence\n",
    "plot(1:num_iterations, F, color=\"black\", label=\"\")\n",
    "xlabel!(\"VMP iteration\")\n",
    "ylabel!(\"Variational free energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"scripts/pp-2.jl\")\n",
    "\n",
    "# Estimated means\n",
    "μ1_estimated = mean(marginals[:m_1])\n",
    "μ2_estimated = mean(marginals[:m_2])\n",
    "v1_estimated = var(marginals[:m_1])\n",
    "v2_estimated = var(marginals[:m_2])\n",
    "Σ1_estimated = cholinv(mean(marginals[:W_1]))\n",
    "Σ2_estimated = cholinv(mean(marginals[:W_2]))\n",
    "\n",
    "# Plot data and overlay estimated posterior probabilities\n",
    "plot_posteriors(X, μ1_estimated, μ2_estimated, Σ1_estimated, Σ2_estimated; x1=range(-10,stop=10), x2=range(-10,stop=10))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia-1.3 1.3.1",
   "language": "julia",
   "name": "julia-1.3-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
