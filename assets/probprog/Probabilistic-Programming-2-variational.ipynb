{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic Programming 2: Mixture modeling\n",
    "## Variational inference\n",
    "\n",
    "In this notebook, we are exploring a more complex problem set using variational inference, namely mixture modeling. It will show you how to deal with the multivariate case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Write a variational inference procedure for a 2-dimensional mixture model.\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes.\n",
    "  - Optional\n",
    "    - [Getting started with ForneyLab](https://biaslab.github.io/forneylab/docs/getting-started/).\n",
    "    - [Demos in ForneyLab](https://github.com/biaslab/ForneyLab.jl/tree/v0.10.0/demo)\n",
    "    - [Probabilistic Programming notebook](https://github.com/bertdv/BMLIP/tree/master/lessons/notebooks/probprog/Probabilistic-Programming.ipynb)\n",
    "    - [Cheatsheets: how does Julia differ from Matlab / Python](https://docs.julialang.org/en/v1/manual/noteworthy-differences/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package managing\n",
    "using Pkg\n",
    "Pkg.activate(\"workspace\")\n",
    "Pkg.instantiate();\n",
    "\n",
    "using Logging; disable_logging(LogLevel(0))\n",
    "using Distributions: MvNormal\n",
    "using StatsPlots\n",
    "using LaTeXStrings\n",
    "using ForneyLab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "We generate samples from three 2-dimensional Gaussian distributions and mix these. These Gaussians are:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{N}_1(\\begin{bmatrix}-2, -2 \\end{bmatrix}, \\begin{bmatrix}1 & 0 \\\\0 & 3 \\end{bmatrix}) \\, , \\quad\n",
    "\\mathcal{N}_2(\\begin{bmatrix}+2, +2 \\end{bmatrix}, \\begin{bmatrix}3 & 0 \\\\0 & 1 \\end{bmatrix}) \\, , \\quad\n",
    "\\mathcal{N}_3(\\begin{bmatrix}-2, +2 \\end{bmatrix}, \\begin{bmatrix}1 & 0 \\\\0 & 1 \\end{bmatrix}) \\, .\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We draw $N_1$, $N_2$ and $N_3$ samples from each, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size\n",
    "N1 = 10\n",
    "N2 = 20\n",
    "N3 = 30\n",
    "\n",
    "# Parameters for each cluster\n",
    "μ1 = [-2. -2.]\n",
    "Σ1 = [1.0 0.0; 0.0 3.0]\n",
    "μ2 = [+2. +2.]\n",
    "Σ2 = [3.0 0.0; 0.0 1.0]\n",
    "μ3 = [-2. +2.]\n",
    "Σ3 = [1.0 0.0; 0.0 1.0]\n",
    "\n",
    "function generate_data(means::Array{Float64,2}, covariances::Array{Float64,3}; num_samples::Array{Int64,1}=[10])\n",
    "    \"Generate data from a 2-dimensional mixture model\"\n",
    "    \n",
    "    # Extract number of components\n",
    "    K = length(num_samples)\n",
    "    \n",
    "    # Extract dimensionality\n",
    "    D = size(covariances)[1]\n",
    "    \n",
    "    # Preallocate vectors\n",
    "    X = zeros(sum(num_samples), D)\n",
    "    C = zeros(sum(num_samples),)\n",
    "    \n",
    "    # Create sample index\n",
    "    ix = cat(0, cumsum(num_samples), dims=1)\n",
    "    \n",
    "    for k = 1:K\n",
    "        \n",
    "        # Current indices\n",
    "        ix_k = (ix[k]+1:ix[k+1])\n",
    "    \n",
    "        # Sample from cluster component\n",
    "        X[ix_k, :] = rand(MvNormal(means[k,:], covariances[:,:,k]), num_samples[k])'\n",
    "        \n",
    "        # Store cluster origin\n",
    "        C[ix_k,] = k*ones(num_samples[k])\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return X, C\n",
    "end\n",
    "\n",
    "# Generate data\n",
    "observations, cluster = generate_data(cat(μ1, μ2, μ3, dims=1), cat(Σ1, Σ2, Σ3, dims=3), num_samples=[N1, N2, N3])\n",
    "\n",
    "# Total sample size\n",
    "N = sum([N1, N2, N3])\n",
    "\n",
    "# Visualization\n",
    "scatter(observations[cluster .== 1, 1], observations[cluster .== 1, 2], marker=:circle, color=\"black\", label=\"C1\")\n",
    "scatter!(observations[cluster .== 2, 1], observations[cluster .== 2, 2], marker=:xcross, color=\"black\", label=\"C2\")\n",
    "scatter!(observations[cluster .== 3, 1], observations[cluster .== 3, 2], \n",
    "         marker=:diamond,\n",
    "         color=\"black\", \n",
    "         label=\"C3\", \n",
    "         xlabel=L\"$x_1$\", \n",
    "         ylabel=L\"$x_2$\", \n",
    "         xlim=[-6, 6], \n",
    "         ylim=[-6, 6], \n",
    "         size=(500,500),\n",
    "         grid=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model specification\n",
    "\n",
    "Let's quickly recap the $K$-component Gaussian mixture model. \n",
    "\n",
    "We have a data set of $X$ of $N$ samples by $D$ features. Each component is a Gaussian distribution, $ p_k(x \\mid \\mu_k, \\Sigma_k) \\triangleq \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)$. In total, we model $N$ samples:\n",
    "\n",
    "$$ p(X \\mid z, \\mu, \\Sigma) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} p_k(x_i \\mid \\mu_k, \\Sigma_k)^{z_i=k} \\, .$$\n",
    "\n",
    "<!-- Note that we summarize parameters as follows: $\\phi = (\\phi_1, \\dots, \\phi_K), \\mu = (\\mu_1, \\dots, \\mu_K), \\Sigma = (\\Sigma_1, \\dots, \\Sigma_K)$.  -->\n",
    "The variable $z_i$ is the assignment of sample $i$ to class $k$. The probability $p(z_{i} = k)$ is also called the _responsibility_ of component $k$ for the $i$-th sample. All $z_i$ follow a Categorical distribution with mixture weights as parameters:\n",
    "\n",
    "$$ p(z_i) \\sim \\text{Cat}(\\phi) \\, ,$$\n",
    "\n",
    "Being Bayesians, we put priors on all unknown parameters ($\\phi, \\mu, \\Sigma$):\n",
    "\n",
    "$$ \\begin{align}\n",
    "p(\\phi) \\sim&\\ \\text{Dir}(a_0) \\\\\n",
    "p(\\mu) \\sim&\\ \\mathcal{N}(m_0, l_0^{-1}\\Sigma) \\\\\n",
    "p(\\Sigma) \\sim&\\ \\mathcal{W}(W_0, v_0) \\end{align} \\, .$$\n",
    "\n",
    "The distribution for the mixture weights, $p(\\phi)$, follows a Dirichlet distribution because it is a vector that should sum to one. The component parameter priors are a Wishart distribution for the covariance matrix and a Gaussian distribution for the mean, conditioned on the sampled covariance matrix. Together, these two priors form a [Normal-Inverse-Wishart distribution](https://en.wikipedia.org/wiki/Normal-inverse-Wishart_distribution), $p(\\mu, \\Sigma)$, which is the conjugate prior for the Gaussian components. The full joint distribution is therefore:\n",
    "\n",
    "$$ \\begin{align} p(X, z, \\phi, \\mu, \\Sigma) =&\\ p(X \\mid z, \\phi, \\mu, \\Sigma)\\ p(z \\mid \\phi)\\ p(\\mu, \\Sigma)\\ p(\\phi) \\\\\n",
    "=&\\ \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)^{z_i=k}\\ \\text{Cat}(z_i=k \\mid \\phi_k)\\ \\mathcal{NIW}(\\mu_k, \\Sigma_k \\mid m_{0k}, l_{0k}, W_{0k}, v_{0k})\\ \\text{Dir}(\\phi_k \\mid a_{0k}) \\end{align}$$\n",
    "\n",
    "We are interested in the posterior distribution over $z$, which assigns the current samples to clusters via $\\arg \\max_{k} p(z_i = k)$. For example, if the posterior distribution for $z_i$ is $[0.32\\ 0.21\\ 0.47]$, then the $\\arg \\max_k$ is 3 and we say that $x_i$ belongs to component 3. We are also interested in the posterior distributions for the component parameters, because these allow us to assign future samples to the current clusters. The posterior for $z, \\phi, \\mu, \\Sigma$ has the form:\n",
    "\n",
    "$$p(z, \\phi, \\mu, \\Sigma \\mid X) = \\frac{p(X, z, \\phi, \\mu, \\Sigma)}{\\int p(X, z, \\phi, \\mu, \\Sigma)\\ \\mathrm{d}X} \\, .$$\n",
    "\n",
    "We cannot evaluate the integral in the denominator, but because it only serves as a normalization factor we ignore it for now. Note that the numerator is the joint distribution. Unfortunately, it doesn't simplify to a known form. However, in mixture models, it is usually possible to perform a conditional factorization into the two distributions: if we fix the parameters $\\phi, \\mu, \\Sigma$ to certain values, we obtain the factor:\n",
    "\n",
    "$$p(z_i \\mid X, \\phi, \\mu, \\Sigma) = \\sum_{i=1}^{N} \\sum_{k=1}^{K} \\rho_k^{z_i = k} \\, ,$$\n",
    "\n",
    "where $\\rho_k = \\phi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)$ for all $k$. This is a Categorical distribution! The only problem is that $\\rho$ is not normalized correctly. But we know that it _should_ be correctly normalized, after division by the denominator $\\int p(X, z, \\phi, \\mu, \\Sigma) \\ \\mathrm{d}X$. We can therefore perform the following re-parameterization:\n",
    "\n",
    "$$r_k = \\frac{\\rho_k}{\\sum_{k} \\rho_k} \\, .$$\n",
    "\n",
    "Now, $r_k$ are valid responsibilities for the assignment variable $z$. In other words, $p(z_i \\mid X, \\phi, \\mu, \\Sigma) \\sim \\text{Cat}(r_k)$. \n",
    "\n",
    "If we fix $z_i$ to certain values, it acts as a component selector. Take for instance $z_i = 2$;\n",
    "\n",
    "$$\\begin{align} p(\\phi_2, \\mu_2, \\Sigma_2 \\mid x_i, z_i) =&\\quad \\mathcal{N}(x_i \\mid \\mu_1, \\Sigma_1)^{0} \\ 0\\ \\ \\ \\mathcal{NIW}(\\mu_1, \\Sigma_1 \\mid m_{01}, l_{01}, W_{01}, v_{01}) \\ \\text{Dir}(\\phi_1 \\mid a_{01}) \\\\\n",
    "&+ \\mathcal{N}(x_i \\mid \\mu_2, \\Sigma_2)^{1} \\ \\phi_2\\ \\mathcal{NIW}(\\mu_2, \\Sigma_2 \\mid m_{02}, l_{02}, W_{02}, v_{02}) \\ \\text{Dir}(\\phi_2 \\mid a_{02}) \\\\\n",
    "&+ \\mathcal{N}(x_i \\mid \\mu_3, \\Sigma_3)^{0} \\ 0\\ \\ \\ \\mathcal{NIW}(\\mu_3, \\Sigma_3 \\mid m_{03}, l_{03}, W_{03}, v_{03}) \\ \\text{Dir}(\\phi_3 \\mid a_{03}) \\\\\n",
    "=& \\ \\mathcal{N}(x_i \\mid \\mu_2, \\Sigma_2) \\ \\phi_2\\ \\mathcal{NIW}(\\mu_2, \\Sigma_2 \\mid m_{02}, l_{02}, W_{02}, v_{02}) \\ \\text{Dir}(\\phi_2 \\mid a_{02})  \\end{align} \\, .$$\n",
    "\n",
    "Fixing all $z_i$ produces $p(\\phi, \\mu, \\Sigma \\mid X, z)$. This can be further factorized into $p(\\phi \\mid z) p(\\mu, \\Sigma \\mid X)$:\n",
    "\n",
    "$$\\begin{align} \n",
    "p(\\phi_k \\mid z) =&\\ \\sum_{i=1}^{N} \\text{Cat}(z_i \\mid \\phi_k)\\ \\text{Dir}(\\phi_k \\mid a_{0}) \\\\ \n",
    "p(\\mu_k, \\Sigma_k \\mid X) =&\\ \\sum_{i=1}^{N} \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\\ \\mathcal{NIW}(\\mu_k, \\Sigma_k \\mid m_{0k}, l_{0k}, W_{0k}, v_{0k}) \n",
    "\\end{align}$$\n",
    "\n",
    "which are Dirichlet and a Normal-Inverse-Wishart distributions. So, to summarize:\n",
    "\n",
    "1. Fix $\\phi, \\mu, \\Sigma$ and compute $p(z \\mid X, \\phi, \\mu, \\Sigma)$\n",
    "2. Fix $z$ and compute both $p(\\phi \\mid z)$ and $p(\\mu, \\Sigma \\mid X)$.\n",
    "\n",
    "You will recognize this as being similar to Expectation-Maximization. The difference lies in that we obtain posterior distributions for $\\phi, \\mu, \\Sigma$ instead of point estimates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the posterior factorizes into marginal distributions into known forms (i.e. Categorical, Dirichlet, Normal, Wishart), we can approximate it using an appropriate recognition distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a graph\n",
    "g = FactorGraph()\n",
    "\n",
    "# Mixture weights are drawn from a Beta distribution (1D analog of a Dirichlet)\n",
    "@RV phi ~ Beta(1.0, 1.0)\n",
    "\n",
    "# Marginals for first component\n",
    "@RV W_1 ~ Wishart([1. 0.; 0. 1.], 2.)\n",
    "@RV m_1 ~ GaussianMeanPrecision([0.0, 0.0], W_1)\n",
    "\n",
    "# Marginals of second component\n",
    "@RV W_2 ~ Wishart([1. 0.; 0. 1.], 2.)\n",
    "@RV m_2 ~ GaussianMeanPrecision([0.0, 0.0], W_2)\n",
    "\n",
    "z = Vector{Variable}(undef, N)\n",
    "y = Vector{Variable}(undef, N)\n",
    "for i = 1:N\n",
    "    \n",
    "    # Assignment variable is drawn from a Bernoulli distribution (1D analog of a Categorical)\n",
    "    @RV z[i] ~ Bernoulli(phi)\n",
    "    \n",
    "    # ForneyLab has a special node for a Gaussian mixture component\n",
    "    @RV y[i] ~ GaussianMixture(z[i], m_1, W_1, m_2, W_2)\n",
    "    \n",
    "    # Add data \n",
    "    placeholder(y[i], :y, dims=(2,), index=i)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to specify a distribution with which you will approximate the posterior distribution that you would normally obtain through exact Bayesian inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify recognition factorization (mean-field)\n",
    "q = RecognitionFactorization(phi, m_1, W_1, m_2, W_2, z, ids=[:PHI, :M1, :W1, :M2, :W2, :Z])\n",
    "\n",
    "# Generate the algorithm\n",
    "algo = variationalAlgorithm(q)\n",
    "algo_F = freeEnergyAlgorithm(q);\n",
    "\n",
    "# Load algorithms\n",
    "eval(Meta.parse(algo))\n",
    "eval(Meta.parse(algo_F));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we execute the inference algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to a format suited to ForneyLab\n",
    "observed_ = [observations[i,:] for i in 1:N]\n",
    "\n",
    "# Add to data dictionary\n",
    "data = Dict(:y => observed_)\n",
    "\n",
    "# Prepare recognition distributions\n",
    "marginals = Dict(:phi => ProbabilityDistribution(Beta, a=1.0, b=1.0),\n",
    "                 :m_1 => ProbabilityDistribution(Multivariate, GaussianMeanPrecision, m=[-1.0, +1.0], w=[1. 0.;0. 1.]),\n",
    "                 :W_1 => ProbabilityDistribution(MatrixVariate, Wishart, v=[1. 0.;0. 1.], nu=2.),\n",
    "                 :m_2 => ProbabilityDistribution(Multivariate, GaussianMeanPrecision, m=[+1.0, -1.0], w=[1. 0.;0. 1.]),\n",
    "                 :W_2 => ProbabilityDistribution(MatrixVariate, Wishart, v=[1. 0.;0. 1.], nu=2.))\n",
    "for i = 1:N\n",
    "    marginals[:z_*i] = ProbabilityDistribution(Bernoulli, p=1/2.)\n",
    "end\n",
    "\n",
    "# Execute algorithm\n",
    "num_iterations = 1\n",
    "\n",
    "# Preallocate free energy tracking array\n",
    "F = Float64[]\n",
    "\n",
    "for i = 1:num_iterations\n",
    "    \n",
    "    stepZ!(data, marginals)\n",
    "    stepPHI!(data, marginals)\n",
    "    stepM1!(data, marginals)\n",
    "    stepW1!(data, marginals)\n",
    "    stepM2!(data, marginals)\n",
    "    stepW2!(data, marginals)\n",
    "        \n",
    "    # Store variational free energy for visualization\n",
    "    push!(F, freeEnergy(data, marginals))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot free energy to check for convergence\n",
    "plot(1:num_iterations, F, color=\"black\", label=\"\")\n",
    "xlabel!(\"VMP iteration\")\n",
    "ylabel!(\"Variational free energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../scripts/pp-2.jl\")\n",
    "\n",
    "# Estimated means\n",
    "μ1_estimated = mean(marginals[:m_1])\n",
    "μ2_estimated = mean(marginals[:m_2])\n",
    "Σ1_estimated = mean(marginals[:W_1])\n",
    "Σ2_estimated = mean(marginals[:W_2])\n",
    "\n",
    "# Plot data and overlay estimated posterior probabilities\n",
    "plot_posteriors(observations, μ1_estimated, μ2_estimated, Σ1=Σ1_estimated, Σ2=Σ2_estimated, x1=range(-10,stop=10), x2=range(-10,stop=10))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia-1.3 1.3.1",
   "language": "julia",
   "name": "julia-1.3-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
