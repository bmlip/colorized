{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discriminative Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to discriminative classification models\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 213-220 \n",
    "    - [T. Minka (2005), Discriminative models, not discriminative training](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Minka-2005-Discriminative-models-not-discriminative-training.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Challenge: difficult class-conditional data distribitions\n",
    "\n",
    "Our task will be the same as in the preceding class on (generative) classification. But this time, the class-conditional data distributions look very non-Gaussian, yet the linear discriminative boundary looks easy enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/bert/.julia/compiled/v1.2/PyPlot/oatAj.ji for PyPlot [d330b81b-6aea-500a-939a-2ce795aea3ee]\n",
      "└ @ Base loading.jl:1240\n",
      "┌ Info: Recompiling stale cache file /Users/bert/.julia/compiled/v1.2/Distributions/xILW0.ji for Distributions [31c24e10-a181-5473-b8eb-7969acd0382f]\n",
      "└ @ Base loading.jl:1240\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate dataset {(x1,y1),...,(xN,yN)}\n",
    "# x is a 2-d feature vector [x_1;x_2]\n",
    "# y ∈ {false,true} is a binary class label\n",
    "# p(x|y) is multi-modal (mixture of uniform and Gaussian distributions)\n",
    "using PyPlot\n",
    "include(\"../scripts/lesson8_helpers.jl\")\n",
    "N = 200\n",
    "X, y = genDataset(N) # Generate data set, collect in matrix X and vector y\n",
    "X_c1 = X[:,findall(.!y)]'; X_c2 = X[:,findall(y)]' # Split X based on class label\n",
    "X_test = [3.75; 1.0] # Features of 'new' data point\n",
    "function plotDataSet()\n",
    "    plot(X_c1[:,1], X_c1[:,2], \"bx\", markersize=8)\n",
    "    plot(X_c2[:,1], X_c2[:,2], \"r+\", markersize=8, fillstyle=\"none\")\n",
    "    plot(X_test[1], X_test[2], \"ko\")   \n",
    "    xlabel(L\"x_1\"); ylabel(L\"x_2\"); \n",
    "    legend([L\"y=0\", L\"y=1\",L\"y=?\"], loc=2)\n",
    "    xlim([-2;10]); ylim([-4, 8])\n",
    "end\n",
    "plotDataSet();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Main Idea of Discriminative Classification \n",
    "\n",
    "- Again, a data set is given by  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$ with $x_n \\in \\mathbb{R}^D$ and $y_n \\in \\mathcal{C}_k$, with $k=1,\\ldots,K$.\n",
    "\n",
    "-  Sometimes, the precise assumptions of the (multinomial-Gaussian) generative model $$p(x_n,y_n\\in\\mathcal{C}_k|\\theta) =  \\pi_k \\cdot \\mathcal{N}(x_n|\\mu_k,\\Sigma)$$ clearly do not match the data distribution.\n",
    "\n",
    "- Here's an **IDEA**! Let's model the posterior $$p(y_n\\in\\mathcal{C}_k|x_n)$$  *directly*, without any assumptions on the class densities.\n",
    "\n",
    "- Of course, this implies also that we build direct models for the **discrimination boundaries** \n",
    "  $$\\log \\frac{p(y_n\\in\\mathcal{C}_k|x_n)}{p(y_n\\in\\mathcal{C}_j|x_n)} \\overset{!}{=} 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Logistic Regression\n",
    "\n",
    "- We will work this idea out for a 2-class problem. Assume a data set is given by  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$ with $x_n \\in \\mathbb{R}^D$ and $y_n \\in \\{0,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Model Specification\n",
    "\n",
    "- What model should we use for the posterior distribution $p(y_n \\in \\mathcal{C}_k|x_n)$?\n",
    "\n",
    "- In Bayesian Logistic Regression, we take inspiration from the generative approach, where the **logistic function** (softmax for multi-class problems) \"emerged\" as the posterior. Here, we **choose** the familiar logistic structure with linear discrimination bounderies for the posterior class probability\n",
    "$$\n",
    "p(y_n =1 \\,|\\, x_n, w) = \\sigma(w^T x_n) \\,.\n",
    "$$\n",
    "where $$\\sigma(a) = \\frac{1}{1+e^{-a}}$$ is the _logistic_ function.\n",
    "\n",
    "- <font color=\"red\"> Add a plot of the logistic function</font>\n",
    "\n",
    "- Adding the other class ($y_n=0$) leads to the following posterior class distribution:\n",
    "$$\\begin{align*}\n",
    "p(y_n \\,|\\, x_n, w) &=  \\sigma(w^T x_n)^{y_n} \\left(1 - \\sigma(w^T x_n)\\right)^{(1-y_n)} \\tag{B-4.89} \\\\\n",
    "  &= \\sigma\\left( (2y_n-1) w^T x_n\\right) \\\\\n",
    "  &= \\mathrm{Bernoulli}\\left(y_n \\,|\\, \\sigma(w^T x_n) \\right) \n",
    "\\end{align*}$$\n",
    "  - Note that for the 2nd equality, we have made use of the fact that $\\sigma(-a) = 1-\\sigma(a)$.\n",
    "  - (Each of these three models in B-4.89 are equivalent. We mention all three notational options since they all appear in the literature).  \n",
    "  \n",
    "- Note that in this model specification, we do not impose a Gaussian structure on the class features. In the discriminative approach, the parameters $w$ are **not** structured into $\\{\\mu,\\Sigma,\\pi \\}$. This provides discriminative approach with more flexibility than the generative approach. \n",
    "  \n",
    "- In *Bayesian* logistic regression, we add a **Gaussian prior on the weights**: \n",
    "$$\\begin{align*}\n",
    "p(w) = \\mathcal{N}(w \\,|\\, m_0, S_0) \\tag{B-4.140}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### <a id=\"#logistic-regression-posterior\">Inference</a>\n",
    "\n",
    "- The posterior for the weights follows by Bayes rule\n",
    "$$\\begin{align*}\n",
    "\\underbrace{p(w \\,|\\, D)}_{\\text{posterior}} \\propto  \\underbrace{\\prod_{n=1}^N \\sigma\\left( (2y_n-1) w^T x_n\\right)}_{\\text{likelihood}} \\cdot \\underbrace{\\mathcal{N}(w \\,|\\, m_0, S_0)}_{\\text{prior}} \\tag{B-4.142}\n",
    "\\end{align*}$$\n",
    "\n",
    "- In principle, Bayesian inference is done now. Unfortunately, the posterior is not Gaussian and the evidence $p(D)$ is also not analytically computable. (We will deal with this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Predictive distribution\n",
    "\n",
    "- For a new data point $x_\\bullet$, the predictive distribution for $y_\\bullet$ is given by \n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &= \\int p(y_\\bullet = 1 \\,|\\, x_\\bullet, w) \\, p(w\\,|\\, D) \\,\\mathrm{d}w \\\\\n",
    "  &= \\int \\sigma(w^T x_\\bullet) \\, p(w\\,|\\, D) \\,\\mathrm{d}w \\tag{B-4.145}\n",
    "\\end{align*}$$\n",
    "\n",
    "- After substitution of $p(w | D)$ from B-4.142, we have an integral that is not solvable in closed-form. \n",
    "\n",
    "- Many methods have been developed to approximate the integrals for the predictive distribution and evidence. Here, we present the **Laplace approximation**, which is one of the simplest methods with broad applicability to Bayesian calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Laplace Approximation\n",
    "\n",
    "- The central idea of the Laplace approximation is to approximate a (possibly unnormalized) distribution $f(z)$ by a Gaussian distribution $q(z)$. \n",
    "\n",
    "- Note that $\\log q(z)$ is a second order polynomial in $z$, so we will find the Gaussian by fitting a parabola to $\\log f(z)$. \n",
    "\n",
    "- <font color=\"red\"> Needs an image</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### estimation of mean \n",
    "\n",
    "- The mean ($z_0$) of $q(z)$ is placed on the mode of $\\log f(z)$, i.e., \n",
    "\n",
    "$$z_0 = \\arg\\max_z \\log f(z) \\tag{B-4.126}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### estimation of variance\n",
    "\n",
    "- Since the gradient $\\nabla \\left. f(z) \\right|_{z=z_0}$ vanishes at the mode, we can (Taylor) expand $\\log f(z)$ around $z=z_0$ as \n",
    "$$\n",
    "\\log f(z) \\approx \\log f(z_0) - \\frac{1}{2} (z-z_0)^T A (z-z_0) \\tag{B-4.131}\n",
    "$$\n",
    "where the [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix) $A$ is defined by\n",
    "$$\n",
    "A = - \\nabla \\nabla \\left. \\log f(z) \\right|_{z=z_0} \\tag{B-4.132}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Laplace approximation\n",
    "\n",
    "- After taking exponentials in eq. B-4.131, we obtain\n",
    "\n",
    "$$\n",
    "f(z) \\approx f(z_0) \\exp\\left( - \\frac{1}{2} (z-z_0)^T A (z-z_0)\\right) \n",
    "$$\n",
    "\n",
    "- We can now identify $q(z)$ as\n",
    "$$\n",
    "q(z) = \\mathcal{N}\\left( z\\,|\\,z_0, A^{-1}\\right) \\tag{B-4.134}\n",
    "$$\n",
    "with $z_0$ and $A$ defined by eqs. B-4.126 and B-4.132.\n",
    "\n",
    "\n",
    "- <font color=\"red\"> insert fig 4.14</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Logistic Regression with the Laplace Approximation\n",
    "\n",
    "- Let's get back to the challenge of computing the predictive class distribution (B-4.145) for Bayesian logistic regression. We first work out the Gaussian Laplace approximation $q(w)$ to the [posterior weight distribution](#logistic-regression-posterior) \n",
    "$$\\begin{align*}\n",
    "\\underbrace{p(w | D)}_{\\text{posterior}} \\propto  \\underbrace{\\prod_{n=1}^N \\sigma\\left( (2y_n-1) w^T x_n\\right)}_{\\text{likelihood}} \\cdot \\underbrace{\\mathcal{N}(w \\,|\\, m_0, S_0)}_{\\text{prior}} \\tag{B-4.142}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### A Gausian Laplace approximation to the weights posterior \n",
    "\n",
    "- It is straightforward to compute the gradient and Hessian of $\\log p(w | D)$:\n",
    "$$\\begin{align*}\n",
    "\\nabla_w \\log p(w | D) &= S_0^{-1}\\cdot \\left(m_0-w\\right) + \\sum_n (2y_n-1) (1-\\sigma_n) x_n \\\\\n",
    "\\nabla_w \\nabla_w \\log p(w | D) &= -S_0^{-1} - \\sum_n \\sigma_n (1-\\sigma_n) x_n x_n^T \\tag{B-4.143}\n",
    "\\end{align*}$$\n",
    "where we used shorthand $\\sigma_n$ for $\\sigma\\left( (2y_n-1) w^T x_n\\right)$. \n",
    "\n",
    "- We can use the gradient to find the mode $w_{\\text{MAP}}$ of $\\log p(w|D)$ and use the Hessian to get the variance of $q(w)$, leading to a <a id=\"Laplace-posterior-logistic-regression\">**Gaussian approximate weights posterior**</a>:\n",
    "\n",
    "$$\\begin{align*}\n",
    "q(w) &= \\mathcal{N}\\left(w\\,|\\, w_{\\text{MAP}}, S_N\\right) \\tag{B-4.144}\\\\\n",
    "S_N^{-1} &= S_0^{-1} + \\sum_n \\sigma_n (1-\\sigma_n) x_n x_n^T \\tag{B-4.143}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "##### Using the Laplace weights posterior to evaluate the predictive distribution \n",
    "\n",
    "- In the analytically unsolveable expressions for evidence and the predictive distribution (estimating the class of a new observation), we proceed with using the Laplace approximation to the weights posterior. For a new observation $x_\\bullet$, the class probability is now\n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &\\approx \\int p(y_\\bullet = 1 \\,|\\, x_\\bullet, w) \\cdot \\underbrace{q(w)}_{\\text{Gaussian}} \\,\\mathrm{d}w \\\\\n",
    "  &= \\int \\sigma(w^T x_\\bullet) \\cdot \\mathcal{N}\\left(w\\,|\\, w_{\\text{MAP}}, S_N\\right) \\,\\mathrm{d}w \\tag{B-4.145}\n",
    "\\end{align*}$$\n",
    "\n",
    "- This looks better but we need two more clever tricks to evaluate this expression. \n",
    "  1. First, note that $w$ only appears in inner products, so through substitution of $a:=w^T x_\\bullet$, the expression simplifies to an integral over the scalar $a$ (see Bishop for derivation):\n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &\\approx \\int \\sigma(a) \\, \\mathcal{N}\\left(a\\,|\\, \\mu_a, \\Sigma_a\\right) \\,\\mathrm{d}a \\tag{B-4.151}\\\\\n",
    "\\mu_a  &= w^T_{\\text{MAP}} x_\\bullet \\tag{B-4.149}\\\\\n",
    "\\Sigma_a &= x^T_\\bullet S_N x_\\bullet \\tag{B-4.150}\n",
    "\\end{align*}$$\n",
    "  1. Secondly, while the integral of the product of a logistic function with a Gaussian is not analytically solvable, the integral of the product of a Gaussian CDF (cumulative distribution function) with a Gaussian _does_ have a closed-form solution. Fortunately, \n",
    "$$\\Phi(\\lambda a) \\approx \\sigma(a)$$\n",
    "with the Gaussian CDF $\\Phi(x)= \\frac{1}{\\sqrt(2\\pi)}\\int_{-\\infty}^{x}e^{-t^2/2}\\mathrm{d}t$, $ \\lambda^2= \\pi / 8 $ and $\\sigma(a) = 1/(1+e^{-a})$. Thus, substituting $\\Phi(\\lambda a)$ with $ \\lambda^2= \\pi / 8 $ for $\\sigma(a)$ leads to \n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &\\approx \\int \\Phi(\\lambda a) \\, \\mathcal{N}\\left(a\\,|\\, \\mu_a, \\Sigma_a\\right) \\,\\mathrm{d}a \\\\ &= \\Phi\\left( \\frac{\\mu_a}{\\sqrt(\\lambda^{-2} +\\sigma_a^2)}\\right) \\tag{B-4.152}\n",
    "\\end{align*}$$\n",
    "\n",
    "- We now have an approximate but **closed-form expression for the predictive class distribution for a new observation** with a Bayesian logistic regression model.  \n",
    "\n",
    "- Note that, by [Eq.B-4.143](#Laplace-posterior-logistic-regression), the variance $S_N$ (and consequently $\\sigma_a^2$) for the weight vector depends on the distribution of the training set. Large uncertainty about the weights (in areas with little training data and uninformative prior variance $S_0$) takes the posterior class probability eq. B-4.152 closer to $0.5$. Does that make sense?\n",
    "\n",
    "- Apparently, the Laplace approximation leads to a closed-form solutions for Bayesian logistic regression (although admittedly, the derivation is no walk in the park). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  2. ML Estimation for Discriminative Classification\n",
    " \n",
    "- TODO TODO ### <font color=\"red\">Derive ML frmo Bayesian approach</font>\n",
    "\n",
    "-  The conditional log-likelihood for discriminative classification is \n",
    "\n",
    "     $$\n",
    "    \\mathrm{L}(\\theta) = \\log \\prod_n \\prod_k {p(\\mathcal{C}_k|x_n,\\theta)}^{y_{nk}} \n",
    "     $$\n",
    "\n",
    "     \n",
    "- Computing the gradient $\\nabla_{\\theta_k} \\mathrm{L}(\\theta)$ (NB: revised text) leads to (for proof, see next slide) \n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta_k} \\mathrm{L}(\\theta) = \\sum_n \\Big( \\underbrace{y_{nk}}_{\\text{target}} - \\underbrace{\\frac{e^{\\theta_k^T x_n}}{ \\sum_j e^{\\theta_j^T x_n}}}_{\\text{prediction}} \\Big)\\cdot x_n \n",
    "$$\n",
    "\n",
    "  \n",
    "- Compare this to the gradient for _linear_ regression:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathrm{L}(\\theta) =  \\sum_n \\left(y_n - \\theta^T x_n \\right)  x_n\n",
    "$$\n",
    "\n",
    "- In both cases\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathrm{L} =  \\sum_n \\left( \\text{target}_n - \\text{prediction}_n \\right) \\cdot \\text{input}_n \n",
    "$$\n",
    "\n",
    "- The parameter vector $\\theta$ for logistic regression can be estimated through iterative gradient-based adaptation. E.g. (with iteration index $i$),\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}^{(i+1)} =  \\hat{\\theta}^{(i)} + \\eta \\cdot \\left. \\nabla_\\theta   \\mathrm{L}(\\theta)  \\right|_{\\theta = \\hat{\\theta}^{(i)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "Let us perform ML estimation of $w$ on the data set from the introduction. To allow an offset in the discrimination boundary, we add a constant 1 to the feature vector $x$. We only have to specify the (negative) log-likelihood and the gradient w.r.t. $w$. Then, we use an off-the-shelf optimisation library to minimize the negative log-likelihood.\n",
    "\n",
    "We plot the resulting maximum likelihood discrimination boundary. For comparison we also plot the ML discrimination boundary obtained from the [code example in the generative Gaussian classifier lesson](https://nbviewer.jupyter.org/github/bertdv/BMLIP/blob/master/lessons/notebooks/classification/Generative-Classification.ipynb#code-generative-classification-example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: \"\\\" is not a unary operator",
     "output_type": "error",
     "traceback": [
      "syntax: \"\\\" is not a unary operator",
      ""
     ]
    }
   ],
   "source": [
    "using Optim # Optimization library\n",
    "\n",
    "y_1 = zeros(length(y))# class 1 indicator vector\n",
    "y_1[findall(y)] .= 1\n",
    "X_ext = vcat(X, ones(1, length(y))) # Extend X with a row of ones to allow an offset in the discrimination boundary\n",
    "\n",
    "# Implement negative log-likelihood function\n",
    "function negative_log_likelihood(θ::Vector)\n",
    "    # Return negative log-likelihood: -L(θ)\n",
    "    p_1 = 1.0 ./ (1.0 .+ exp.(-X_ext' * θ))   # P(C1|X,θ)\n",
    "    return -sum(log.( (y_1 .* p_1) + ((1 .- y_1).*(1 .- p_1))) ) # negative log-likelihood\n",
    "end\n",
    "\n",
    "# Use Optim.jl optimiser to minimize the negative log-likelihood function w.r.t. θ\n",
    "results = optimize(negative_log_likelihood, zeros(3), LBFGS())\n",
    "θ = results.minimizer\n",
    "\n",
    "# Plot the data set and ML discrimination boundary\n",
    "plotDataSet()\n",
    "p_1(x) = 1.0 ./ (1.0 .+ exp(-([x;1.]' * θ)))\n",
    "boundary(x1) = -1 ./ θ[2] * (θ[1]*x1 .+ θ[3])\n",
    "plot([-2.;10.], boundary([-2.; 10.]), \"k-\");\n",
    "# # Also fit the generative Gaussian model from lesson 7 and plot the resulting discrimination boundary for comparison\n",
    "generative_boundary = buildGenerativeDiscriminationBoundary(X, y)\n",
    "plot([-2.;10.], generative_boundary([-2;10]), \"k:\");\n",
    "legend([L\"y=0\";L\"y=1\";L\"y=?\";\"Discr. boundary\";\"Gen. boundary\"], loc=3);\n",
    "\n",
    "Given $\\hat{\\theta}$, we can classify a new input $x_\\bullet = [3.75, 1.0]^T$:\n",
    "\n",
    "x_test = [3.75;1.0]\n",
    "println(\"P(C1|x•,θ) = $(p_1(x_test))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The generative model gives a bad result because the feature distribution of one class is clearly non-Gaussian: the model does not fit the data well. \n",
    "\n",
    "- The discriminative approach does not suffer from this problem because it makes no assumptions about the feature distribition $p(x|y)$, it just estimates the conditional class distribution $p(y|x)$ directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap Classification\n",
    "\n",
    "<table>\n",
    "<tr> <td></td><td style=\"text-align:center\"><b>Generative</b></td> <td style=\"text-align:center\"><b>Discriminative</b></td> </tr> \n",
    "\n",
    "<tr> <td>1</td><td>Like <b>density estimation</b>, model joint prob.\n",
    "$$p(\\mathcal{C}_k) p(x|\\mathcal{C}_k) = \\pi_k \\mathcal{N}(\\mu_k,\\Sigma)$$</td> <td>Like (linear) <b>regression</b>, model conditional\n",
    "$$p(\\mathcal{C}_k|x,\\theta)$$</td> </tr>\n",
    "\n",
    "<tr> <td>2</td><td>Leads to <b>softmax</b> posterior class probability\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) = e^{\\theta_k^T x}/Z$$\n",
    "with <b>structured</b> $\\theta$</td> <td> <b>Choose</b> also softmax posterior class probability\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) = e^{\\theta_k^T x}/Z$$\n",
    "but now with 'free' $\\theta$</td> </tr>\n",
    "\n",
    "<tr> <td>3</td><td>For Gaussian $p(x|\\mathcal{C}_k)$ and multinomial priors,\n",
    "$$\\hat \\theta_k  = \\left[ {\\begin{array}{c}\n",
    "   { - \\frac{1}{2} \\mu_k^T \\sigma^{-1} \\mu_k  + \\log \\pi_k}  \\\\\n",
    "   {\\sigma^{-1} \\mu_k }  \\\\\n",
    "\\end{array}} \\right]$$\n",
    "<b>in one shot</b>.</td> <td>Find $\\hat\\theta_k$ through gradient-based adaptation\n",
    "$$\\nabla_{\\theta_k}\\mathrm{L}(\\theta) = \\sum_n \\Big( y_{nk} - \\frac{e^{\\theta_k^T x_n}}{\\sum_{k^\\prime} e^{\\theta_{k^\\prime}^T x_n}} \\Big)\\, x_n$$ </td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OPTIONAL MATERIALS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Application - Classify a new input\n",
    "\n",
    "-  Discriminative model-based prediction for a new input $x_\\bullet$ is easy, namely substitute the ML estimate in the model to get\n",
    "\n",
    "$$\n",
    "p(\\mathcal{C}_k |\\, x_\\bullet,\\hat\\theta) = \\frac{ \\mathrm{exp}\\left( \\hat \\theta_k^T x_\\bullet \\right) }{ \\sum_{k^\\prime} \\mathrm{exp}\\left(\\hat \\theta_{k^\\prime}^T x_\\bullet \\right)} \n",
    "  \\propto \\mathrm{exp}\\left(\\hat \\theta_k^T x_\\bullet\\right) \n",
    "$$\n",
    "\n",
    "-  The contours of equal probability (**discriminant boundaries**) are lines (hyperplanes) in feature space given by\n",
    "$$\n",
    "\\log \\frac{{p(\\mathcal{C}_k|x,\\hat \\theta )}}{{p(\\mathcal{C}_j|x,\\hat \\theta )}} = \\left( \\hat{\\theta}_{k} - \\hat{\\theta}_j\\right) ^T x = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  2. <span style=\"color:red\">(OPTIONAL)</span> Proof of Derivative of Log-likelihood for  Discriminative Classification\n",
    "\n",
    "\n",
    "- The Log-likelihood is $\n",
    "    \\mathrm{L}(\\theta) = \\log \\prod_n \\prod_k {\\underbrace{p(\\mathcal{C}_k|x_n,\\theta)}_{p_{nk}}}^{y_{nk}} = \\sum_{n,k} y_{nk} \\log p_{nk}$\n",
    "\n",
    "     \n",
    "- Use the fact that the softmax $\\phi_k \\equiv e^{a_k} / {\\sum_j e^{a_j}}$ has analytical derivative:\n",
    "\n",
    "$$ \\begin{align*}\n",
    " \\frac{\\partial \\phi_k}{\\partial a_j} &= \\frac{(\\sum_j e^{a_j})e^{a_k}\\delta_{kj}-e^{a_j}e^{a_k}}{(\\sum_j e^{a_j})^2} = \\frac{e^{a_k}}{\\sum_j e^{a_j}}\\delta_{kj} - \\frac{e^{a_j}}{\\sum_j e^{a_j}} \\frac{e^{a_k}}{\\sum_j e^{a_j}}\\\\\n",
    "     &= \\phi_k \\cdot(\\delta_{kj}-\\phi_j)\n",
    " \\end{align*}$$\n",
    "\n",
    "<!---\n",
    "%    -  Again we try to minimize the cross-entropy ($\\sum_{nk} y_{nk} \\log \\frac{y_{nk}}{p_{nk}}$) between the data `targets' $t_{nk}$ and the model outputs $p_{nk}$.\n",
    "--->\n",
    "\n",
    " -  Take the derivative of $\\mathrm{L}(\\theta)$ (or: how to spend a hour ...)\n",
    "$$\\begin{align*} \n",
    "\\nabla_{\\theta_j} \\mathrm{L}(\\theta) &= \\sum_{n,k} \\frac{\\partial \\mathrm{L}_{nk}}{\\partial p_{nk}} \\cdot\\frac{\\partial p_{nk}}{\\partial a_{nj}}\\cdot\\frac{\\partial a_{nj}}{\\partial \\theta_j} \\\\\n",
    "  &= \\sum_{n,k} \\frac{y_{nk}}{p_{nk}} \\cdot p_{nk} (\\delta_{kj}-p_{nj}) \\cdot x_n \\\\\n",
    "  &= \\sum_n \\Big( y_{nj} (1-p_{nj}) -\\sum_{k\\neq j} y_{nk} p_{nj} \\Big) \\cdot x_n \\\\\n",
    "  &= \\sum_n \\left( y_{nj} - p_{nj} \\right)\\cdot x_n \\\\\n",
    "  &= \\sum_n \\Big( \\underbrace{y_{nj}}_{\\text{target}} - \\underbrace{\\frac{e^{\\theta_j^T x_n}}{\\sum_{j^\\prime} e^{\\theta_{j^\\prime}^T x_n}}}_{\\text{prediction}} \\Big)\\cdot x_n \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Evidence Estimation with the Laplace Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "footer.hidden-print {\n",
       "    display: none !important;\n",
       "}\n",
       "    \n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", read(f,String))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
