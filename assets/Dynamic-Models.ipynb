{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dynamic Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to dynamic (=temporal) Latent Variable Models, including the Hidden Markov Model and Kalman filter.   \n",
    "- Materials\n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional \n",
    "    - Bishop pp.605-615 on Hidden Markov Models\n",
    "    - Bishop pp.635-641 on Kalman filters\n",
    "    - Faragher (2012), [Understanding the Basis of the Kalman Filter](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Faragher-2012-Understanding-the-Basis-of-the-Kalman-Filter.pdf)\n",
    "    - Minka (1999), [From Hidden Markov Models to Linear Dynamical Systems](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Minka-1999-from-HMM-to-LDS.pdf)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example Problem\n",
    "\n",
    "- We consider a one-dimensional cart position tracking problem, see [Faragher 2012](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Faragher-2012-Understanding-the-Basis-of-the-Kalman-Filter.pdf).  \n",
    "\n",
    "- The hidden states are the position $z_t$ and velocity $\\dot z_t$. We can apply an external acceleration/breaking force $u_t$. (Noisy) observations are represented by $x_t$. \n",
    "\n",
    "- The equations of motions are given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\begin{bmatrix} z_t \\\\ \\dot{z_t}\\end{bmatrix} &=  \\begin{bmatrix} 1 & \\Delta t \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix} z_{t-1} \\\\ \\dot z_{t-1}\\end{bmatrix} + \\begin{bmatrix} (\\Delta t)^2/2 \\\\ \\Delta t\\end{bmatrix} u_t + \\mathcal{N}(0,\\Sigma_z) \\\\\n",
    "x_t &= \\begin{bmatrix} z_t \\\\ \\dot{z_t}\\end{bmatrix} + \\mathcal{N}(0,\\Sigma_x) \n",
    "\\end{align*}$$\n",
    "\n",
    "- Task: Infer the position $z_t$ after 10 time steps. (Solution later in this lesson).\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/Faragher-2012-cart-1.png\" width=\"600px\"><p style=\"text-align:center;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dynamical Models\n",
    "\n",
    "- In this lesson, we consider models where the sequence order of observations matters. \n",
    "\n",
    "- Consider the _ordered_ observation sequence $x^T \\triangleq \\left(x_1,x_2,\\ldots,x_T\\right)$.\n",
    "  - (For brevity, in this lesson we use the notation $x_t^T$ to denote $(x_t,x_{t+1},\\ldots,x_T)$ and drop the subscript if $t=1$, so $x^T = x_1^T = \\left(x_1,x_2,\\ldots,x_T\\right)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We wish to develop a generative model\n",
    "    $$ p( x^T )$$\n",
    "that 'explains' the time series $x^T$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We cannot use the IID assumption $p( x^T ) = \\prod_t p(x_t )$. In general, we _can_ use the **chain rule** (a.k.a. **the general product rule**)\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(x^T) &= p(x_T|x^{T-1}) \\,p(x^{T-1}) \\\\\n",
    "  &=  p(x_T|x^{T-1}) \\,p(x_{T-1}|x^{T-2}) \\cdots p(x_2|x_1)\\,p(x_1) \\\\\n",
    "  &= p(x_1)\\prod_{t=2}^T p(x_t\\,|\\,x^{t-1})\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Generally, we will want to limit the depth of dependencies on previous observations. For example, a $K$th-order linear **Auto-Regressive** (AR) model that is given by\n",
    "$$\\begin{align*}\n",
    "  p(x_t\\,|\\,x^{t-1}) = \\mathcal{N}\\left(x_t \\,\\middle|\\,  \\sum_{k=1}^K a_k x_{t-k}\\,,\\sigma^2\\,\\right)  \n",
    "\\end{align*}$$\n",
    "limits the dependencies to the past $K$ samples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### State-space Models\n",
    "\n",
    "- A limitation of AR models is that they need a lot of parameters in order to create a flexible model. E.g., if $x_t \\in \\mathbb{R}^M$ is an $M$-dimensional time series, then the $K$-th order AR model for $x_t$ will have $KM^2$ parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Similar to our work on Gaussian Mixture models, we can create a flexible dynamic system by introducing _latent_ (unobserved) variables  $z^T \\triangleq \\left(z_1,z_2,\\dots,z_T\\right)$ (one $z_t$ for each observation $x_t$). In dynamic systems, $z_t$ are called _state variables_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **state space model** is a particular latent variable dynamical model defined by\n",
    "$$\\begin{align*}\n",
    " p(x^T,z^T) &= \\underbrace{p(z_1)}_{\\text{initial state}} \\prod_{t=2}^T \\underbrace{p(z_t\\,|\\,z_{t-1})}_{\\text{state transitions}}\\,\\prod_{t=1}^T \\underbrace{p(x_t\\,|\\,z_t)}_{\\text{observations}}\n",
    "\\end{align*}$$\n",
    "  - The condition $p(z_t\\,|\\,z^{t-1}) = p(z_t\\,|\\,z_{t-1})$ is called a $1$st-order Markov condition.\n",
    "\n",
    "<!--- - In a Markovian state-space model, the observation sequence $x^T$ is not a first-order Markov chain, i.e., for the state-space model\n",
    "    $$\\begin{align*}\n",
    " p(x^T,z^T) &= p(z_1) \\prod_{t=2}^T p(z_t\\,|\\,z_{t-1})\\,\\prod_{t=1}^T p(x_t\\,|\\,z_t)\n",
    "\\end{align*}$$\n",
    "the following statement holds: \n",
    "    $$p(x_t\\,|\\,x_{t-1},x_{t-2}) \\neq p(x_t\\,|\\,x_{t-1})\\,.$$\n",
    "In other words, the latent variables $z_t$ represent a memory bank for past observations beyond $t-1$. (Proof as exercise). \n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The Forney-style factor graph for a state-space model:\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-state-space.png\" width=\"600px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden Markov Models and Linear Dynamical Systems\n",
    "\n",
    "- A **Hidden Markov Model** (HMM) is a specific state-space model with <span class=\"emphasis\">discrete-valued</span> state variables $z_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Typically, $z_t$ is a $K$-dimensional one-hot coded latent 'class indicator' with transition probabilities $a_{jk} \\triangleq p(z_{tk}=1\\,|\\,z_{t-1,j}=1)$, or equivalently,\n",
    "  $$p(z_t|z_{t-1}) = \\prod_{k=1}^K \\prod_{j=1}^K a_{jk}^{z_{t-1,j}\\cdot z_{tk}}$$\n",
    "which is usually accompanied by an initial state distribution $p(z_{1k}=1) = \\pi_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  \n",
    "- The classical HMM has also discrete-valued observations but in pratice any (probabilistic) observation model $p(x_t|z_t)$ may be coupled to the hidden Markov chain. \n",
    "\n",
    "<!---\n",
    "- The following figure shows the typical trellis structure of the many possible state transitions paths.   \n",
    "<img src=\"./figures/Figure13.7.png\" width=\"400px\">\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Another well-known state-space model with <span class=\"emphasis\">continuous-valued</span> state variables $z_t$ is the **(Linear) Gaussian Dynamical System** (LGDS), which is defined as\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(z_t\\,|\\,z_{t-1}) &= \\mathcal{N}\\left(\\, A z_{t-1}\\,,\\,\\Sigma_z\\,\\right) \\\\ \n",
    "p(x_t\\,|\\,z_t) &= \\mathcal{N}\\left(\\, C z_t\\,,\\,\\Sigma_x\\,\\right) \\\\\n",
    "p(z_1) &= \\mathcal{N}\\left(\\, \\mu_1\\,,\\,\\Sigma_1\\,\\right)\n",
    "\\end{align*}$$\n",
    "<!---or, equivalently (in the usual state-space notation)\n",
    "$$\\begin{align*}\n",
    "z_k &= A z_{k-1} + \\mathcal{N}\\left(0,\\Sigma_z \\right) \\\\ \n",
    "x_k &= C z_k + \\mathcal{N}\\left( 0, \\Sigma_x \\right) \\\\\n",
    "z_1 &= \\mu_1 + \\mathcal{N}\\left( 0, \\Sigma_1\\right)\n",
    "\\end{align*}$$\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Note that the joint distribution over all states and observations $\\{(x_1,z_1),\\ldots,(x_t,z_t)\\}$ is a (large-dimensional) Gaussian distribution. This means that, in principle, every inference problem on the LGDS model also leads to a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- HMM's and LGDS's (and variants thereof) are at the basis of a wide range of complex information processing systems, such as speech and language recognition, robotics and automatic car navigation, and even processing of DNA sequences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Signal Processing Tasks as Message Passing-based Inference\n",
    "\n",
    "- As we have seen, inference tasks in linear Gaussian state space models can be analytically solved.\n",
    "\n",
    "- However, these derivations quickly become cumbersome and prone to errors.\n",
    "\n",
    "- Alternatively, we could specify the generative model in a (Forney-style) factor graph and use automated message passing to infer the posterior over the hidden variables. Here follows some examples.\n",
    "\n",
    "- **Filtering**, a.k.a. state estimation: estimation of a state (at time step $t$), based on past and current (at $t$) observations. \n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-state-space-filtering.png\" width=\"600px\"></p>\n",
    "\n",
    "- **Smoothing**: estimation of a state based on both past and future observations. Needs backward messages from the future.  \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-state-space-smoothing.png\" width=\"600px\"></p>\n",
    "\n",
    "- **Prediction**: estimation of future state or observation based only on observations of the past.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-state-space-prediction.png\" width=\"600px\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <a id='kalman-filter'>Kalman Filtering</a>\n",
    "\n",
    "- Technically, a [**Kalman filter**](https://en.wikipedia.org/wiki/Kalman_filter) is the solution to the recursive estimation (inference) of the hidden state $z_t$ based on past observations in an LGDS, i.e., Kalman filtering solves the problem $p(z_t\\,|\\,x^t)$ based on the previous estimate $p(z_{t-1}\\,|\\,x^{t-1})$ and a new observation $x_t$ (in the context of the given model specification of course). \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "- Let's infer the Kalman filter for a scalar linear Gaussian dynamical system:\n",
    "$$\\begin{aligned}\n",
    "    p(z_t\\,|\\,z_{t-1}) &= \\mathcal{N}(z_t\\,|\\,a z_{t-1},\\sigma_z^2) \\qquad &&\\text{(state transition)}   \\\\\n",
    "    p(x_t\\,|\\,z_t) &= \\mathcal{N}(x_t\\,|\\,c z_t,\\sigma_x^2) \\qquad &&\\text{(observation)}     \n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "        \n",
    "- Kalman filtering comprises inferring $p(z_t\\,|\\,x^t)$ from a given prior estimate $p(z_{t-1}\\,|\\,x^{t-1})$ (available after the previous time step) and a new observation $x_t$. Let us assume that \n",
    "$$\\begin{align*} \n",
    "p(z_{t-1}\\,|\\,x^{t-1}) = \\mathcal{N}(z_{t-1} \\,|\\, \\mu_{t-1}, \\sigma_{t-1}^2) \\qquad \\text{(prior)}\n",
    "\\end{align*}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Note that everything is Gaussian, so it is _in principle_ possible to execute inference problems analytically and the result will be a Gaussian posterior:\n",
    "  - (In the following derivation we make use of the renormalization equality $\\mathcal{N}(x\\,|\\,cz,\\sigma^2) = \\frac{1}{c}\\mathcal{N}\\left(z \\,\\big|\\,\\frac{x}{c},\\left(\\frac{\\sigma}{c}\\right)^2\\right)$).\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\underbrace{p(z_t\\,|\\,x^t)}_{\\text{posterior}} &= p(z_t\\,|\\,x_t,x^{t-1}) \\\\\n",
    "  &\\propto p(x_t,z_t\\,|\\,x^{t-1}) \\\\\n",
    "  &= p(x_t\\,|\\,z_t) \\,p(z_t\\,|\\,x^{t-1}) \\\\\n",
    "  &= p(x_t\\,|\\,z_t) \\, \\int p(z_t,z_{t-1}\\,|\\,x^{t-1}) \\mathrm{d}z_{t-1} \\\\\n",
    "  &= \\underbrace{p(x_t\\,|\\,z_t)}_{\\text{observation}} \\, \\int \\underbrace{p(z_t\\,|\\,z_{t-1})}_{\\text{state transition}} \\, \\underbrace{p(z_{t-1}\\,|\\,x^{t-1})}_{\\text{prior}} \\mathrm{d}z_{t-1} \\\\\n",
    "    &= \\mathcal{N}(x_t\\,|\\,c z_t,\\sigma_x^2) \\int \\mathcal{N}(z_t\\,|\\,a z_{t-1},\\sigma_z^2) \\, \\mathcal{N}(z_{t-1} \\,|\\, \\mu_{t-1}, \\sigma_{t-1}^2) \\mathrm{d}z_{t-1} \\\\\n",
    "  &= \\frac{1}{c}\\mathcal{N}\\left(z_t\\bigm| \\frac{x_t}{c} ,\\left(\\frac{\\sigma_x}{c}\\right)^2\\right)  \\int \\frac{1}{a}\\underbrace{\\mathcal{N}\\left(z_{t-1}\\bigm|  \\frac{z_t}{a},\\left(\\frac{\\sigma_z}{a}\\right)^2 \\right) \\, \\mathcal{N}(z_{t-1} \\,|\\, \\mu_{t-1}, \\sigma_{t-1}^2)}_{\\text{use Gaussian multiplication formula SRG-6}} \\mathrm{d}z_{t-1} \\\\\n",
    "  &\\propto \\underbrace{\\mathcal{N}\\left(z_t\\,\\bigm| \\,\\frac{x_t}{c} ,\\left(\\frac{\\sigma_x}{c}\\right)^2\\right) \\cdot \\mathcal{N}\\left(z_t\\, \\bigm|\\,a \\mu_{t-1},\\sigma_z^2 + \\left(a \\sigma_{t-1}\\right)^2 \\right)}_{\\text{use SRG-6 again}} \\\\\n",
    "  &\\propto \\mathcal{N}\\left( z_t \\,|\\, \\mu_t, \\sigma_t^2\\right)\n",
    "\\end{align*}$$\n",
    "with\n",
    "$$\\begin{align*}\n",
    "  \\rho_t^2 &= a^2 \\sigma_{t-1}^2 + \\sigma_z^2 \\quad \\text{(predicted variance)}\\\\\n",
    "  K_t &= \\frac{c \\rho_t^2}{c^2 \\rho_t^2 + \\sigma_x^2} \\quad \\text{(Kalman gain)} \\\\\n",
    "  \\mu_t &= \\underbrace{a \\mu_{t-1}}_{\\text{prior prediction}} + K_t \\cdot \\underbrace{\\left( x_t - c a \\mu_{t-1}\\right)}_{\\text{prediction error}} \\quad \\text{(posterior mean)}\\\\\n",
    "  \\sigma_t^2 &= \\left( 1 - c\\cdot K_t \\right) \\rho_t^2 \\quad \\text{(posterior variance)}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Kalman filtering consists of computing/updating these last four equations for each new observation ($x_t$). This is a very efficient recursive algorithm to estimate the state $z_t$ from all observations (until $t$).\n",
    "\n",
    "- It turns out that it's also possible to get an analytical result for $p(x_t|x^{t-1})$, which is the **model evidence** in a filtering context. See [optional slides](#kalman-proof) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-dimensional Kalman Filtering\n",
    "\n",
    "- The Kalman filter equations can also be derived for multidimensional state-space models. In particular, for the model\n",
    "$$\\begin{align*}\n",
    "z_t &= A z_{t-1} + \\mathcal{N}(0,\\Gamma) \\\\\n",
    "x_t &= C z_t + \\mathcal{N}(0,\\Sigma)\n",
    "\\end{align*}$$\n",
    "the Kalman filter update equations for the posterior $p(z_t |x^t) = \\mathcal{N}\\left(z_t \\bigm| \\mu_t, V_t \\right)$ are given by (see Bishop, pg.639)\n",
    "$$\\begin{align*}\n",
    "P_t &= A V_{t-1} A^T + \\Gamma \\qquad &&\\text{(predicted variance)}\\\\\n",
    "K_t &= P_t C^T \\cdot \\left(C P_t C^T  + \\Sigma \\right)^{-1} \\qquad &&\\text{(Kalman gain)} \\\\\n",
    "\\mu_t &= A \\mu_{t-1} + K_t\\cdot\\left(x_t - C A \\mu_{t-1} \\right) \\qquad &&\\text{(posterior state mean)}\\\\\n",
    "V_t &= \\left(I-K_t C \\right) P_{t}  \\qquad &&\\text{(posterior state variance)}\n",
    "\\end{align*}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code Example: Kalman Filtering and the Cart Position Tracking Example Revisited\n",
    "\n",
    "\n",
    "- We can now solve the cart tracking problem of the introductory example by implementing the Kalman filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: MvNormalMeanCovariance(\n",
      "μ: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33.84729614450893, 2.8705150714491463]\n",
      "Σ: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.29587873356633 0.39215729555837747; 0.39215729552375644 0.3415636711872436]\n",
      ")\n",
      "\n",
      "Measurement: MvNormalMeanCovariance(\n",
      "μ: [34.13385083841329, 2.7174144561308906]\n",
      "Σ: [1.0 0.0; 0.0 2.0]\n",
      ")\n",
      "\n",
      "Posterior: MvNormalMeanCovariance(\n",
      "μ: [33.99386551826291, 2.895070850822045]\n",
      "Σ: [0.5516100295248194 0.1501897218006391; 0.15018972178737977 0.24143326064425966]\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using RxInfer, LinearAlgebra, PyPlot\n",
    "\n",
    "include(\"scripts/cart_tracking_helpers.jl\")\n",
    "\n",
    "# Specify the model parameters\n",
    "Δt = 1.0                     # assume the time steps to be equal in size\n",
    "A = [1.0 Δt;\n",
    "     0.0 1.0]\n",
    "b = [0.5*Δt^2; Δt] \n",
    "Σz = convert(Matrix,Diagonal([0.2*Δt; 0.1*Δt])) # process noise covariance\n",
    "Σx = convert(Matrix,Diagonal([1.0; 2.0]))     # observation noise covariance;\n",
    "\n",
    "# Generate noisy observations\n",
    "n = 10                # perform 10 timesteps\n",
    "z_start = [10.0; 2.0] # initial state\n",
    "u = 0.2 * ones(n)     # constant input u\n",
    "noisy_x = generateNoisyMeasurements(z_start, u, A, b, Σz, Σx);\n",
    "\n",
    "m_z = noisy_x[1]                                    # initial predictive mean\n",
    "V_z = A * (1e8*Diagonal(I,2) * A') + Σz             # initial predictive covariance\n",
    "\n",
    "for t = 2:n\n",
    "    global m_z, V_z, m_pred_z, V_pred_z\n",
    "    #predict\n",
    "    m_pred_z = A * m_z + b * u[t]                   # predictive mean\n",
    "    V_pred_z = A * V_z * A' + Σz                    # predictive covariance\n",
    "    #update\n",
    "    gain = V_pred_z * inv(V_pred_z + Σx)            # Kalman gain\n",
    "    m_z = m_pred_z + gain * (noisy_x[t] - m_pred_z) # posterior mean update\n",
    "    V_z = (Diagonal(I,2)-gain)*V_pred_z             # posterior covariance update\n",
    "end\n",
    "println(\"Prediction: \",MvNormalMeanCovariance(m_pred_z,V_pred_z))\n",
    "println(\"Measurement: \", MvNormalMeanCovariance(noisy_x[n],Σx))\n",
    "println(\"Posterior: \", MvNormalMeanCovariance(m_z,V_z))\n",
    "plotCartPrediction(m_pred_z[1], V_pred_z[1], m_z[1], V_z[1], noisy_x[n][1], Σx[1][1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Cart Tracking Problem Revisited: Inference by Message Passing\n",
    "\n",
    "- Let's solve the cart tracking problem by sum-product message passing in a factor graph like the one depicted above. All we have to do is create factor nodes for the state-transition model $p(z_t|z_{t-1})$ and the observation model $p(x_t|z_t)$. Then we let [RxInfer](https://biaslab.github.io/rxinfer-website/) execute the message passing schedule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "@model function cart_tracking(n, A, b, Σz, Σx, z_prev_m_0, z_prev_v_0, u)\n",
    "    \n",
    "    # We create constvar references for better efficiency\n",
    "    cA = constvar(A)\n",
    "    cB = constvar(b)\n",
    "    cΣz = constvar(Σz)\n",
    "    cΣx = constvar(Σx)\n",
    "    \n",
    "    znodes = Vector{Any}(undef, n)\n",
    "    # `z` is a sequence of hidden states\n",
    "    z = randomvar(n)\n",
    "    # `x` is a sequence of \"clamped\" observations\n",
    "    x = datavar(Vector{Float64}, n)\n",
    "    \n",
    "    z_prior ~ MvNormalMeanCovariance(z_prev_m_0, z_prev_v_0)\n",
    "    \n",
    "    z_prev = z_prior\n",
    "    \n",
    "    for i in 1:n\n",
    "        znodes[i],z[i] ~ MvNormalMeanCovariance(cA * z_prev + cB*u[i], cΣz)\n",
    "        x[i] ~ MvNormalMeanCovariance(z[i], cΣx)\n",
    "        z_prev = z[i]\n",
    "    end\n",
    "    \n",
    "    return z, x, znodes\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we've built the model, we can perform Kalman filtering by inserting measurement data into the model and performing message passing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: MvNormalMeanCovariance(\n",
      "μ: [33.92508876941521, 2.887326725945281]\n",
      "Σ: [1.2934227334046857 0.3916229823498387; 0.3916229823498387 0.3414332606222485]\n",
      ")\n",
      "\n",
      "Measurement: MvNormalMeanCovariance(\n",
      "μ: [34.13385083841329, 2.7174144561308906]\n",
      "Σ: [1.0 0.0; 0.0 2.0]\n",
      ")\n",
      "\n",
      "Posterior: MvNormalMeanCovariance(\n",
      "μ: [34.02739228345128, 2.898161770320011]\n",
      "Σ: [0.551150997075792 0.1501469959500068; 0.1501469959500068 0.24141815274489328]\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import RxInfer.ReactiveMP: messageout, getinterface, materialize!\n",
    "import RxInfer.Rocket: getrecent\n",
    "\n",
    "z_prev_m_0 = noisy_x[1]  \n",
    "z_prev_v_0 = A * (1e8*Diagonal(I,2) * A') + Σz ;\n",
    "result = inference(model=cart_tracking(n, A,b, Σz, Σx, z_prev_m_0, z_prev_v_0,u), data=(x=noisy_x,), free_energy=true);\n",
    "μz_posterior, Σz_posterior = mean.(result.posteriors[:z])[end], cov.(result.posteriors[:z])[end];\n",
    "prediction_z_1 =  messageout(getinterface(result.returnval[end][end], :out))\n",
    "prediction = materialize!(getrecent(prediction_z_1));\n",
    "println(\"Prediction: \",MvNormalMeanCovariance(mean(prediction), cov(prediction)))\n",
    "println(\"Measurement: \", MvNormalMeanCovariance(noisy_x[n], Σx))\n",
    "println(\"Posterior: \", MvNormalMeanCovariance(μz_posterior, Σz_posterior))\n",
    "plotCartPrediction(mean(prediction)[1], cov(prediction)[1], μz_posterior[1], Σz_posterior[1], noisy_x[n][1], Σx[1][1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Note that both the analytical Kalman filtering solution and the message passing solution lead to the same results. The advantage of message passing-based inference with RxInfer is that we did not need to derive any inference equations. RxInfer took care of all that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap Dynamical Models \n",
    "\n",
    "- Dynamical systems do not obey the sample-by-sample independence assumption, but still can be specified, and state and parameter estimation equations can be solved by similar tools as for static models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Two of the more famous and powerful models with latent states include the hidden Markov model (with discrete states) and the Linear Gaussian dynamical system (with continuous states)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- For the LGDS, the Kalman filter is a well-known recursive state estimation procedure. The Kalman filter can be derived through Bayesian update rules on Gaussian distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If anything changes in the model, e.g., the state noise is not Gaussian, then you have to re-derive the inference equations again from scratch and it may not lead to an analytically pleasing answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\Rightarrow$ Generally, we will want to automate the inference processes. As we discussed, message passing in a factor graph is a visually appealing method to automate inference processes. We showed how Kalman filtering emerged naturally by automated message passing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center> OPTIONAL SLIDES</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <a id='kalman-proof'>Proof of Kalman filtering equations including evidence updating</a>\n",
    "\n",
    "- Now let's proof the Kalman filtering equations including evidence updating by probabilistic calculus:\n",
    "$$\\begin{align*}\n",
    "\\overbrace{p(z_t\\,|\\,x^t)}^{\\text{posterior}} \\cdot \\overbrace{p(x_t\\,|\\,x^{t-1})}^{\\text{evidence}} \n",
    " &= p(x_t,z_t\\,|\\,x^{t-1}) \\\\\n",
    " &= p(x_t\\,|\\,z_t) \\,p(z_t\\,|\\,x^{t-1}) \\\\\n",
    " &= p(x_t\\,|\\,z_t) \\, \\int p(z_t,z_{t-1}\\,|\\,x^{t-1}) \\mathrm{d}z_{t-1}  \\\\\n",
    " &= p(x_t\\,|\\,z_t) \\, \\int p(z_t\\,|\\,z_{t-1}) \\, p(z_{t-1}\\,|\\,x^{t-1}) \\mathrm{d}z_{t-1} \\\\\n",
    " &=  \\underbrace{\\mathcal{N}(x_t|c z_t, \\sigma_x^2)}_{\\text{likelihood}} \\, \\int \\underbrace{\\mathcal{N}(z_t\\,|\\,a z_{t-1},\\sigma_z^2)}_{\\text{state transition} } \\, \\underbrace{\\mathcal{N}(z_{t-1} \\,|\\, \\mu_{t-1}, \\sigma_{t-1}^2) }_{\\text{prior}}  \\mathrm{d}z_{t-1} \n",
    "\\end{align*}$$\n",
    " \n",
    "- The RHS can be further evaluated by making use of the Gaussian multiplication rules and the following renormalization equality: \n",
    "$$\\begin{align*}\n",
    "\\mathcal{N}(x\\,|\\,cz,\\sigma^2) = \\frac{1}{c}\\cdot \\mathcal{N}\\left(z \\,\\big|\\,\\frac{x}{c},\\left(\\frac{\\sigma}{c}\\right)^2\\right) \\qquad &&\\text{(renormalization)}\\,.\n",
    "\\end{align*}$$\n",
    "\n",
    "In particular, the RHS evaluates to\n",
    "$$\\begin{align*} \n",
    "\\mathcal{N}(x_t|c z_t, &\\sigma_x^2) \\, \\int \\underbrace{\\mathcal{N}(z_t\\,|\\,a z_{t-1},\\sigma_z^2)}_{\\text{use renormalization}} \\mathcal{N}(z_{t-1} \\,|\\, \\mu_{t-1}, \\sigma_{t-1}^2) \\mathrm{d}z_{t-1} \\\\\n",
    "&= \\mathcal{N}(x_t|c z_t, \\sigma_x^2) \\, \\int \\frac{1}{a}\\underbrace{\\mathcal{N}\\left(z_{t-1}\\bigm|  \\frac{z_t}{a},\\left(\\frac{\\sigma_z}{a}\\right)^2 \\right) \\mathcal{N}(z_{t-1} \\,|\\, \\mu_{t-1}, \\sigma_{t-1}^2)}_{\\text{use Gaussian multiplication formula SRG-6}} \\mathrm{d}z_{t-1} \\\\\n",
    "&= \\frac{1}{a} \\mathcal{N}(x_t|c z_t, \\sigma_x^2) \\, \\int \\underbrace{\\mathcal{N}\\left(\\mu_{t-1}\\bigm|  \\frac{z_t}{a},\\left(\\frac{\\sigma_z}{a}\\right)^2 + \\sigma_{t-1}^2 \\right)}_{\\text{not a function of }z_{t-1}} \\underbrace{\\mathcal{N}(z_{t-1} \\,|\\, \\cdot, \\cdot)}_{\\text{integrates to }1} \\mathrm{d}z_{t-1} \\\\\n",
    "&= \\frac{1}{a} \\underbrace{\\mathcal{N}(x_t|c z_t, \\sigma_x^2)}_{\\text{use renormalization rule}} \\, \\underbrace{\\mathcal{N}\\left(\\mu_{t-1}\\bigm|  \\frac{z_t}{a},\\left(\\frac{\\sigma_z}{a}\\right)^2 + \\sigma_{t-1}^2 \\right)}_{\\text{use renormalization rule}} \\\\\n",
    "&= \\frac{1}{c} \\underbrace{\\mathcal{N}\\left(z_t \\bigm|  \\frac{x_t}{c}, \\left( \\frac{\\sigma_x}{c}\\right)^2 \\right) \\mathcal{N}\\left(z_t\\, \\bigm|\\,a \\mu_{t-1},\\sigma_z^2 + \\left(a \\sigma_{t-1}\\right)^2 \\right)}_{\\text{use SRG-6 again}} \\\\\n",
    "&= \\underbrace{\\frac{1}{c} \\mathcal{N}\\left( \\frac{x_t}{c} \\bigm| a \\mu_{t-1}, \\left( \\frac{\\sigma_x}{c}\\right)^2+ \\sigma_z^2 + \\left(a \\sigma_{t-1}\\right)^2\\right)}_{\\text{use renormalization}} \\, \\mathcal{N}\\left( z_t \\,|\\, \\mu_t, \\sigma_t^2\\right)\\\\\n",
    "  &= \\underbrace{\\mathcal{N}\\left(x_t \\,|\\, ca \\mu_{t-1}, \\sigma_x^2 + c^2(\\sigma_z^2+a^2\\sigma_{t-1}^2) \\right)}_{\\text{evidence } p(x_t|x^{t-1})} \\cdot \\underbrace{\\mathcal{N}\\left( z_t \\,|\\, \\mu_t, \\sigma_t^2\\right)}_{\\text{posterior }p(z_t|x^t) }\n",
    "\\end{align*}$$\n",
    "with\n",
    "$$\\begin{align*}\n",
    "  \\rho_t^2 &= a^2 \\sigma_{t-1}^2 + \\sigma_z^2 \\qquad &&\\text{(predicted variance)}\\\\\n",
    "  K_t &= \\frac{c \\rho_t^2}{c^2 \\rho_t^2 + \\sigma_x^2} \\qquad &&\\text{(Kalman gain)} \\\\\n",
    "  \\mu_t &= \\underbrace{a \\mu_{t-1}}_{\\text{prior prediction}} + K_t \\cdot \\underbrace{\\left( x_t - c a \\mu_{t-1}\\right)}_{\\text{prediction error}} \\qquad &&\\text{(posterior mean)}\\\\\n",
    "  \\sigma_t^2 &= \\left( 1 - c\\cdot K_t \\right) \\rho_t^2 \\qquad &&\\text{(posterior variance)}\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extensions of Generative Gaussian Models\n",
    "\n",
    "- Using the methods of the previous lessons, it is possible to create your own new models based on stacking Gaussian and categorical distributions in new ways: \n",
    "\n",
    "<img src=\"./figures/fig-generative-Gaussian-models.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "footer.hidden-print {\n",
       "    display: none !important;\n",
       "}\n",
       "    \n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        CommonHTML: {\n",
       "                            scale: 200\n",
       "                        },\n",
       "                TeX: {\n",
       "                    extensions: [\"AMSmath.js\"],\n",
       "                    equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f display(\"text/html\", read(f, String)) end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
