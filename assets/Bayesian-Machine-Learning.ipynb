{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goals\n",
    "  - Introduction to Bayesian (i.e., probabilistic) modeling\n",
    "- Materials\n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 68-74 (on the coin toss example)\n",
    "    - [Ariel Caticha - 2012 - Entropic Inference and the Foundations of Physics](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Caticha-2012-Entropic-Inference-and-the-Foundations-of-Physics.pdf), pp.35-44 (section 2.9, on deriving Bayes rule for updating probabilities)\n",
    "    - [David Blei - 2014 - Build, Compute, Critique, Repeat: Data Analysis with Latent Variable Models](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Blei-2014-Build-Compute-Critique-Repeat.pdf), on the _Build-Compute-Critique-Repeat_ design model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Challenge: Predicting a Coin Toss\n",
    "\n",
    "- **Problem**: We observe a the following sequence of heads (h) and tails (t) when tossing the same coin repeatedly $$D=\\{hthhtth\\}\\,.$$\n",
    "\n",
    "- What is the probability that heads comes up next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Solution**: later in this lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Bayesian Machine Learning Framework\n",
    "\n",
    "- Suppose that your application is to predict a future observation $x$, based on $N$ past observations $D=\\{x_1,\\dotsc,x_N\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The Bayesian design approach to solving this task involves four stages: \n",
    "\n",
    "<span style=color:red>REPEAT</span>\n",
    "  > 1. Model specification\n",
    "  > 2. Parameter estimation (i.e., learning from an observed data set using Bayesian inference)\n",
    "  > 3. Model evaluation (how good is this (trained) model?)     \n",
    "  \n",
    "<span style=color:red>UNTIL model performance is satisfactory</span>\n",
    "  > 4. Apply model, e.g. for prediction or classification of new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In principle, based on the model evaluation results, you may want to re-specify your model and _repeat_ the design process (a few times), until model performance is acceptable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  \n",
    "- Next, we discuss these four stages in a bit more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (1) Model specification\n",
    "\n",
    "- Your first task is to propose a probabilistic model ($m$) for generating the observations $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A probabilistic model $m$ consists of a joint distribution $p(x,\\theta|m)$ that relates observations $x$ to model parameters $\\theta$. Usually, the model is proposed in the form of a data generating  distribution $p(x|\\theta,m)$ and a prior $p(\\theta|m)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- _You_ are responsible to choose the data generating distribution $p(x|\\theta)$ based on your physical understanding of the data generating process. (For simplicity, we dropped the given dependency on $m$ from the notation).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- _You_ must also choose the prior $p(\\theta)$ to reflect what you know about the parameter values before you see the data $D$.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (2) Parameter estimation\n",
    "\n",
    "- Note that, for a given data set $D=\\{x_1,x_2,\\dots,x_N\\}$ with _independent_ observations $x_n$, the likelihood is \n",
    "$$ p(D|\\theta) = \\prod_{n=1}^N p(x_n|\\theta)\\,,$$\n",
    "so usually you select a model for generating one observation $x_n$ and then use (in-)dependence assumptions to combine these models into a likelihood function for the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The likelihood and prior both contain information about the model parameters. Next, you use Bayes rule to fuse these two information sources into a posterior distribution for the parameters,\n",
    "$$\n",
    "p(\\theta|D) = \\frac{p(D|\\theta) p(\\theta)}{p(D)} \\propto p(D|\\theta) p(\\theta)\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that there's **no need for you to design some clever parameter estimation algorithm**. Bayes rule _is_ the parameter estimation algorithm. The only complexity lies in the computational issues! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This \"recipe\" works only if the right-hand side (RHS) factors can be evaluated; the computational details can be quite challenging and this is what machine learning is about.     \n",
    "  \n",
    "- $\\Rightarrow$ **Machine learning is EASY, apart from computational details :)**\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  (3) Model Evaluation \n",
    "\n",
    "- In the framework above, parameter estimation was executed by \"perfect\" Bayesian reasoning. So is everything settled now? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- No, there appears to be one remaining problem: how good really were our model assumptions $p(x|\\theta)$ and $p(\\theta)$? We want to \"score\" the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that this question is only interesting if we have alternative models to choose from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Let's assume that we have more candidate models, say $\\mathcal{M} = \\{m_1,\\ldots,m_K\\}$ where each model relates to specific prior $p(\\theta|m_k)$ and likelihood $p(D|\\theta,m_k)$? Can we evaluate the relative performance of a model against another model from the set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Start again with **model specification**. _You_ must now specify a prior $p(m_k)$ (next to the likelihood $p(D|\\theta,m_k)$ and prior $p(\\theta|m_k)$) for each of the models and then solve the desired inference problem:      \n",
    "$$\\begin{align*} \n",
    "\\underbrace{p(m_k|D)}_{\\substack{\\text{model}\\\\\\text{posterior}}} &= \\frac{p(D|m_k) p(m_k)}{p(D)} \\\\\n",
    "  &\\propto p(m_k) \\cdot p(D|m_k) \\\\\n",
    "  &= p(m_k)\\cdot \\int_\\theta p(D,\\theta|m_k) \\,\\mathrm{d}\\theta\\\\\n",
    "  &= \\underbrace{p(m_k)}_{\\substack{\\text{model}\\\\\\text{prior}}}\\cdot \\underbrace{\\int_\\theta \\underbrace{p(D|\\theta,m_k)}_{\\text{likelihood}} \\,\\underbrace{p(\\theta|m_k)}_{\\text{prior}}\\, \\mathrm{d}\\theta }_{\\substack{\\text{evidence }p(D|m_k)\\\\\\text{= model likelihood}}}\\\\\n",
    "\\end{align*}$$\n",
    "- This procedure is called **Bayesian model comparison**, which requires that you calculate the \"evidence\" (= model likelihood). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\Rightarrow$ In a Bayesian framework, **model estimation** follows the same recipe as parameter estimation; it just works at one higher hierarchical level. Compare the required calulations:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\theta|D) &\\propto p(D|\\theta) p(\\theta) \\; &&\\text{(parameter estimation)} \\\\\n",
    "p(m_k|D) &\\propto p(D|m_k) p(m_k) \\; &&\\text{(model comparison)}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In principle, you could proceed with asking how good your choice for the candidate model set $\\mathcal{M}$ was. You would have to provide a set of alternative model sets $\\{\\mathcal{M}_1,\\mathcal{M}_2,\\ldots,\\mathcal{M}_M\\}$ with priors $p(\\mathcal{M}_m)$ for each set and compute posteriors $p(\\mathcal{M}_m|D)$. And so forth ...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- With the (relative) performance evaluation scores of your model in hand, you could now re-specify your model (hopefully an improved model) and _repeat_ the design process until the model performance score is acceptable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (4) Prediction\n",
    "\n",
    "- Once we are satisfied with the evidence for a (trained) model, we can apply the model to our prediction/classification/etc task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Given the data $D$, our knowledge about the yet unobserved datum $x$ is captured by (everything is conditioned on the selected model)\n",
    "$$\\begin{align*}\n",
    "p(x|D) &\\stackrel{s}{=} \\int p(x,\\theta|D) \\,\\mathrm{d}\\theta\\\\\n",
    " &\\stackrel{p}{=} \\int p(x|\\theta,D) p(\\theta|D) \\,\\mathrm{d}\\theta\\\\\n",
    " &\\stackrel{m}{=} \\int \\underbrace{p(x|\\theta)}_{\\text{data generation dist.}} \\cdot \\underbrace{p(\\theta|D)}_{\\text{posterior}} \\,\\mathrm{d}\\theta\\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Again, **no need to invent a special prediction algorithm**. Probability theory takes care of all that. The complexity of prediction is just computational: how to carry out the marginalization over $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the application of the learned posterior $p(\\theta|D)$ not necessarily has to be a prediction task. We use it here as an example, but other applications (e.g., classification, regression etc.) are of course also possible. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What did we learn from $D$? Without access to $D$, we would predict new observations through\n",
    "$$\n",
    "p(x) = \\int p(x,\\theta) \\,\\mathrm{d}\\theta = \\int p(x|\\theta) \\cdot \\underbrace{p(\\theta)}_{\\text{prior}} \\,\\mathrm{d}\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Prediction with multiple models\n",
    "\n",
    "- When you have a posterior $p(m_k|D)$ for the models, you don't need to choose one model for the prediction task. You can do prediction by **Bayesian model averaging** to utilitize the predictive power from all models:\n",
    "$$\\begin{align*}\n",
    "p(x|D) &= \\sum_k \\int p(x,\\theta,m_k|D)\\,\\mathrm{d}\\theta \\\\\n",
    " &= \\sum_k \\int  p(x|\\theta,m_k) \\,p(\\theta|m_k,D)\\, p(m_k|D) \\,\\mathrm{d}\\theta \\\\\n",
    "  &= \\sum_k \\underbrace{p(m_k|D)}_{\\substack{\\text{model}\\\\\\text{posterior}}} \\cdot \\int \\underbrace{p(\\theta|m_k,D)}_{\\substack{\\text{parameter}\\\\\\text{posterior}}} \\, \\underbrace{p(x|\\theta,m_k)}_{\\substack{\\text{data generating}\\\\\\text{distribution}}} \\,\\mathrm{d}\\theta\n",
    "\\end{align*}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Alternatively, if you need to work with one model (e.g. due to computational resource constraints), you can for instance select the model with largest posterior $p(m_k|D)$ and use that model for prediction. This is called **Bayesian model selection**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Machine Learning and the Scientific Method Revisited\n",
    "\n",
    "- The Bayesian design process provides a unified framework for the Scientific Inquiry method. We can now add equations to the design loop. (Trial design to be discussed in [Intelligent Agent lesson](https://nbviewer.jupyter.org/github/bertdv/BMLIP/blob/master/lessons/notebooks/Intelligent-Agents-and-Active-Inference.ipynb).) \n",
    "\n",
    "<img src=\"./figures/scientific-inquiry-loop-w-BML-eqs.png\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now Solve the Example Problem: Predicting a Coin Toss\n",
    "\n",
    "- We observe a the following sequence of heads ($h$) and tails ($t$) when tossing the same coin repeatedly $$D=\\{hthhtth\\}\\,.$$\n",
    "\n",
    "- What is the probability that heads comes up next? We solve this in the next slides ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coin toss example (1): Model Specification\n",
    "\n",
    "- We observe a sequence of $N$ coin tosses $D=\\{x_1,\\ldots,x_N\\}$ with $n$ heads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let us denote outcomes by \n",
    "$$x_k = \\begin{cases} h & \\text{if heads comes up} \\\\\n",
    "  t & \\text{if tails} \\end{cases}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Likelihood\n",
    "\n",
    "- Assume a [**Bernoulli** distributed](https://en.wikipedia.org/wiki/Bernoulli_distribution) variable $p(x_k=h|\\mu)=\\mu$, which leads to a [**binomial** distribution](https://en.wikipedia.org/wiki/Binomial_distribution) for the likelihood (assume $n$ times heads were thrown):\n",
    "$$   \n",
    "p(D|\\mu) = \\prod_{k=1}^N p(x_k|\\mu) = \\mu^n (1-\\mu)^{N-n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### <a id=\"beta-prior\">Prior</a>\n",
    "\n",
    "-  Assume the prior belief is governed by a [**beta distribution**](https://en.wikipedia.org/wiki/Beta_distribution)\n",
    "\n",
    "$$\n",
    "p(\\mu) = \\mathcal{B}(\\mu|\\alpha,\\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\mu^{\\alpha-1}(1-\\mu)^{\\beta-1}\n",
    "$$\n",
    "<!---  - The Gamma function is sort of a generalized factorial function. If $\\alpha,\\beta$ are integers, then $\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)(\\Gamma(\\beta)} = \\frac{(\\alpha+\\beta-1)!}{(\\alpha-1)!\\,(\\beta-1)!}$\n",
    "---> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A _what_ distribution? Yes, the **beta distribution** is a **conjugate prior** for the binomial distribution, which means that \n",
    "$$\n",
    "\\underbrace{\\text{beta}}_{\\text{posterior}} \\propto \\underbrace{\\text{binomial}}_{\\text{likelihood}} \\times \\underbrace{\\text{beta}}_{\\text{prior}}\n",
    "$$\n",
    "so we get a closed-form posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\alpha$ and $\\beta$ are called **hyperparameters**, since they parameterize the distribution for another parameter ($\\mu$). E.g., $\\alpha=\\beta=1$ (uniform).\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./figures/B-fig-2.2.png\" width=\"600px\">\n",
    "- (Bishop Fig.2.2). Plots of the beta distribution $\\mathcal{B}(μ|a, b)$ as a function of $μ$ for various values of the hyperparameters $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coin toss example (2): Parameter estimation\n",
    "\n",
    "- Infer posterior PDF over $\\mu$ through Bayes rule\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(\\mu|D) &\\propto p(D|\\mu)\\,p(\\mu|\\alpha,\\beta)  \\\\ \n",
    "        &= \\mu^n (1-\\mu)^{N-n} \\times \\mu^{\\alpha-1} (1-\\mu)^{\\beta-1} \\\\\n",
    "        &= \\mu^{n+\\alpha-1} (1-\\mu)^{N-n+\\beta-1} \n",
    "\\end{align*}$$\n",
    "\n",
    "hence the posterior is also beta-distributed as\n",
    "\n",
    "$$\n",
    "p(\\mu|D) = \\mathcal{B}(\\mu|\\,n+\\alpha, N-n+\\beta)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coin Toss Example (3): Prediction\n",
    "\n",
    "- For simplicity, we skip the model evaluation task here and proceed to **apply** the trained model. Let's use it to predict future observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Marginalize over the parameter posterior to get the predictive PDF for a new coin toss $x_\\bullet$, given the data $D$,\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(x_\\bullet=h|D)  &= \\int_0^1 p(x_\\bullet=h|\\mu)\\,p(\\mu|D) \\,\\mathrm{d}\\mu \\\\\n",
    "  &= \\int_0^1 \\mu \\times  \\mathcal{B}(\\mu|\\,n+\\alpha, N-n+\\beta) \\,\\mathrm{d}\\mu  \\\\\n",
    "  &= \\frac{n+\\alpha}{N+\\alpha+\\beta}\n",
    "\\end{align*}$$\n",
    "\n",
    "- This result is known as [**Laplace's rule of succession**](https://en.wikipedia.org/wiki/Rule_of_succession)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Finally, we're ready to solve our example problem: for $D=\\{hthhtth\\}$ and uniform prior ($\\alpha=\\beta=1$), we get\n",
    "\n",
    "$$ p(x_\\bullet=h|D)=\\frac{n+1}{N+2} = \\frac{4+1}{7+2} = \\frac{5}{9}$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Coin Toss Example: What did we learn?\n",
    "\n",
    "- What did we learn from the data? Before seeing any data, we think that $$p(x_\\bullet=h)=\\left. p(x_\\bullet=h|D) \\right|_{n=N=0} = \\frac{\\alpha}{\\alpha + \\beta}\\,.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Hence, $\\alpha$ and $\\beta$ are prior pseudo-counts for heads and tails respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After the $N$ coin tosses, we think that $p(x_\\bullet=h|D) = \\frac{n+\\alpha}{N+\\alpha+\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Note the following decomposition\n",
    "\n",
    "$$\\begin{align*}\n",
    "    p(x_\\bullet=h|\\,D) &= \\frac{n+\\alpha}{N+\\alpha+\\beta} = \\frac{n}{N+\\alpha+\\beta} + \\frac{\\alpha}{N+\\alpha+\\beta} \\\\\n",
    "        &= \\frac{N}{N+\\alpha+\\beta}\\cdot \\frac{n}{N} + \\frac{\\alpha+\\beta}{N+\\alpha+\\beta} \\cdot \\frac{\\alpha}{\\alpha+\\beta} \\\\\n",
    "        &= \\underbrace{\\frac{\\alpha}{\\alpha+\\beta}}_{\\substack{\\text{prior}\\\\\\text{prediction}}} + \\underbrace{\\underbrace{\\frac{N}{N+\\alpha+\\beta}}_{\\text{gain}}\\cdot \\underbrace{\\big( \\underbrace{\\frac{n}{N}}_{\\substack{\\text{data-based}\\\\\\text{prediction}}} - \\underbrace{\\frac{\\alpha}{\\alpha+\\beta}}_{\\substack{\\text{prior}\\\\\\text{prediction}}} \\big)}_{\\text{prediction error}}}_{\\text{correction}}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Note that, since $0\\leq \\underbrace{\\frac{N}{N+\\alpha+\\beta}}_{\\text{gain}} \\lt 1$, the Bayesian prediction lies between (fuses) the prior and data-based predictions. The data plays the role of \"correcting\" the prior prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For large $N$, the gain goes to $1$ and $\\left. p(x_\\bullet=h|D)\\right|_{N\\rightarrow \\infty} \\rightarrow \\frac{n}{N}$ goes to the data-based prediction (the observed relative frequency)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In principle, we can now continue with \"(4) Model appraisal\" where we compare the $\\texttt{bernoulli}+\\texttt{beta}$ model assumptions to alternative models. For simplicity, we will skip this stage here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "**Bayesian evolution of $p(\\mu|D)$ for the coin toss**\n",
    "\n",
    "Let's see how $p(\\mu|D)$ evolves as we increase the number of coin tosses $N$. We'll use two different priors to demonstrate the effect of the prior on the posterior (set $N=0$ to inspect the prior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package Distributions not found in current path:\n- Run `import Pkg; Pkg.add(\"Distributions\")` to install the Distributions package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package Distributions not found in current path:\n- Run `import Pkg; Pkg.add(\"Distributions\")` to install the Distributions package.\n",
      "",
      "Stacktrace:",
      " [1] require(::Module, ::Symbol) at ./loading.jl:887",
      " [2] top-level scope at In[3]:3"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.activate(\"probprog/workspace\");Pkg.instantiate();\n",
    "IJulia.clear_output();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package PyPlot not found in current path:\n- Run `import Pkg; Pkg.add(\"PyPlot\")` to install the PyPlot package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package PyPlot not found in current path:\n- Run `import Pkg; Pkg.add(\"PyPlot\")` to install the PyPlot package.\n",
      "",
      "Stacktrace:",
      " [1] require(::Module, ::Symbol) at ./loading.jl:887",
      " [2] top-level scope at In[2]:1"
     ]
    }
   ],
   "source": [
    "using PyPlot, Distributions\n",
    "f = figure()\n",
    "range_grid = range(0.0, stop=1.0, length=100)\n",
    "μ = 0.4\n",
    "samples = rand(192) .<= μ # Flip 192 coins\n",
    "posterior1 = Array{Distribution}(undef,193)\n",
    "posterior2 = Array{Distribution}(undef,193)\n",
    "for N=0:1:192\n",
    "    n = sum(samples[1:N]) # Count number of heads in first N flips\n",
    "    posterior1[N+1] = Beta(1+n, 1+(N-n))\n",
    "    posterior2[N+1] = Beta(5+n, 5+(N-n))\n",
    "  \n",
    "end\n",
    "\n",
    "fig = figure(\"Posterior distributions\", figsize=(10,8));\n",
    "ax1 = fig.add_subplot(2,2,1);\n",
    "ax2 = fig.add_subplot(2,2,2);\n",
    "ax3 = fig.add_subplot(2,2,3);\n",
    "ax4 = fig.add_subplot(2,2,4);\n",
    "plt.subplot(ax1); plot(range_grid,pdf.(posterior1[3],range_grid), \"k-\");\n",
    "plt.subplot(ax1); plot(range_grid,pdf.(posterior2[3],range_grid), \"k--\");\n",
    "xlabel(L\"\\mu\"); ylabel(L\"p(\\mu|\\mathcal{D})\"); grid()\n",
    "title(L\"p(\\mu|\\mathcal{D})\"*\" for N=$(3), n=$(sum(samples[1:3])) (real \\$\\\\mu\\$=$(μ))\")\n",
    "legend([\"Based on uniform prior \"*L\"B(1,1)\",\"Based on prior \"*L\"B(5,5)\"], loc=4)\n",
    "\n",
    "plt.subplot(ax2); plot(range_grid,pdf.(posterior1[10],range_grid), \"k-\");\n",
    "plt.subplot(ax2); plot(range_grid,pdf.(posterior2[10],range_grid), \"k--\");\n",
    "xlabel(L\"\\mu\"); ylabel(L\"p(\\mu|\\mathcal{D})\"); grid()\n",
    "title(L\"p(\\mu|\\mathcal{D})\"*\" for N=$(10), n=$(sum(samples[1:10])) (real \\$\\\\mu\\$=$(μ))\")\n",
    "legend([\"Based on uniform prior \"*L\"B(1,1)\",\"Based on prior \"*L\"B(5,5)\"], loc=4)\n",
    "\n",
    "plt.subplot(ax3); plot(range_grid,pdf.(posterior1[50],range_grid), \"k-\");\n",
    "plt.subplot(ax3); plot(range_grid,pdf.(posterior2[50],range_grid), \"k--\");\n",
    "xlabel(L\"\\mu\"); ylabel(L\"p(\\mu|\\mathcal{D})\"); grid()\n",
    "title(L\"p(\\mu|\\mathcal{D})\"*\" for N=$(50), n=$(sum(samples[1:50])) (real \\$\\\\mu\\$=$(μ))\")\n",
    "legend([\"Based on uniform prior \"*L\"B(1,1)\",\"Based on prior \"*L\"B(5,5)\"], loc=4)\n",
    "\n",
    "\n",
    "plt.subplot(ax4); plot(range_grid,pdf.(posterior1[150],range_grid), \"k-\");\n",
    "plt.subplot(ax4); plot(range_grid,pdf.(posterior2[150],range_grid), \"k--\");\n",
    "xlabel(L\"\\mu\"); ylabel(L\"p(\\mu|\\mathcal{D})\"); grid()\n",
    "title(L\"p(\\mu|\\mathcal{D})\"*\" for N=$(150), n=$(sum(samples[1:150])) (real \\$\\\\mu\\$=$(μ))\")\n",
    "legend([\"Based on uniform prior \"*L\"B(1,1)\",\"Based on prior \"*L\"B(5,5)\"], loc=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\Rightarrow$ With more data, the relevance of the prior diminishes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### From Posterior to Point-Estimate\n",
    "\n",
    "- In the example above, Bayesian parameter estimation and prediction were tractable in closed-form. This is often not the case. We will need to approximate some of the computations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall Bayesian prediction\n",
    "\n",
    "$$\n",
    "p(x|D) = \\int p(x|\\theta)p(\\theta|D)\\,\\mathrm{d}{\\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If we approximate posterior $p(\\theta|D)$ by a delta function for one 'best' value $\\hat\\theta$, then the predictive distribution collapses to\n",
    "\n",
    "$$\n",
    "p(x|D)= \\int p(x|\\theta)\\,\\delta(\\theta-\\hat\\theta)\\,\\mathrm{d}{\\theta} = p(x|\\hat\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This is just the data generating distribution $p(x|\\theta)$ evaluated at $\\theta=\\hat\\theta$, which is easy to evaluate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The next question is how to get the parameter estimate $\\hat{\\theta}$? (See next slide)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some Well-known Point-Estimates\n",
    "\n",
    "- **Bayes estimate** (the mean of the posterior)\n",
    "\n",
    "$$\n",
    "\\hat \\theta_{bayes}  = \\int \\theta \\, p\\left( \\theta |D \\right)\n",
    "\\,\\mathrm{d}{\\theta}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Maximum A Posteriori** (MAP) estimate \n",
    "$$\n",
    "\\hat \\theta_{\\text{map}}=  \\arg\\max _{\\theta} p\\left( \\theta |D \\right) =\n",
    "\\arg \\max_{\\theta}  p\\left(D |\\theta \\right) \\, p\\left(\\theta \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Maximum Likelihood** (ML) estimate\n",
    "$$\n",
    "\\hat \\theta_{ml}  = \\arg \\max_{\\theta}  p\\left(D |\\theta\\right)\n",
    "$$\n",
    "  - Note that Maximum Likelihood is MAP with uniform prior\n",
    "  - ML is the most common approximation to the full Bayesian posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian vs Maximum Likelihood Learning\n",
    "\n",
    "Consider the task: predict a datum $x$ from an observed data set $D$.\n",
    "\n",
    "<table>\n",
    "<tr><td></td><td style=\"text-align:center\"> <b>Bayesian</b></td><td style=\"text-align:center\"> <b>Maximum Likelihood </b></td></tr>\n",
    "<tr><td>1. <b>Model Specification</b></td><td>Choose a model $m$ with data generating distribution $p(x|\\theta,m)$ and parameter prior $p(\\theta|m)$</td><td>Choose a model $m$ with same data generating distribution $p(x|\\theta,m)$. No need for priors.</td></tr>\n",
    "<tr><td>2. <b>Learning</b></td><td>use Bayes rule to find the parameter posterior,\n",
    "$$\n",
    "p(\\theta|D) = \\propto p(D|\\theta) p(\\theta)\n",
    "$$  </td><td>By Maximum Likelihood (ML) optimization,\n",
    "$$ \n",
    "    \\hat \\theta  = \\arg \\max_{\\theta}  p(D |\\theta)\n",
    "$$</td></tr>\n",
    "<tr><td>3. <b>Prediction</b></td><td>$$\n",
    "p(x|D) = \\int p(x|\\theta) p(\\theta|D) \\,\\mathrm{d}\\theta\n",
    "$$</td><td>\n",
    "$$ \n",
    "    p(x|D) =  p(x|\\hat\\theta)\n",
    "$$</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Report Card on Maximum Likelihood Estimation\n",
    "\n",
    "- Maximum Likelihood (ML) is MAP with uniform prior. MAP is sometimes called a 'penalized' ML procedure:\n",
    "\n",
    "$$\n",
    "\\hat \\theta_{map}  = \\arg \\max _\\theta  \\{ \\underbrace{\\log\n",
    "p\\left( D|\\theta  \\right)}_{\\text{log-likelihood}} + \\underbrace{\\log\n",
    "p\\left( \\theta \\right)}_{\\text{penalty}} \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- (good!). ML works rather well if we have a lot of data because the influence of the prior diminishes with more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- (good!). Computationally often do-able. Useful fact that makes the optimization easier (since $\\log$ is monotonously increasing):\n",
    "\n",
    "$$\\arg\\max_\\theta \\log p(D|\\theta) =  \\arg\\max_\\theta p(D|\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- (bad). Cannot be used for model comparison! The evidence for a model does not exist in the case of ML approximation:\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(D|m) &= \\int p(D|\\theta) \\cdot p(\\theta|m)\\,\\mathrm{d}\\theta \\\\\n",
    "  &= \\int \\underbrace{\\delta(\\theta-\\hat{\\theta}_{ML})}_{\\substack{\\text{max.llh}\\\\\\text{approximation}}} \\cdot \\underbrace{\\text{Uniform}(\\theta)}_{\\text{improper distribution}}\\mathrm{d}\\theta \\\\\n",
    "  &= \\text{not computable}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\Rightarrow$ **ML estimation is an approximation to Bayesian learning**, but for good reason a very popular learning method when faced with lots of available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "footer.hidden-print {\n",
       "    display: none !important;\n",
       "}\n",
       "    \n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f display(\"text/html\", read(f, String)) end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
