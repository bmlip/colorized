{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Simple maximum likelihood estimates for Gaussian and categorical distributions\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 67-70, 74-76, 93-94      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why Density Estimation?\n",
    "\n",
    "Density estimation relates to building a model $p(x|\\theta)$ from observations $D=\\{x_1,\\dotsc,x_N\\}$. \n",
    "\n",
    "Why is this interesting? Some examples:\n",
    "\n",
    "- **Outlier detection**. Suppose $D=\\{x_n\\}$ are benign mammogram images. Build $p(x | \\theta)$ from $D$. Then low value for $p(x^\\prime | \\theta)$ indicates that $x^\\prime$ is a risky mammogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Compression**. Code a new data item based on **entropy**, which is a functional of $p(x|\\theta)$: \n",
    "$$\n",
    "H[p] = -\\sum_x p(x | \\theta)\\log p(x |\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Classification**. Let $p(x | \\theta_1)$ be a model of attributes $x$ for credit-card holders that paid on time and $p(x | \\theta_2)$ for clients that defaulted on payments. Then, assign a potential new client $x^\\prime$ to either class based on the relative probability of $p(x^\\prime | \\theta_1)$ vs. $p(x^\\prime|\\theta_2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Example Problem\n",
    "\n",
    "<span class=\"exercise\">\n",
    "Consider a set of observations $D=\\{x_1,…,x_N\\}$ in the 2-dimensional plane (see Figure). All observations were generated by the same process. We now draw an extra observation $x_\\bullet = (a,b)$ from the same data generating process. What is the probability that $x_\\bullet$ lies within the shaded rectangle $S$?\n",
    "</span>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching strides(::LinearAlgebra.Adjoint{Float64,Array{Float64,1}})\nClosest candidates are:\n  strides(!Matched::SubArray) at subarray.jl:264\n  strides(!Matched::Base.CodeUnits) at strings/basic.jl:696\n  strides(!Matched::PermutedDimsArray{T,N,perm,iperm,AA} where AA<:AbstractArray where iperm) where {T, N, perm} at permuteddimsarray.jl:62\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching strides(::LinearAlgebra.Adjoint{Float64,Array{Float64,1}})\nClosest candidates are:\n  strides(!Matched::SubArray) at subarray.jl:264\n  strides(!Matched::Base.CodeUnits) at strings/basic.jl:696\n  strides(!Matched::PermutedDimsArray{T,N,perm,iperm,AA} where AA<:AbstractArray where iperm) where {T, N, perm} at permuteddimsarray.jl:62\n  ...",
      "",
      "Stacktrace:",
      " [1] stride(::LinearAlgebra.Adjoint{Float64,Array{Float64,1}}, ::Int64) at ./abstractarray.jl:342",
      " [2] array2py(::LinearAlgebra.Adjoint{Float64,Array{Float64,1}}, ::Int64, ::Int64) at /Users/bert/.julia/packages/PyCall/0jMpb/src/conversions.jl:305",
      " [3] array2py(::LinearAlgebra.Adjoint{Float64,Array{Float64,1}}) at /Users/bert/.julia/packages/PyCall/0jMpb/src/conversions.jl:325",
      " [4] Type at /Users/bert/.julia/packages/PyCall/0jMpb/src/conversions.jl:327 [inlined]",
      " [5] macro expansion at /Users/bert/.julia/packages/PyCall/0jMpb/src/exception.jl:84 [inlined]",
      " [6] _pycall!(::PyCall.PyObject, ::PyCall.PyObject, ::Tuple{LinearAlgebra.Adjoint{Float64,Array{Float64,1}},LinearAlgebra.Adjoint{Float64,Array{Float64,1}},String}, ::Int64, ::PyCall.PyObject) at /Users/bert/.julia/packages/PyCall/0jMpb/src/pyfncall.jl:21",
      " [7] _pycall!(::PyCall.PyObject, ::PyCall.PyObject, ::Tuple{LinearAlgebra.Adjoint{Float64,Array{Float64,1}},LinearAlgebra.Adjoint{Float64,Array{Float64,1}},String}, ::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:zorder,),Tuple{Int64}}}) at /Users/bert/.julia/packages/PyCall/0jMpb/src/pyfncall.jl:11",
      " [8] #pycall#88(::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:zorder,),Tuple{Int64}}}, ::Function, ::PyCall.PyObject, ::Type{PyCall.PyAny}, ::LinearAlgebra.Adjoint{Float64,Array{Float64,1}}, ::Vararg{Any,N} where N) at /Users/bert/.julia/packages/PyCall/0jMpb/src/pyfncall.jl:86",
      " [9] (::getfield(PyCall, Symbol(\"#kw##pycall\")))(::NamedTuple{(:zorder,),Tuple{Int64}}, ::typeof(PyCall.pycall), ::PyCall.PyObject, ::Type{PyCall.PyAny}, ::LinearAlgebra.Adjoint{Float64,Array{Float64,1}}, ::Vararg{Any,N} where N) at ./none:0",
      " [10] #plot#85(::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:zorder,),Tuple{Int64}}}, ::Function, ::LinearAlgebra.Adjoint{Float64,Array{Float64,1}}, ::Vararg{Any,N} where N) at /Users/bert/.julia/packages/PyPlot/fZuOQ/src/PyPlot.jl:179",
      " [11] #plot at ./none:0 [inlined]",
      " [12] plotObservations(::Array{Float64,2}) at ./In[1]:5",
      " [13] top-level scope at In[1]:11"
     ]
    }
   ],
   "source": [
    "using Distributions, PyPlot\n",
    "N = 100\n",
    "generative_dist = MvNormal([0,1.], [0.8 0.5; 0.5 1.0])\n",
    "function plotObservations(obs::Matrix)\n",
    "    plot(obs[1,:]', obs[2,:]', \"kx\", zorder=3)\n",
    "    fill_between([0., 2.], 1., 2., color=\"k\", alpha=0.4, zorder=2) # Shaded area\n",
    "    text(2.05, 1.8, \"S\", fontsize=12)\n",
    "    xlim([-3,3]); ylim([-2,4]); xlabel(\"a\"); ylabel(\"b\")\n",
    "end\n",
    "D = rand(generative_dist, N) # Generate observations from generative_dist\n",
    "plotObservations(D)\n",
    "x_dot = rand(generative_dist) # Generate x∙\n",
    "plot(x_dot[1], x_dot[2], \"ro\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Log-Likelihood for a Multivariate Gaussian (MVG)\n",
    "\n",
    "- Assume we are given a set of IID data points $D=\\{x_1,\\ldots,x_N\\}$, where $x_n \\in \\mathbb{R}^D$. We want to build a model for these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Model specification**: Let's assume a MVG model $x_n=\\mu+\\epsilon_n$ with $\\epsilon_n \\sim \\mathcal{N}(0,\\Sigma)$, or equivalently,\n",
    "\n",
    "$$\\begin{align*}\n",
    "p(x_n|\\mu,\\Sigma) &= \\mathcal{N}(x_n|\\mu,\\Sigma) \n",
    "= |2 \\pi \\Sigma|^{-1/2} \\mathrm{exp} \\left\\{-\\frac{1}{2}(x_n-\\mu)^T\n",
    "\\Sigma^{-1} (x_n-\\mu) \\right\\}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since the data are IID, $p(D|\\theta)$ factorizes as\n",
    "\n",
    "$$  \n",
    "p(D|\\theta) = p(x_1,\\ldots,x_N|\\theta) \\stackrel{\\text{IID}}{=} \\prod_n p(x_n|\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This choice of model yields the following log-likelihood (use (B-C.9) and (B-C.4)),\n",
    "\n",
    "$$\\begin{align*}\n",
    " \\log &p(D|\\theta) = \\log \\prod_n p(x_n|\\theta) = \\sum_n \\log \\mathcal{N}(x_n|\\mu,\\Sigma) \\tag{1}\\\\\n",
    "     &= N \\cdot \\log | 2\\pi\\Sigma |^{-1/2} - \\frac{1}{2} \\sum\\nolimits_{n} (x_n-\\mu)^T \\Sigma^{-1} (x_n-\\mu) \n",
    " \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood estimation of mean of MVG\n",
    "\n",
    "- We want to maximize $\\log p(D|\\theta)$ wrt the parameters $\\theta=\\{\\mu,\\Sigma\\}$. Let's take derivatives; first to mean $\\mu$, (making use of  (B-C.25) and (B-C.27)),\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_\\mu \\log p(D|\\theta) &= -\\frac{1}{2}\\sum_n \\nabla_\\mu \\left[ (x_n-\\mu)^T \\Sigma^{-1} (x_n-\\mu) \\right] \\\\\n",
    "&= -\\frac{1}{2}\\sum_n \\nabla_\\mu \\mathrm{Tr} \\left[ -2\\mu^T\\Sigma^{-1}x_n + \\mu^T\\Sigma^{-1}\\mu \\right]  \\\\\n",
    "&= -\\frac{1}{2}\\sum_n \\left( -2\\Sigma^{-1}x_n + 2\\Sigma^{-1}\\mu \\right) \\\\\n",
    "&= \\Sigma^{-1}\\,\\sum_n \\left( x_n-\\mu \\right)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Setting the derivative to zero yields the **sample mean**\n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\boxed{\n",
    "\\hat \\mu = \\frac{1}{N} \\sum_n x_n\n",
    "}\n",
    "\\end{equation*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood estimation of variance of MVG\n",
    "\n",
    "- Now we take the gradient of the log-likelihood wrt the **precision matrix** $\\Sigma^{-1}$ (making use of B-C.28 and B-C.24)\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla_{\\Sigma^{-1}}  &\\log p(D|\\theta) \\\\\n",
    "&= \\nabla_{\\Sigma^{-1}} \\left[ \\frac{N}{2} \\log |2\\pi\\Sigma|^{-1} - \\frac{1}{2} \\sum_{n=1}^N (x_n-\\mu)^T \\Sigma^{-1} (x_n-\\mu)\\right] \\\\\n",
    "&= \\nabla_{\\Sigma^{-1}} \\left[ \\frac{N}{2} \\log |\\Sigma^{-1}| - \\frac{1}{2} \\sum_{n=1}^N \\mathrm{Tr} \\left[ (x_n-\\mu) (x_n-\\mu)^T \\Sigma^{-1}\\right] \\right]\\\\\n",
    "&= \\frac{N}{2}\\Sigma -\\frac{1}{2}\\sum_n (x_n-\\mu)(x_n-\\mu)^T\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Get optimum by setting the gradient to zero,\n",
    "$$\\begin{equation*}\n",
    "\\boxed{\n",
    "\\hat \\Sigma = \\frac{1}{N} \\sum_n (x_n-\\hat\\mu)(x_n - \\hat\\mu)^T}\n",
    "\\end{equation*}$$\n",
    "which is also known as the **sample variance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sufficient Statistics\n",
    "\n",
    "- Note that the ML estimates can also be written as\n",
    "$$\\begin{equation*}\n",
    "\\hat \\Sigma = \\sum_n x_n x_n^T - \\left( \\sum_n x_n\\right)\\left( \\sum_n x_n\\right)^T, \\quad \\hat \\mu = \\frac{1}{N} \\sum_n x_n\n",
    "\\end{equation*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- I.o.w., the two statistics (a 'statistic' is a function of the data) $\\sum_n x_n$ and $\\sum_n x_n x_n^T$ are sufficient to estimate the parameters $\\mu$ and $\\Sigma$ from $N$ observations. In the literature, $\\sum_n x_n$ and  $\\sum_n x_n x_n^T$ are called **sufficient statistics** for the Gaussian PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The actual parametrization of a PDF is always a re-parameteriation of the sufficient statistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sufficient statistics are useful because they summarize all there is to learn about the data set in a minimal set of variables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution to Example Problem\n",
    "\n",
    "<span class=\"exercise\">\n",
    "We apply maximum likelihood estimation to fit a 2-dimensional Gaussian model ($m$) to data set $D$. Next, we evaluate $p(x_\\bullet \\in S | m)$ by (numerical) integration of the Gaussian pdf over $S$: $p(x_\\bullet \\in S | m) = \\int_S p(x|m) \\mathrm{d}x$.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: objects of type Array{Float64,2} are not callable\nUse square brackets [] for indexing an Array.",
     "output_type": "error",
     "traceback": [
      "MethodError: objects of type Array{Float64,2} are not callable\nUse square brackets [] for indexing an Array.",
      "",
      "Stacktrace:",
      " [1] mapreduce_first(::Array{Float64,2}, ::Function, ::Int64) at ./reduce.jl:295",
      " [2] mapreduce(::Array{Float64,2}, ::Function, ::Int64) at ./reduce.jl:322",
      " [3] sum(::Array{Float64,2}, ::Int64) at ./reduce.jl:397",
      " [4] top-level scope at In[2]:2"
     ]
    }
   ],
   "source": [
    "using HCubature # Numerical integration package\n",
    "\n",
    "# Maximum likelihood estimation of 2D Gaussian\n",
    "μ = 1/N * sum(D,2)[:,1]\n",
    "D_min_μ = D - repmat(μ, 1, N)\n",
    "Σ = Hermitian(1/N * D_min_μ*D_min_μ')\n",
    "m = MvNormal(μ, convert(Matrix, Σ))\n",
    "\n",
    "# Contour plot of estimated Gaussian density\n",
    "A = Matrix{Float64}(100,100); B = Matrix{Float64}(100,100)\n",
    "density = Matrix{Float64}(100,100)\n",
    "for i=1:100\n",
    "    for j=1:100\n",
    "        A[i,j] = a = (i-1) * 6 / 100 .-2\n",
    "        B[i,j] = b = (j-1) * 6 / 100 .-3\n",
    "        density[i,j] = pdf(m, [a,b])\n",
    "    end\n",
    "end\n",
    "c = contour(A, B, density, 6, zorder=1)\n",
    "PyPlot.set_cmap(\"cool\")\n",
    "clabel(c, inline=1, fontsize=10)\n",
    "\n",
    "# Plot observations, x∙, and the countours of the estimated Gausian density\n",
    "plotObservations(D)\n",
    "plot(x_dot[1], x_dot[2], \"ro\")\n",
    "\n",
    "# Numerical integration of p(x|m) over S:\n",
    "(val,err) = hcubature((x)->pdf(m,x), [0., 1.], [2., 2.])\n",
    "println(\"p(x⋅∈S|m) ≈ $(val)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discrete Data: the 1-of-K Coding Scheme\n",
    "\n",
    "- Consider a coin-tossing experiment with outcomes $x \\in\\{0,1\\}$ (tail and head) and let $0\\leq \\mu \\leq 1$ represent the probability of heads. This model can written as a **Bernoulli distribution**:\n",
    "$$ \n",
    "p(x|\\mu) = \\mu^{x}(1-\\mu)^{1-x}\n",
    "$$\n",
    "  - Note that the variable $x$ acts as a (binary) **selector** for the tail or head probabilities. Think of this as an 'if'-statement in programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **1-of-K coding scheme**. Now consider a $K$-sided coin (a _die_ (pl.: dice)). It is convenient to code the outcomes by $x=(x_1,\\ldots,x_K)^T$ with **binary selection variables**\n",
    "$$\n",
    "x_k = \\begin{cases} 1 & \\text{if die landed on $k$th face}\\\\\n",
    "0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "  - E.g., for $K=6$, if the die lands on the 3rd face $\\,\\Rightarrow x=(0,0,1,0,0,0)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Assume the probabilities $p(x_k=1) = \\mu_k$ with  $\\sum_k \\mu_k  = 1$. The data generating distribution is then (note the similarity to the Bernoulli distribution)\n",
    "\n",
    "$$\n",
    "p(x|\\mu) = \\mu_1^{x_1} \\mu_2^{x_2} \\cdots \\mu_k^{x_k}=\\prod_k \\mu_k^{x_k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This generalized Bernoulli distribution is called the **categorical distribution** (or sometimes the 'multi-noulli' distribution).\n",
    "\n",
    "<!---\n",
    "\n",
    "- Note that $\\sum_k x_k = 1$ and verify for yourself that $\\mathrm{E}[x|\\mu] = \\mu$.\n",
    "- In these notes, we use the superscript to indicate that we are working with a **binary selection variable** in a 1-of-$K$ scheme.\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Categorical vs. Multinomial Distribution\n",
    "\n",
    "- Observe a data set $D=\\{x_1,\\ldots,x_N\\}$  of $N$ IID rolls of a $K$-sided die, with generating PDF\n",
    "$$\n",
    "p(D|\\mu) = \\prod_n \\prod_k \\mu_k^{x_{nk}} = \\prod_k \\mu_k^{\\sum_n x_{nk}} = \\prod_k \\mu_k^{m_k}\n",
    "$$\n",
    "where $m_k= \\sum_n x_{nk}$ is the total number of occurrences that we 'threw' $k$ eyes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This distribution depends on the observations **only** through the quantities $\\{m_k\\}$, with generally $K \\ll N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A related distribution is the distribution over $D_m=\\{m_1,\\ldots,m_K\\}$, which is called the **multinomial distribution**,\n",
    "$$\n",
    "p(D_m|\\mu) =\\frac{N!}{m_1! m_2!\\ldots m_K!} \\,\\prod_k \\mu_k^{m_k}\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The catagorical distribution $p(D|\\mu) = p(\\,x_1,\\ldots,x_N\\,|\\,\\mu\\,)$ is a distribution over the **observations** $\\{x_1,\\ldots,x_N\\}$, whereas the multinomial distribution $p(D_m|\\mu) = p(\\,m_1,\\ldots,m_K\\,|\\,\\mu\\,)$ is a distribution over the **data frequencies** $\\{m_1,\\ldots,m_K\\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Estimation for the Multinomial\n",
    "\n",
    "- Now let's find the ML estimate for $\\mu$, based on $N$ throws of a $K$-sided die. Again we use the shorthand $m_k \\triangleq \\sum_n x_{nk}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The log-likelihood for the multinomial distribution is given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathrm{L}(\\mu) &\\triangleq \\log p(D_m|\\mu) \\propto \\log \\prod_k \\mu_k^{m_k} =  \\sum_k m_k \\log \\mu_k \\tag{2}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When doing ML estimation, we must obey the constraint $\\sum_k \\mu_k  = 1$, which can be accomplished by a <span style=\"color:red\">Lagrange multiplier</span>. The **augmented log-likelihood** with Lagrange multiplier is then\n",
    "\n",
    "$$\n",
    "\\mathrm{L}^\\prime(\\mu) = \\sum_k m_k \\log \\mu_k  + \\lambda \\cdot (1 - \\sum_k \\mu_k )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Set derivative to zero yields the **sample proportion** for $\\mu_k$ \n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\nabla_{\\mu_k}   \\mathrm{L}^\\prime = \\frac{m_k }\n",
    "{\\hat\\mu_k } - \\lambda  \\overset{!}{=} 0 \\; \\Rightarrow \\; \\boxed{\\hat\\mu_k = \\frac{m_k }{N}}\n",
    "\\end{equation*}$$\n",
    "\n",
    "where we get $\\lambda$ from the constraint \n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\sum_k \\hat \\mu_k = \\sum_k \\frac{m_k}\n",
    "{\\lambda} = \\frac{N}{\\lambda} \\overset{!}{=}  1\n",
    "\\end{equation*}$$\n",
    "\n",
    "<!---\n",
    "- Interesting special case: **Binomial** (=$N$ coin tosses): \n",
    "$$p(x_n|\\theta)= \\theta^{[x_n=h]}(1-\\theta)^{[x_n=t]}=\\theta_h^{[x_n=h]} \\theta_t^{[x_n=t]}\n",
    "$$ \n",
    "yields $$\\hat \\theta = \\frac{N_h}{N_h +N_t} $$\n",
    "\n",
    "- Compare this answer to Laplace's rule for predicting the next coin toss (in probability theory lesson) $$p(\\,x_\\bullet=h\\,|\\,\\theta\\,)=\\frac{N_h+1}{N_h +N_t+2}\\,.$$ What is the source of the difference?\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap ML for Density Estimation\n",
    "\n",
    "Given $N$ IID observations $D=\\{x_1,\\dotsc,x_N\\}$.\n",
    "\n",
    "- For a **multivariate Gaussian** model $p(x_n|\\theta) = \\mathcal{N}(x_n|\\mu,\\Sigma)$, we obtain ML estimates\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat \\mu &= \\frac{1}{N} \\sum_n x_n \\tag{sample mean} \\\\\n",
    "\\hat \\Sigma &= \\frac{1}{N} \\sum_n (x_n-\\hat\\mu)(x_n - \\hat \\mu)^T \\tag{sample variance}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For discrete outcomes modeled by a 1-of-K **categorical distribution** we find\n",
    "\n",
    "$$\\begin{align}\n",
    "\\hat\\mu_k  = \\frac{1}{N} \\sum_n x_{nk} \\quad \\left(= \\frac{m_k}{N} \\right) \\tag{sample proportion}\n",
    "\\end{align}$$\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "- Note the similarity for the means between discrete and continuous data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We didn't use a co-variance matrix for discrete data. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", read(f,String))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
