{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intelligent Agents and Active Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to Active Inference and application to the design of synthetic intelligent agents \n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "    - Karl Friston - 2016 - [The Free Energy Principle](https://www.youtube.com/watch?v=NIu_dJGyIQI) (video)\n",
    "  - Optional\n",
    "    - Raviv (2018), [The Genius Neuroscientist Who Might Hold the Key to True AI](https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/).\n",
    "        - Interesting article on Karl Friston, who is a leading theoretical neuroscientist working on a theory that relates life and intelligent behavior to physics (and Free Energy minimization). (**highly recommended**) \n",
    "    - Kirsch (2019), [Theories of Intelligence: Active Inference](http://louiskirsch.com/ai/active-inference) \n",
    "        - A nice tutural blog on active inference.\n",
    "    - Van de Laar and De Vries (2019), [Simulating Active Inference Processes by Message Passing](https://www.frontiersin.org/articles/10.3389/frobt.2019.00020/full)\n",
    "        - How to implement active inference by message passing in a Forney-style factor graph.\n",
    "\n",
    "  - References\n",
    "    - Friston (2013), [Life as we know it](https://royalsocietypublishing.org/doi/full/10.1098/rsif.2013.0475) \n",
    "    - Conant and Ashby (1970), [Every good regulator of a system must be a model of that system](https://www.tandfonline.com/doi/abs/10.1080/00207727008920220)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agents\n",
    "\n",
    "- In the previous lessons we assumed that a data set was given. \n",
    "- In this lesson we consider _agents_. An agent is a system that _interacts_ with its environment through both sensors and actuators.\n",
    "- Crucially, by acting onto the environment, the agent is able to affect the data that it will sense in the future.\n",
    "  - As an example, by changing the direction where I look, I can affect the (visual) data that will be sensed by my retina.\n",
    "- With this definition of an agent, (biological) organisms are agents, and so are robots, self-driving cars, etc.\n",
    "- In an engineering context, we are particularly interesting in agents that behave with a *purpose* (with a goal in mind), e.g., to drive a car or to design a speech recognition algorithm.\n",
    "- In this lesson, we will describe how __goal-directed behavior__ by biological (and synthetic) agents can also be interpreted as minimization of a free energy functional. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustrative Example: Steering a cart to a parking spot\n",
    "\n",
    "- In this example, we consider a cart that can move in a 1D space. At each time step the cart can be steered a bit to the left or right by a controller (the \"agent\"). The agent's knowledge about the cart's process dynamics (equations of motion) are known up to some additive Gaussian process noise. The agent also makes noisy observations of the position and velocity of the cart. Your challenge is to design an agent that steers the car to the zero position. (The agent should be specified as a probabilistic model and the control signal should be formulated as a Bayesian inference task).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p style=\"text-align:center;\"><img src=\"./ai_agent/agent-cart-interaction.png\" width=\"600px\"></p>\n",
    "\n",
    "- Solution at the end of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Karl Friston and the Free Energy Principle\n",
    "\n",
    "- We begin with a motivating example that requires \"intelligent\" goal-directed decision making: assume that you are an owl and that you're hungry. What are you going to do?\n",
    "\n",
    "- Have a look at [Prof. Karl Friston](https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/)'s answer in this  [video segment by on the cost function for intelligent behavior](https://youtu.be/L0pVHbEg4Yw). (**Do watch the video!**)\n",
    "\n",
    "- Friston argues that intelligent decision making (behavior, action making) by an agent requires *minimization of a functional of beliefs*. \n",
    "\n",
    "- Friston further argues (later in the lecture and his papers) that this functional is a (variational) free energy (to be defined below), thus linking decision-making and acting to Bayesian inference. \n",
    "\n",
    "- In fact, Friston's **Free Energy Principle** (FEP) claims that all [biological self-organizing processes (including brain processes) can be described as Free Energy minimization in a probabilistic model](https://royalsocietypublishing.org/doi/full/10.1098/rsif.2013.0475).\n",
    "  - This includes perception, learning, attention mechanisms, recall, acting and decision making, etc.\n",
    "  \n",
    "- Taking inspiration from FEP, if we want to develop synthetic \"intelligent\" agents, we have (only) two issues to consider:\n",
    "  1. Specification of the FE functional.\n",
    "  2. *How* to minimize the FE functional (often in real-time under situated conditions).  \n",
    "\n",
    "- Agents that follow the FEP are said to be involved in **Active Inference** (AIF). An AIF agent updates its states and parameters by FE minimization, and selects its actions through (expected) FE minimization (to be explained below).    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution of an AIF Agent\n",
    "\n",
    "- We consider an AIF agent with observations (sensory states) $x_k$, latent internal states $s_k$ and latent control states $u_k$ for $k=1,2,\\ldots$. \n",
    "\n",
    "- The agent is embedded in an environment with \"external states\" $\\tilde{s}_k$. The dynamics of the environment are driven by actions. \n",
    "\n",
    "- Actions $a_k$ are selected by the agent. Actions affect the environment and consequently affect future observations. \n",
    "\n",
    "- In pseudo-code, an AIF agent executes the following algorithm:\n",
    "\n",
    "> ASSUME generative model $p(x,s,u)$    \n",
    "> ASSUME environmental process $R$\n",
    ">\n",
    "> FORALL t DO    \n",
    ">\n",
    "> 1.  $(x_t, \\tilde{s}_t) = R(a_t, \\tilde{s}_{t-1})$   % environment generates new observation  \n",
    "> 2.  $q(s_t) = \\arg\\min_q F[q]$         % update internal states\n",
    "> 3.  $q(u_{t+1}) = \\arg\\min_q H[q]$     % update control states\n",
    "> 4.  $a_{t+1} \\sim q(u_{t+1})$          % sample action \n",
    ">     \n",
    "> END\n",
    "\n",
    "- Next, we discuss these steps in more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions: Specification of AIF Agent's model and Environmental Dynamics\n",
    "\n",
    "- We specify the **generative model** of the agent as (other factorizations are also possible, but this one is common and follows the state space modeling approach)\n",
    "$$\n",
    "p(x_k,s_k,u_k|s_{k-1}) = \\underbrace{p(x_k|s_k)}_{\\text{observations}} \\cdot \\underbrace{p(s_k|s_{k-1},u_k)}_{\\substack{\\text{state} \\\\ \\text{transition}}} \\cdot \\underbrace{p(u_k)}_{\\substack{\\text{action} \\\\ \\text{prior}}}\n",
    "$$\n",
    "\n",
    "- We also assume that the agent interacts with an environment, which we represent by a dynamic model $R$ as\n",
    "$$\n",
    "(x_t,\\tilde{s}_t) = R\\left( a_t,\\tilde{s}_{t-1}\\right)\n",
    "$$\n",
    "where $a_t$ are _actions_ , $x_t$ are _outcomes_ (the agent's observations) and $\\tilde{s}_t$ holds the environmental _states_. \n",
    "\n",
    "- Note that $R$ only needs to be specified for simulated environments. If we were to deploy the agent in a real-world environment, we would not need to specify $R$.  \n",
    "\n",
    "- Note that we distinguish between _control states_ and _actions_. Control states $u_k$ are latent variables in the agent's generative model. An action $a_k$ is a realization of a control state as observed by the environmental model. \n",
    "\n",
    "- Observations $x_k$ are generated by the environment and observed by the agent. In contrast, actions $a_k = \\hat{u}_k$ are generated by the agent and observed by the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Updating in the AIF Agent\n",
    "\n",
    "- Assume the following at time step $t$:\n",
    "  - the state of the agent's model has already been updated to $q(s_{t-1}|\\hat{x}_{1:t-1})$. \n",
    "  - the agent has selected a new action $a_t=\\hat{u}_t$.\n",
    "  - the agent has recorded a new observation $x_t=\\hat{x}_t$. \n",
    "\n",
    "- The state updating task is to infer $q(s_{t}|\\hat{x}_{1:t})$, based on the previous estimate $q(s_{t-1}|\\hat{x}_{1:t-1})$, the new data $\\{a_t=\\hat{u}_t,x_t=\\hat{x}_{t}\\}$, and the agent's generative model. \n",
    "\n",
    "- Technically, this is a Bayesian filtering task. In a real brain, this process is called **perception**.   \n",
    "\n",
    "- We specify the following FE functional\n",
    "$$\n",
    "F[q] = \\sum_{s_t} q(s_t|\\hat{x}_{1:t}) \\log \\frac{\\overbrace{q(s_t|\\hat{x}_{1:t})}^{\\text{state posterior}}}{\\underbrace{p(\\hat{x}_t|s_t) p(s_t|s_{t-1},\\hat{u}_t)}_{\\text{generative model}} \\underbrace{q(s_{t-1}|\\hat{x}_{1:t-1})}_{\\text{state prior}}}\n",
    "$$\n",
    "\n",
    "- The state updating task can be formulated as minimization of the above FE (see also eq.2 above):\n",
    "$$\n",
    "q(s_t|\\hat{x}_{1:t}) = \\arg\\min_q F[q]\n",
    "$$\n",
    "\n",
    "- In case the generative model is a Linear Gaussian Dynamical System, minimization of the FE can be solved analytically in closed-form and leads to the standard Kalman filter.\n",
    "\n",
    "- In case these (linear Gaussian) conditions are not met, we can still minimize the FE by other means and arrive at some approximation of the Kalman filter, see for example [Baltieri and  Isomura (2021)](https://arxiv.org/abs/2111.10530) for a Laplace approximation to variational Kalman filtering.  \n",
    "\n",
    "- Our toolbox `RxInfer.jl` specializes in executing this minimization task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Updating in an AIF Agent\n",
    "\n",
    "- In order to select __good__ actions, we need to investigate and compare future consequences of a sequence of future actions. \n",
    "\n",
    "- A sequence of future actions $a= (a_{t+1}, a_{t+2}, \\ldots, a_{t+T})$ is called a **policy**. \n",
    "\n",
    "- In order to assess future consequences of policy choices, we will, as a function of a selected policy, run the generative model forward to make predictions about future observations $x_{t+1:T}$. \n",
    "\n",
    "- Consider an AIF agent at time step $t$ with (future) observations $x = (x_{t+1}, x_{t+2}, \\ldots, x_{t+T})$,  latent future internal states $s= (s_{t+1}, \\ldots, s_{t+T})$, and latent future control variables $u= (u_{t+1}, u_{t+2}, \\ldots, u_{t+T})$. \n",
    "\n",
    "- From the agent's viewpoint, the evolution of these future variables are constrained by its generative model, rolled out into the future:\n",
    "$$\\begin{align*}\n",
    "p(x,s,u) &= \\underbrace{q(s_{t})}_{\\substack{\\text{current}\\\\ \\text{state}}} \\cdot \\underbrace{\\prod_{k=t+1}^{t+T} p(x_k|s_k) \\cdot p(s_k | s_{k-1}, u_k) p(u_k)}_{\\text{GM roll-out to future}}\n",
    "\\end{align*}$$\n",
    "\n",
    "- Consider the Free Energy functional for estimating posterior beliefs $q(s,u)$ over future states and control signals: \n",
    "$$\n",
    "H[q] = \\sum_{x,s,u} q(x,s,u) \\log \\frac{q(s,u)}{p(x,s,u)} \n",
    "$$\n",
    "\n",
    "- In principle, this is a regular FE functional, with one difference to previous versions: since future observations $x$ have not yet occurred, the FE functional marginalizes not only over latent states $s$ and policies $u$, but also over future observations $x$.\n",
    "\n",
    "\n",
    "- We will update the beliefs over policies by minimization of Free Energy functional $H[q]$, see eq.3 above. In the [optional slides below, we prove that the solution to this optimization task](#q-star) is given by\n",
    "$$\\begin{aligned}\n",
    "q^*(u) &= \\arg\\min_q H[q] \\\\\n",
    "&\\propto p(u)\\exp(-G(u))\n",
    "\\end{aligned}$$\n",
    "\n",
    "- The factor $p(u)$ is a prior over admissible policies. \n",
    "\n",
    "- The factor $\\exp(-G(u))$ updates the prior with information about future consequences of a selected policy $u$. The function \n",
    "$$G(u) = \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p(x,s|u)}$$ \n",
    "is called the **Expected Free Energy** (EFE) for policy $u$. Next, we will further analyze the EFE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Inference Analysis: exploitation-exploration dilemma \n",
    "\n",
    "- Note that, since \n",
    "$$\n",
    "q^*(u) \\propto p(u)\\exp(-G(u))\n",
    "$$\n",
    "the probability $q^*(u)$ for a given policy $u$ increases when EFE $G(u)$ gets smaller. Therefore, we are most interested in policies with low EFE.  \n",
    "\n",
    "- Note the following decomposition of EFE:\n",
    "$$\\begin{aligned}\n",
    "G(u) &= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p(x,s|u)} \\\\\n",
    "&= \\sum_{x,s} q(x,s|u) \\log \\frac{1}{p(x)} + \\sum_{x,s} q(x,s|u) \\log \\frac{q(s|u)}{p(s|x,u)}\\frac{q(s|x)}{q(s|x)} \\\\\n",
    "&= \\sum_x q(x|u) \\log \\frac{1}{p(x)} + \\sum_{x,s} q(x,s|u) \\log \\frac{q(s|u)}{q(s|x)} + \\underbrace{\\sum_{x,s} q(x,s|u) \\log \\frac{q(s|x)}{p(s|x,u)}}_{E\\left[ D_{\\text{KL}}[q(s|x),p(s|x,u)] \\right]\\geq 0} \\\\\n",
    "&\\geq \\underbrace{\\sum_x q(x|u) \\log \\frac{1}{p(x)}}_{\\substack{\\text{goal-seeking behavior} \\\\ \\text{(exploitation)}}} - \\underbrace{\\sum_{x,s} q(x,s|u) \\log \\frac{q(s|x)}{q(s|u)}}_{\\substack{\\text{information-seeking behavior}\\\\ \\text{(exploration)}}} \n",
    "\\end{aligned}$$ \n",
    "\n",
    "- Let us analyze these results. Apparently, minimization of EFE leads to selection of policies that balances the following two imperatives: \n",
    "\n",
    "  1. minimization of the first term of $G(u)$, i.e. minimizing $\\sum_x q(x|u) \\log \\frac{1}{p(x)}$ leads to actions ($u$) that align the inferred observations $q(x|u)$ with the prior on observations $p(x)$. We are in control to choose any prior $p(x)$ and usually we choose a prior that aligns with desired (goal) observations. Hence, policies with low EFE leads to **goal-seeking behavior** (a.k.a. pragmatic behavior or exploitation).\n",
    "  \n",
    "  1. minimization of $G(u)$ maximizes the second term \n",
    "  $$\\begin{aligned}\n",
    "  \\sum_{x,s} q(x,s|u) \\log \\frac{q(s|x)}{q(s|u)} &= \\sum_{x,s} q(x,s|u) \\log \\frac{q(s|x)}{q(s|u)}\\frac{q(x|u)}{q(x|u)} \\\\\n",
    "  &= \\underbrace{\\sum_{x,s} q(x,s|u) \\log \\frac{q(x,s|u)}{q(x|u)q(s|u)}}_{\\text{(conditional) mutual information }I[x,s|u]}\n",
    "  \\end{aligned}$$ \n",
    "  which is the [__mutual information__](https://en.wikipedia.org/wiki/Mutual_information) between (posteriors on) observations and states, for a given policy $u$. Thus, maximizing this term leads to actions that maximize statistical dependency between observations and states. In other words, a policy with low EFE also leads to **information-seeking behavior** (a.k.a. epistemic behavior or exploration). \n",
    "\n",
    "- (The third term $\\sum_{x,s} q(x,s|u) \\log \\frac{q(s|x)}{p(s|x)}$ is an (expected) KL divergence between posterior and prior on the states. This can be interpreted as a complexity term and $G(u)$ minimization will drive this term to zero.)   \n",
    "\n",
    "- Seeking actions that balance goal-seeking behavior (exploitation) and information-seeking behavior (exploration) is a [fundamental problem in the Reinforcement Learning literature](http://tomstafford.staff.shef.ac.uk/?p=48). \n",
    "\n",
    "- **Active Inference solves the exploration-exploitation dilemma**. Both objectives are served by EFE minimization without need for any tuning parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Brain's Action-Perception Loop by FE Minimization\n",
    "\n",
    "- The derivations are not easy, but we have just shown that FE minimization (and EFE for action policy selection) in a generative model leads to variational Bayesian perception (a la Kalman filtering), and a balanced exploration-exploitation trade-off for policy selection. \n",
    "\n",
    "<p style=\"text-align:center;\"><img style=\"border:2px solid #000000; width:auto\" src=\"./figures/brain-design-cycle.png\"></p>\n",
    "\n",
    "- The current FEP theory claims that minimization of FE (and EFE) is all that brains do, i.e., FE minimization leads to perception, policy selection, learning, structure adaptation, attention, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Engineering Challenge: Synthetic AIF Agents for Automated Algorithm Design\n",
    "\n",
    "- A big AI challenge is to design synthetic AIF agents based only on FE/EFE minimization.\n",
    "\n",
    "<p style=\"text-align:center;\"><img style=\"border:2px solid #000000; width:auto\" src=\"./figures/Synthetic-FEP-agent.png\"></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What Makes a Good Agent?\n",
    "\n",
    "- So, according to Friston, an ``intelligent'' agent like a brain minimizes a variational free energy functional, which, in general, is a functional of a probability distribution $p$ and a variational posterior $q$. \n",
    "\n",
    "- What should the agent's model $p$ be modeling? This question was (already) answered by [Conant and Ashby (1970)](https://www.tandfonline.com/doi/abs/10.1080/00207727008920220) as the [*good regulator theorem*](https://en.wikipedia.org/wiki/Good_regulator ): **every good regulator of a system must be a model of that system**. \n",
    "  \n",
    "- A Quote from Conant and Ashby's paper (this statement was later finessed by [Friston (2013)](https://royalsocietypublishing.org/doi/full/10.1098/rsif.2013.0475)): \n",
    "> \"The theory has the interesting corollary that the living brain, insofar as it is successful and efficient as a regulator for survival, *must* proceed, in learning, by the formation of a model (or models) of its environment.\"\n",
    "\n",
    "<p style=\"text-align:center;\"><img style=\"border:2px solid #000000; width:500px\" src=\"./figures/good-regulator.png\"></p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Active Inference Agents \n",
    "\n",
    "- We will follow the idea that an agent needs to hold a generative model for its environment, which is observed through sensory channels. The environmental dynamics can be affected through actions onto the environment.\n",
    "\n",
    "- Agents that follow the FEP and infer actions by inference in a generative model of the environment are engaged in a process called **active inference**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planning by Inference\n",
    "\n",
    "- In order to select __good__ actions, we need to investigate and compare future consequences of actions. \n",
    "\n",
    "- In order to assess future consequences, we will, at time step $t$, run the generative model run forward to make predictions (beliefs) about future observations $x_{t+1:T}$ (as a function of a selected sequence of actions $u_{t+1:T}$).\n",
    "\n",
    "- Note that in this system, the agent's future observations depend on the agent's current (and past) actions: $$x_{t+1} = x_{t+1} \\left( a_{t+1} \\right) = x_{t+1} \\left( a_{t+1} \\left( u_{t+1}\\left( x_t \\left( a_t \\left( \\cdots \\right) \\right) \\right)\\right) \\right)$$\n",
    "  - $\\Rightarrow$ As a result, the agent actively engages in selecting its own data set!\n",
    "\n",
    "-  So, **actions are intended to manipulate future observations**. \n",
    "\n",
    "- How should the agent choose its actions (controls) $u_k$? Since future observations have not yet happened, we cannot minimize the original FE functional that assumes observations have been recorded. \n",
    "\n",
    "- The FEP takes on the following stance: if FE minimization is all that an agent does, then the only consistent behavior for an agent is to select actions minimize the **expected** Free Energy in the future (where expectation is taken over current beliefs about future observations). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Technically, an active inference-based agent comprises:\n",
    "\n",
    "  1. A free energy functional $F[q] = \\mathbb{E}_q\\left[ \\log\\frac{q(z)}{p(x,z)}\\right]$, where \n",
    "     - $p(x,z) = \\prod_k p(x_k,z_k|z_{k-1})$ is a _generative_ model with observations $\\{x_k\\}$, latent variables $\\{z_k\\} = \\left\\{ \\{s_k\\}, \\{u_k\\}, \\{\\theta_k\\}\\right\\}$ and $k$ is a time index.\n",
    "     - $q(z)$ is a _recognition_ model. \n",
    "  2. A recipe to minimize the free energy $F[q]$ \n",
    "\n",
    "\n",
    "- Let's draw a diagram to show the interactions between an active inference agent and its environment. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/agent-environment-interaction.png\" width=\"600px\"></p>\n",
    "\n",
    "- In the model above, the hidden variables $\\{z_k\\}$ of the agent comprise *internal* states $\\{s_k\\}$, *control* variables $\\{u_k\\}$ (which are \"observed\" by the environment as actions $\\{a_k\\}$), and *parameters* $\\{\\theta_k\\}$.   \n",
    "\n",
    "- In neuroscience/psychology parlance, \n",
    "  - _behavior_ (movement) is inference for the control signals ($u$)\n",
    "  - _perception_ is inference for the internal states ($s$). \n",
    "  - _learning_ is inference for the parameters ($\\theta$)\n",
    "  \n",
    "- We also assume that the agent interacts with an environment, which we represent by a dynamic model\n",
    "$$\n",
    "(y_t,\\tilde{s}_t) = R_t\\left( a_t,\\tilde{s}_{t-1}\\right)\n",
    "$$\n",
    "where $a_t$ are _actions_ , $y_t$ are _outcomes_ and $\\tilde{s}_t$ holds the environmental _states_. \n",
    "\n",
    "- In the above equations, $u_t$ and $x_t$ are owned by the agent model, whereas $a_t$ and $y_t$ are variables in the environment model.\n",
    "\n",
    "- The agent can push actions $a_t$ onto the environment and measure responses $y_t$, but has no access to the environmental states $\\tilde{s}_t$.\n",
    "\n",
    "- Interactions between the agent and environment are described by \n",
    "$$\\begin{align*}\n",
    "a_t &\\sim q(u_t) \\\\\n",
    "x_t &= y_t \n",
    "\\end{align*}$$\n",
    "iow, actions are drawn from the posterior over control signals. \n",
    "\n",
    "\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goal-directed Behavior\n",
    "\n",
    "\n",
    "<!---\n",
    "\n",
    "- The CA decomposition of free energy shows that _actions_ aim to maximize accuracy since model complexity is not a function of the observations (and $x = x(a)$) \n",
    "$$ F[q]=  \\underbrace{\\sum_z q(z)\\log\\frac{q(z)}{p(z)}}_{\\text{complexity}} - \\underbrace{\\sum_z q(z) \\log p(x|z)}_{\\text{accuracy}}$$\n",
    "\n",
    "- The DE decomposition reveals that _perception_ and _learning_ minimize inference costs since log-evidence is not affected by inference (not a function of $q$)\n",
    "$$F[q] = \\underbrace{\\sum_z q(z) \\log \\frac{q(z)}{p(z|x)}}_{\\substack{\\text{divergence}\\\\ \\text{\"inference costs\"}}} - \\underbrace{\\log p(x)}_{\\text{log-evidence}}$$\n",
    "\n",
    "\n",
    "- Finally, the EE decomposition discloses a deep link with the 2nd law of thermodynamics (drive towards maximum entropy). An agent aims to maximize entropy over its beliefs subject to constraints put up by its generative model and inference skills (the energy term)\n",
    "$$F[q] = \\underbrace{-\\sum_z q(z) \\log p(x,z)}_{\\text{energy}} - \\underbrace{\\sum_z q(z) \\log \\frac{1}{q(z)}}_{\\text{entropy}}$$\n",
    "--->\n",
    "\n",
    "- Biological agents select their observations by controling their environment. Perception (and learning) serve to improve this data selection process by updating beliefs about the state of the world. \n",
    "\n",
    "- This process begs the question: if a (biological) agent seeks out observations, then which observations is the agent interested in? I.o.w., does the agent have a **goal** \"in mind\" when it engages in active data selection?\n",
    "\n",
    "- Yes! Agents set preferences for future observations by setting **prior distributions on future observations**! \n",
    "  - E.g., a self-driving agent in a car expects to observe no collisions.\n",
    "  \n",
    "- Thus, the generative model for an active inference agent at time $t$ includes variables at future time steps and can be run forward to make predictions (beliefs) about future observations $x_{t+1:T}$.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Active Inference Agent Model specification\n",
    "\n",
    "- We assume that agents live in a dynamic environment and consider the following generative model for the agent (omitting parameters $\\theta$), and assuming the current time is $t$:\n",
    "$$\\begin{align*}\n",
    "p^\\prime(x,s,u) &= p(s_{t-1}) \\prod_{k=t}^{t+T} \\underbrace{p(x_k|s_k) \\cdot p(s_k | s_{k-1}, u_k)}_{\\text{internal dynamics}} \\cdot\\underbrace{p(u_k)}_{\\substack{\\text{control}\\\\ \\text{prior}}}\n",
    "\\end{align*}$$\n",
    "  - Note that the generative model includes future time steps.\n",
    "\n",
    "\n",
    "\n",
    "- In order to infer **goal-driven** (i.e., purposeful) behavior, we now add prior beliefs $\\tilde{p}(x)$ about desired future observations, leading to an *extended* agent model:\n",
    "$$\\begin{align*}\n",
    "p(x,s,u) &= \\frac{p^\\prime(x,s,u) \\tilde{p}(x)}{\\int_x p^\\prime(x,s,u) \\tilde{p}(x) \\mathrm{d}x} \\\\\n",
    "  &\\propto \\underbrace{p(s_{t-1}) \\prod_{k=t}^{t+T} p(x_k|s_k) p(s_k | s_{k-1}, u_k) p(u_k)}_{\\text{original generative model}} \\underbrace{\\tilde{p}(x_k)}_{\\substack{\\text{extension}\\\\\\text{\"goal prior\"}}}\n",
    "\\end{align*}$$\n",
    "  - $\\tilde{p}(x)$ encodes priors beliefs by the agent about future observations. \n",
    "\n",
    "- Goal-directed behavior follows from inference for controls (actions) at $t$, based on expectations (encoded by priors) about future ($>t$) observations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FFG for Active Inference Agent Model\n",
    "\n",
    "- After selecting an action $a_t$ and making an observation $y_t$, the FFG for the extended generative model is given by the following FFG:\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/fig-active-inference-model-specification.png\" width=\"600px\"></p>\n",
    "\n",
    "- The (brown) dashed box is the agent's Markov blanket. Given the states on the Markov blanket, the internal states of the agent are independent of the state of the world.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to minimize FE: Online Active Inference\n",
    "\n",
    "- Online active inference proceeds by iteratively executing three stages: (1) act-execute-observe, (2) infer the next control/action, (3) slide forward\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/fig-online-active-inference.png\" width=\"700px\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg;Pkg.activate(\"probprog/workspace\");Pkg.instantiate()\n",
    "IJulia.clear_output();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Cart Park Problem Revisited\n",
    "\n",
    "Here we solve the cart parking problem as stated at the beginning of this lesson. We first specify a generative model for the agent's environment (which is the observed noisy position of the cart) and then constrain future observations by a prior distribution that is located on the target parking spot. Next, we schedule a message passing-based inference algorithm for the next action. This is followed by executing the \"Act-execute-observe --> infer --> slide\" procedure to infer a sequence of consecutive actions. Finally, the position of the cart over time is plotted. Note that the cart convergees onto the target spot.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bert/.julia/conda/3/lib/python3.7/site-packages/matplotlib/cbook/__init__.py:424: MatplotlibDeprecationWarning: \n",
      "Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warn_deprecated(\"2.2\", \"Passing one of 'on', 'true', 'off', 'false' as a \"\n"
     ]
    }
   ],
   "source": [
    "using PyPlot, LinearAlgebra, ForneyLab\n",
    "# Load helper functions. Feel free to explore these\n",
    "include(\"ai_agent/environment_1d.jl\")\n",
    "include(\"ai_agent/helpers_1d.jl\")\n",
    "include(\"ai_agent/agent_1d.jl\")\n",
    "\n",
    "# Internal model perameters\n",
    "gamma   = 100.0 # Transition precision\n",
    "phi     = 10.0 # Observation precision\n",
    "upsilon = 1.0 # Control prior variance\n",
    "sigma   = 1.0 # Goal prior variance\n",
    "\n",
    "T = 10 # Lookahead\n",
    "\n",
    "# Build internal model\n",
    "fg = FactorGraph()\n",
    "\n",
    "o = Vector{Variable}(undef, T) # Observed states\n",
    "s = Vector{Variable}(undef, T) # internal states\n",
    "u = Vector{Variable}(undef, T) # Control states\n",
    "\n",
    "@RV s_t_min ~ GaussianMeanVariance(placeholder(:m_s_t_min),\n",
    "                                   placeholder(:v_s_t_min)) # Prior  state\n",
    "u_t = placeholder(:u_t)\n",
    "@RV u[1] ~ GaussianMeanVariance(u_t, tiny)\n",
    "@RV s[1] ~ GaussianMeanPrecision(s_t_min + u[1], gamma) \n",
    "@RV o[1] ~ GaussianMeanPrecision(s[1], phi)\n",
    "placeholder(o[1], :o_t)\n",
    "\n",
    "s_k_min = s[1]\n",
    "for k=2:T\n",
    "    @RV u[k] ~ GaussianMeanVariance(0.0, upsilon) # Control prior\n",
    "    @RV s[k] ~ GaussianMeanPrecision(s_k_min + u[k], gamma) # State transition model\n",
    "    @RV o[k] ~ GaussianMeanPrecision(s[k], phi) # Observation model\n",
    "    GaussianMeanVariance(o[k], \n",
    "                         placeholder(:m_o, var_id=:m_o_*k, index=k-1),\n",
    "                         placeholder(:v_o, var_id=:v_o_*k, index=k-1)) # Goal prior\n",
    "    s_k_min = s[k]\n",
    "end\n",
    "\n",
    "# Schedule message passing algorithm\n",
    "algo = messagePassingAlgorithm(u[2]) # Infer internal states\n",
    "source_code = algorithmSourceCode(algo)\n",
    "eval(Meta.parse(source_code)) # Loads the step!() function for inference\n",
    "\n",
    "s_0 = 2.0 # Initial State\n",
    "\n",
    "N = 20 # Total simulation time\n",
    "\n",
    "(execute, observe)  = initializeWorld() # Let there be a world\n",
    "(infer, act, slide) = initializeAgent() # Let there be an agent\n",
    "\n",
    "# Step through action-perception loop\n",
    "u_hat = Vector{Float64}(undef, N) # Actions\n",
    "o_hat = Vector{Float64}(undef, N) # Observations\n",
    "for t=1:N\n",
    "    u_hat[t] = act() # Evoke an action from the agent\n",
    "               execute(u_hat[t]) # The action influences hidden external states\n",
    "    o_hat[t] = observe() # Observe the current environmental outcome (update p)\n",
    "               infer(u_hat[t], o_hat[t]) # Infer beliefs from current model state (update q)\n",
    "               slide() # Prepare for next iteration\n",
    "end\n",
    "\n",
    "# Plot active inference results\n",
    "plotTrajectory(u_hat, o_hat)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video\n",
    "\n",
    "- See [this video to verfiy that the robot will be steered to the correct parking spot even after an \"adversarial\" intervention](https://youtu.be/0ABZJJ9r4Dw) :). (This project was completed by [Burak Ergul](https://biaslab.github.io/member/burak/) as part of this MSc graduation project). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extensions\n",
    "\n",
    "- If interested, here is a link to [a more detailed version of the 1D parking problem](ai_agent/robot_car_1d.ipynb). \n",
    "\n",
    "- We also have a [2D version of this cart parking problem implemented on Raspberry Pi-based robot](ai_agent/robot_car_2d.ipynb). (Credits for this implemention to [Thijs van de Laar](https://biaslab.github.io/member/thijs) and [Burak Ergul](https://biaslab.github.io/member/burak)). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>OPTIONAL SLIDES</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In an AIF Agent, Actions fulfill Expectations about the Future\n",
    "\n",
    "- In this cell, we will derive an alternative EFE decompostion that reveals how actions fulfill expectations about the future. \n",
    "\n",
    "- We consider again the EFE and factorize the generative model $p(x,s|u) = p^\\prime(x) p(s|x,u)$ as a product of a __target prior__ $p^\\prime(x)$ on observations and a __veridical__ state model $p(s|x,u)$. \n",
    "\n",
    "- Through the __target prior__ $p^\\prime(x)$, the agent declares which observations it **wants** to observe in the future. (The prime is just to distinguish the semantics of a desired future from the model for the actual future).\n",
    "\n",
    "- Through the __veridical__ state model $p(s|x,u)$ , the agent implicitly declares its beliefs about how the world will **actually** generate observations.\n",
    "  - In particular, note that through the equality (by Bayes rule)\n",
    "$$p(s|x,u) = \\frac{p(x|s)p(s|u)}{p(x|u)} = \\frac{p(x|s)p(s|u)}{\\sum_s p(x|s)p(s|u)}\\,,$$ \n",
    "it follows that in practice the agent may specify $p(s|x,u)$ implicitly by explicitly specifying a state transition model $p(s|u)$ and observation model $p(x|s)$. \n",
    "\n",
    "- Hence, an AIF agent holds both a model for its beliefs about how the world will actually evolve AND a model for its beliefs about how it desires the world to evolve. \n",
    "\n",
    "- To highlight the role of these two models in the EFE, consider the following alternative EFE decomposition:\n",
    "$$\\begin{aligned}\n",
    "G(u) &= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p^\\prime(x)p(s|x,u)} \\\\\n",
    "&= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p^\\prime(x)} \\frac{1}{p(s|x,u)}\\\\\n",
    "&= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p^\\prime(x)} \\frac{p(x|u)}{p(x|s)p(s|u)} \\quad \\text{(use Bayes)}\\\\\n",
    "&= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p(x|s)p(s|u)} \\frac{p(x|u)}{p^\\prime(x)} \\\\\n",
    "&= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p(x|s)p(s|u)} + \\sum_{x,s} q(x,s|u) \\log \\frac{p(x|u)}{p^\\prime(x)} \\\\\n",
    "&= \\sum_{x,s}  p(s|u) p(x|s) \\log \\frac{\\cancel{p(s|u)}}{p(x|s)\\cancel{p(s|u)}} + \\sum_{x,s} p(s|u) p(x|s) \\log \\frac{p(x|u)}{p^\\prime(x)} \\quad \\text{( assume }q(x,s|u)=p(x|s)p(s|u)\\text{ )}\\\\\n",
    "&= \\sum_{s}  p(s|u) \\sum_x p(x|s) \\log \\frac{1}{p(x|s)} + \\sum_x p(x|u) \\log \\frac{p(x|u)}{p^\\prime(x)} \\\\\n",
    "&= \\underbrace{E_{p(s|u)}\\left[ H[p(x|s)]\\right]}_{\\text{ambiguity}} + \\underbrace{D_{\\text{KL}}\\left[ p(x|u), p^\\prime(x)\\right]}_{\\text{risk}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- In this derivation, we have assumed that we can use the generative model to make inferences in the \"forward\" direction. Hence, $q(s|u)=p(s|u)$ and $q(x|s)=p(x|s)$.  \n",
    "\n",
    "- The terms \"ambiguity\" and \"risk\" have their origin in utility theory for behavioral ecocomics. Minimization of EFE leads to minimizing both ambiguity and risk.\n",
    "\n",
    "- Ambiguous (future) states are states that map to large uncertainties about (future) observations. We want to avoid those ambiguous states since it implies that the model is not capable to predict how the world evolves. Ambiguity can be resolved by selecting information-seeking (epistemic) actions. \n",
    "\n",
    "- Minimization of the second term (risk) leads to choosing actions ($u$) that align **predicted** future observations (represented by $p(x|u)$) with **desired** future observations (represented by $p^\\prime(x)$). Agents minimize risk by selecting pragmatic (goal-seeking) actions.\n",
    "\n",
    "- $\\Rightarrow$ Actions fulfill expectations about the future!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof $q^*(u) = \\arg\\min_q H[q] \\propto p(u)\\exp(-G(u))$ \n",
    "\n",
    "- We <a id='q-star'>proof</a> the \n",
    "- Consider the following decomposition:\n",
    "$$\\begin{aligned}\n",
    "H[q] &= \\sum_{x,s,u} q(x,s,u) \\log \\frac{q(s,u)}{p(x,s,u)} \\\\\n",
    "&= \\sum_{x,s,u} q(x,s|u) q(u) \\log \\frac{q(s|u) q(u)}{p(x,s|u) p(u)} \\\\\n",
    "&= \\sum_{u} q(u) \\bigg(\\sum_{x,s} q(x,s|u) \\log \\frac{q(s|u) q(u)}{p(x,s|u) p(u)}\\bigg) \\\\\n",
    "&= \\sum_{u} q(u) \\bigg( \\log q(u) + \\underbrace{\\log \\frac{1}{p(u)}}_{E(u)}+ \\underbrace{\\sum_{x,s} q(x,s|u) \\log \\frac{q(s|u)}{p(x,s|u)}}_{G(u)}\\bigg) \\\\\n",
    "&= \\sum_{u} q(u) \\log \\frac{q(u)}{\\exp\\left( -E(u) - G(u)\\right) }\n",
    "\\end{aligned}$$\n",
    "\n",
    "- This is a KL-divergence. Minimization of $H[q]$ leads to the following posterior for the policy:\n",
    "$$\\begin{aligned}\n",
    "q^*(u) &= \\arg\\min_q H[q] \\\\\n",
    "&= \\frac{1}{Z}\\exp(-E(u) -G(u)) \\\\\n",
    "&= \\frac{1}{Z}p(u)\\exp(-G(u))\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Specification of Free Energy \n",
    "\n",
    "- Consider the agent's inference task at time step $t$, right after having selected an action $a_t$ and having made an observation $y_t$.\n",
    "\n",
    "- As usual, we record actions and observations by substituting the values into the generative model(in the Act-Execute-Observe phase):\n",
    "$$\\begin{align*}\n",
    "p(x,s,u) &\\propto  \\underbrace{p(x_t=y_t|s_t)}_{\\text{observation}} p(s_t|s_{t-1},u_t) p(s_{t-1}) \\underbrace{p(u_t=a_t)}_{\\text{action}} \\\\ & \\quad \\cdot \\underbrace{\\prod_{k=t+1}^{t+T} p(x_k|s_k) p(s_k | s_{k-1}, u_k) p(u_k) p^+(x_k)}_{\\text{future}}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "- Note that (future) $x$ is also a latent variable and hence we include $x$ in the recognition model.  \n",
    "\n",
    "- This leads to the following free energy functional\n",
    "$$\\begin{align*}\n",
    "F[q] &\\propto \\sum_{x,s,u} q(x,s,u) \\log \\frac{q(x,s,u)}{p(x,s,u)} \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FE Decompositions \n",
    "\n",
    "- Lots of interesting FE decompositions are possible again. For instance\n",
    "$$\\begin{align*}\n",
    "F[q] &\\propto \\sum_{x,s,u} q(x,s,u) \\log \\frac{q(x,s,u)}{p(x,s,u)} \\\\\n",
    "&= \\sum_{u} q(u) \\underbrace{\\sum_{x,s} q(x,s|u)\\log \\frac{q(x,s|u)}{p(x,s|u)}}_{F_u[q]} + \\underbrace{\\sum_{u} q(u) \\log \\frac{q(u)}{p(u)}}_{\\text{complexity}}\n",
    "\\end{align*}$$\n",
    "breaks the FE into a complexity term and a term $F_u[q]$ that is conditioned on the policy $u$. \n",
    "\n",
    "- It can be shown (exercise) that the optimal posterior for the policy is now given by\n",
    "$$\n",
    "q^*(u) \\propto p(u) \\exp \\left( -F^*_u \\right)\n",
    "$$\n",
    "\n",
    "- Let's consider a break-up $x=(x_t,x_{>t})$ with $x_{>t} = (x_{t+1},\\ldots,x_{t+T})$ that recognizes the distinction between already observed and future data. Then\n",
    "$$\\begin{align*}\n",
    "F_u[q] &= \\underbrace{-\\log p(x_t)}_{\\substack{-\\log(\\text{evidence})  \\\\ \\text{(surprise)}}} + \\underbrace{\\sum_{x,s} q(x_{>t},s|u)\\log \\frac{q(x_{>t},s|u)}{p(x_{>t},s|u)}}_{\\substack{\\text{divergence}\\\\ \\text{(inference costs)}}}\\,.\n",
    "\\end{align*}$$\n",
    "\n",
    "- The inference costs (divergence term) can be further decomposed to \n",
    "$$\\begin{align*} \\underbrace{-\\sum_{x} q(x_{>t}) \\log p(x_{>t})}_{\\substack{\\text{expected surprise}  \\\\ \\text{(goal-directed, pragmatic costs)}}} + \\underbrace{\\sum_{x,s} q(x_{>t},s|u) \\log \\frac{q(x_{>t},s|u)}{p(s|x_{>t},u)}}_{\\text{epistemic costs}}\n",
    "\\end{align*}$$\n",
    "\n",
    "- Minimizing goal-directed costs selects actions that (expect to) fullfil the priors over future observations. Minimization of epistemic (\"knowledge seeking\") costs leads to actions that maximize information gain about the environmental dynamics. This can be seen by further decomposition of the epistemic costs into\n",
    "$$\\begin{align*}\n",
    "&\\sum_{x,s} q(x_>t,s|u) \\log \\frac{q(s|u)}{p(s|x_{>t},u)} + \\sum_{x,s} q(x_{>t},s|u) \\log q(x_{>t}|s,u) \\\\\n",
    "\\approx &\\underbrace{\\sum_{x,s} q(x_>t,s|u) \\log \\frac{q(s|u)}{q(s|x_{>t},u)}}_{-\\text{mutual information}} - \\underbrace{\\mathbb{E}_{q(s|u)}\\left[ H\\left[ q(x_{>t}|s,u)\\right]\\right]}_{\\text{ambiguity}} \n",
    "\\end{align*}$$\n",
    "where we used the approximation $q(s|x_{>t},u) \\approx p(s|x_{>t},u)$ to illuminate the link to the mutual information. \n",
    "\n",
    "- Minimizing FE leads (approximately) to mutual information maximization between internal states $s$ and observations $x$. In other words, FEM leads to actions that aim to seek out observations that are maximally informative about the hidden causes of these observations. \n",
    "\n",
    "- Ambiguous states have uncertain mappings to observations. Minimizing FE leads to actions that try to avoid ambiguous states. \n",
    "\n",
    "- In short, if the generative model includes variables that represent (yet) unobserved future observations, then action selection by FEM leads to a very sophisticated behavioral strategy that is maximally consistent with  \n",
    "  - Bayesian notions of model complexity\n",
    "  - evidence from past observations\n",
    "  - goal-directed imperatives by priors on future observations\n",
    "  - epistemic (knowledge seeking) value maximization, both in terms of MI maximization and avoidance of ambiguous states\n",
    "  \n",
    "- All these imperatives are simultaneously represented and automatically balanced against each other in a single time-varying cost function (Free Energy) that needs no tuning parameters. \n",
    "\n",
    "- (Just to be sure, you don't need to memorize these derivations nor are you expected to derive them on-the-spot. We present these decompositions only to provide insight into the multitude of forces that underlie FEM-based action selection.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Free energy distribution in FFG \n",
    "\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/ffg-active-inference-for-policy.png\" width=\"600px\"></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "footer.hidden-print {\n",
       "    display: none !important;\n",
       "}\n",
       "    \n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", read(f,String))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
