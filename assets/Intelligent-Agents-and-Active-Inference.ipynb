{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intelligent Agents and Active Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to Active Inference and application to the design of synthetic intelligent agents \n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "    - Karl Friston - 2016 - [The Free Energy Principle](https://www.youtube.com/watch?v=NIu_dJGyIQI) (video)\n",
    "  - Optional\n",
    "    - Raviv (2018), [The Genius Neuroscientist Who Might Hold the Key to True AI](./files/WIRED-Friston.pdf).\n",
    "        - Interesting article on Karl Friston, who is a leading theoretical neuroscientist working on a theory that relates life and intelligent behavior to physics (and Free Energy minimization). (**highly recommended**) \n",
    "    - Friston et al. (2022), [Designing Ecosystems of Intelligence from First Principles](https://arxiv.org/abs/2212.01354)\n",
    "        - Friston's vision on the future of AI. \n",
    "    - Van de Laar and De Vries (2019), [Simulating Active Inference Processes by Message Passing](https://www.frontiersin.org/articles/10.3389/frobt.2019.00020/full)\n",
    "        - How to implement active inference by message passing in a Forney-style factor graph.\n",
    "\n",
    "<!---\n",
    "  - References\n",
    "    - Friston (2013), [Life as we know it](https://royalsocietypublishing.org/doi/full/10.1098/rsif.2013.0475) \n",
    "    - Conant and Ashby (1970), [Every good regulator of a system must be a model of that system](https://www.tandfonline.com/doi/abs/10.1080/00207727008920220)\n",
    "--->    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agents\n",
    "\n",
    "- In the previous lessons we assumed that a data set was given. \n",
    "- In this lesson we consider _agents_. An agent is a system that _interacts_ with its environment through both sensors and actuators.\n",
    "- Crucially, by acting onto the environment, the agent is able to affect the data that it will sense in the future.\n",
    "  - As an example, by changing the direction where I look, I can affect the (visual) data that will be sensed by my retina.\n",
    "- With this definition of an agent, (biological) organisms are agents, and so are robots, self-driving cars, etc.\n",
    "- In an engineering context, we are particularly interesting in agents that behave with a *purpose* (with a goal in mind), e.g., to drive a car or to design a speech recognition algorithm.\n",
    "- In this lesson, we will describe how __goal-directed behavior__ by biological (and synthetic) agents can also be interpreted as minimization of a free energy functional. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustrative Example: Steering a cart to a parking spot\n",
    "\n",
    "- In this example, we consider a cart that can move in a 1D space. \n",
    "- At each time step the cart can be steered a bit to the left or right by a controller (the \"agent\"). The agent's knowledge about the cart's process dynamics (equations of motion) are known up to some additive Gaussian noise. The agent also makes noisy observations of the position and velocity of the cart. \n",
    "- Your challenge is to design an agent that steers the car to the zero position. (The agent should be specified as a probabilistic model and the control signal should be formulated as a Bayesian inference task).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<p style=\"text-align:center;\"><img src=\"./ai_agent/agent-cart-interaction.png\" width=\"600px\"></p>\n",
    "\n",
    "- Solution at the end of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Karl Friston and the Free Energy Principle\n",
    "\n",
    "- We begin with a motivating example that requires \"intelligent\" goal-directed decision making: assume that you are an owl and that you're hungry. What are you going to do?\n",
    "\n",
    "- Have a look at [Prof. Karl Friston](https://www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/)'s answer in this  [video segment on the cost function for intelligent behavior](https://youtu.be/L0pVHbEg4Yw). (**Do watch the video!**)\n",
    "\n",
    "- Friston argues that intelligent decision making (behavior, action making) by an agent requires *minimization of a functional of beliefs*. \n",
    "\n",
    "- Friston further argues (later in the lecture and his papers) that this functional is a (variational) free energy (to be defined below), thus linking decision-making and acting to Bayesian inference. \n",
    "\n",
    "- In fact, Friston's **Free Energy Principle** (FEP) claims that all [biological self-organizing processes (including brain processes) can be described as Free Energy minimization in a probabilistic model](https://royalsocietypublishing.org/doi/full/10.1098/rsif.2013.0475).\n",
    "  - This includes perception, learning, attention mechanisms, recall, acting and decision making, etc.\n",
    "  \n",
    "- Taking inspiration from FEP, if we want to develop synthetic \"intelligent\" agents, we have (only) two issues to consider:\n",
    "  1. Specification of the FE functional.\n",
    "  2. *How* to minimize the FE functional (often in real-time under situated conditions).  \n",
    "\n",
    "- Agents that follow the FEP are said to be involved in **Active Inference** (AIF). An AIF agent updates its states and parameters (and ultimately its model structure) solely by FE minimization, and selects its actions through (expected) FE minimization (to be explained below).    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution of an AIF Agent\n",
    "\n",
    "- Consider an AIF agent with observations (sensory states) $x_t$, latent internal states $s_t$ and latent control states $u_t$ for $t=1,2,\\ldots$. \n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/AIF-agent.png\" width=\"600px\"></p>\n",
    "\n",
    "- The agent is embedded in an environment with \"external states\" $\\tilde{s}_t$. The dynamics of the environment are driven by actions. \n",
    "\n",
    "- Actions $a_t$ are selected by the agent. Actions affect the environment and consequently affect future observations. \n",
    "\n",
    "- In pseudo-code, an AIF agent executes the <a id=\"AIF-algorithm\"></a>following algorithm:\n",
    "\n",
    "> **ACTIVE INFERENCE (AIF) AGENT ALGORITHM**    \n",
    ">\n",
    "> SPECIFY generative model $p(x,s,u)$    \n",
    "> ASSUME/SPECIFY environmental process $R$\n",
    ">\n",
    "> FORALL t DO    \n",
    ">     \n",
    "> 1.  $(x_t, \\tilde{s}_t) = R(a_t, \\tilde{s}_{t-1})$   % environment generates new observation     \n",
    "> 2.  $q(s_t) = \\arg\\min_q F[q]$         % update internal states (process observation)    \n",
    "> 3.  $q(u_{t+1}) = \\arg\\min_q H[q]$     % update control states (process observation)    \n",
    "> 4.  $\\hat{u}_{t+1} \\sim q(u_{t+1})$; $a_{t+1} = \\hat{u}_{t+1}$          % sample next action and push to environment     \n",
    ">       \n",
    "> END    \n",
    "\n",
    "\n",
    "- In the above algorithm, $F[q]$ and $H[q]$ are appropriately defined Free Energy functionals, to be discussed below. Next, we discuss these steps in more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generative Model in an AIF agent\n",
    "\n",
    "- What should the agent's model $p(x,s,u)$ be modeling? This question was (already) answered by [Conant and Ashby (1970)](https://www.tandfonline.com/doi/abs/10.1080/00207727008920220) as the [*good regulator theorem*](https://en.wikipedia.org/wiki/Good_regulator ): **every good regulator of a system must be a model of that system**. See the [OPTIONAL SLIDE for more information](#good-regulator-theorem). \n",
    "\n",
    "- Conant and Ashley state: \"The theorem has the interesting corollary that the living brain, so far as it is to be successful and efficient as a regulator for survival, __must__ proceed, in learning, by the formation of a model (or models) of its environment.\"\n",
    "\n",
    "- Indeed, perception in brains is clearly affected by predictions about sensory inputs by the brain's own generative model.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/the-gardener.png\" width=\"600px\"></p>\n",
    "\n",
    "- In the above picture (The Gardener, by Giuseppe Arcimboldo, ca 1590\n",
    "), on the left you will likely see a bowl of vegetables, while the same picture upside down elicits with most people the perception of a gardener's face rather than an upside-down vegetable bowl. \n",
    "\n",
    "- The reason is that the brain's model predicts to see straight-up faces with much higher probability than upside-down vegetable bowls. \n",
    "\n",
    "- So the <a id=\"model-specification\"></a> agent's model $p$ will be a model that aims to explain how environmental causes (latent states) lead to sensory observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specification of AIF Agent's model and Environmental Dynamics\n",
    "\n",
    "- In this notebook, for illustrative purposes, we specify the **generative model** at time step $t$ of an AIF agent as \n",
    "$$\n",
    "p(x_t,s_t,u_t|s_{t-1}) = \\underbrace{p(x_t|s_t)}_{\\text{observations}} \\cdot \\underbrace{p(s_t|s_{t-1},u_t)}_{\\substack{\\text{state} \\\\ \\text{transition}}} \\cdot \\underbrace{p(u_t)}_{\\substack{\\text{action} \\\\ \\text{prior}}}\n",
    "$$\n",
    "\n",
    "- We will assume that the agent interacts with an environment, which we represent by a dynamic model $R$ as\n",
    "$$\n",
    "(x_t,\\tilde{s}_t) = R\\left( a_t,\\tilde{s}_{t-1}\\right)$$\n",
    "where $a_t$ are _actions_ (by the agent), $x_t$ are _outcomes_ (the agent's observations) and $\\tilde{s}_t$ holds the environmental latent _states_. \n",
    "\n",
    "- Note that $R$ only needs to be specified for simulated environments. If we were to deploy the agent in a real-world environment, we would not need to specify $R$. \n",
    "\n",
    "- The agent's knowledge about environmental process $R$ is expressed by its generative model $p(x_t,s_t,u_t|s_{t-1})$. \n",
    "\n",
    "- Note that we distinguish between _control states_ and _actions_. Control states $u_t$ are latent variables in the agent's generative model. An action $a_t$ is a realization of a control state as observed by the environment. \n",
    "\n",
    "- Observations $x_t=\\hat{x}_t$ are generated by the environment and observed by the agent. Vice versa, actions $a_t = \\hat{u}_t$ are generated by the agent and observed by the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Updating in the AIF Agent\n",
    "\n",
    "- After the agent makes a new observation $x_t=\\hat{x}_t$, it will update beliefs over its latent variables. First the internal state variables $s$. \n",
    "\n",
    "- Assume the following at time step $t$:\n",
    "  - the state of the agent's model has already been updated to $q(s_{t-1}|\\hat{x}_{1:t-1})$. \n",
    "  - the agent has selected a new action $a_t=\\hat{u}_t$.\n",
    "  - the agent has recorded a new observation $x_t=\\hat{x}_t$. \n",
    "\n",
    "- The **state updating** task is to infer $q(s_{t}|\\hat{x}_{1:t})$, based on the previous estimate $q(s_{t-1}|\\hat{x}_{1:t-1})$, the new data $\\{a_t=\\hat{u}_t,x_t=\\hat{x}_{t}\\}$, and the agent's generative model. \n",
    "\n",
    "- Technically, this is a Bayesian filtering task. In a real brain, this process is called **perception**.   \n",
    "\n",
    "- We specify the following FE functional\n",
    "$$\n",
    "F[q] = \\sum_{s_t} q(s_t|\\hat{x}_{1:t}) \\log \\frac{\\overbrace{q(s_t|\\hat{x}_{1:t})}^{\\text{state posterior}}}{\\underbrace{p(\\hat{x}_t|s_t) p(s_t|s_{t-1},\\hat{u}_t)}_{\\text{generative model w new data}} \\underbrace{q(s_{t-1}|\\hat{x}_{1:t-1})}_{\\text{state prior}}}\n",
    "$$\n",
    "\n",
    "- The state updating task can be formulated as minimization of the above FE (see also [AIF Algorithm](#AIF-algorithm), step 2):\n",
    "$$\n",
    "q(s_t|\\hat{x}_{1:t}) = \\arg\\min_q F[q]\n",
    "$$\n",
    "\n",
    "- In case the generative model is a _Linear Gaussian Dynamical System_, minimization of the FE can be solved analytically in closed-form and [leads to the standard Kalman filter](https://nbviewer.jupyter.org/github/bertdv/BMLIP/blob/master/lessons/notebooks/Dynamic-Models.ipynb#kalman-filter). \n",
    "\n",
    "- In case these (linear Gaussian) conditions are not met, we can still minimize the FE by other means and arrive at some approximation of the Kalman filter, see for example [Baltieri and  Isomura (2021)](https://arxiv.org/abs/2111.10530) for a Laplace approximation to variational Kalman filtering.  \n",
    "\n",
    "- Our toolbox [RxInfer](http://rxinfer.ml) specializes in automated execution of  this minimization task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Updating in an AIF Agent\n",
    "\n",
    "- Once the agent has updated its internal states, it will turn to inferring the next action. \n",
    "\n",
    "- In order to select a __good__ next action, we need to investigate and compare consequences of a _sequence_ of future actions. \n",
    "\n",
    "- A sequence of future actions $a= (a_{t+1}, a_{t+2}, \\ldots, a_{t+T})$ is called a **policy**. Since relevant consequences are usually the result of an future action sequence rather than a single action, we will be interested in updating beliefs over policies. \n",
    "\n",
    "- In order to assess the consequences of a selected policy, we will, as a function of that policy, run the generative model forward-in-time to make predictions about future observations $x_{t+1:T}$. \n",
    "\n",
    "- Note that perception (state updating) preceeds policy updating. In order to accurately predict the future, the agent first needs to understand the current state of the world.  \n",
    "\n",
    "- Consider an AIF agent at time step $t$ with (future) observations $x = (x_{t+1}, x_{t+2}, \\ldots, x_{t+T})$,  latent future internal states $s= (s_t, s_{t+1}, \\ldots, s_{t+T})$, and latent future control variables $u= (u_{t+1}, u_{t+2}, \\ldots, u_{t+T})$. \n",
    "\n",
    "- From the agent's viewpoint, the evolution of these future variables are constrained by its generative model, rolled out into the future:\n",
    "$$\\begin{align*}\n",
    "p(x,s,u) &= \\underbrace{q(s_{t})}_{\\substack{\\text{current}\\\\ \\text{state}}} \\cdot \\underbrace{\\prod_{k=t+1}^{t+T} p(x_k|s_k) \\cdot p(s_k | s_{k-1}, u_k) p(u_k)}_{\\text{GM roll-out to future}}\n",
    "\\end{align*}$$\n",
    "\n",
    "- Consider the Free Energy functional for estimating posterior beliefs $q(s,u)$ over future states and control signals: \n",
    "$$\\begin{align*}\n",
    "H[q] &= \\overbrace{\\sum_{x,s} q(x|s)}^{\\text{marginalize }x} \\bigg( \\overbrace{\\sum_u q(s,u) \\log \\frac{q(s,u)}{p(x,s,u)} }^{\\text{variational Free Energy}}\\bigg) \\\\\n",
    "&= \\sum_{x,s,u} q(x,s,u) \\log \\frac{q(s,u)}{p(x,s,u)}\n",
    "\\end{align*}$$\n",
    "\n",
    "- In principle, this is a regular FE functional, with one difference to previous versions: since future observations $x$ have not yet occurred, $H[q]$ marginalizes not only over latent states $s$ and policies $u$, but also over future observations $x$.\n",
    "\n",
    "- We will update the beliefs over policies by minimization of Free Energy functional $H[q]$. In the [optional slides below, we prove that the solution to this optimization task](#q-star) is given by (see [AIF Algorithm](#AIF-algorithm), step 3, above)\n",
    "$$\\begin{aligned}\n",
    "q^*(u) &= \\arg\\min_q H[q] \\\\\n",
    "&\\propto p(u)\\exp(-G(u))\\,,\n",
    "\\end{aligned}$$\n",
    "<a id='q-star-main-cell'></a> where the factor $p(u)$ is a prior over admissible policies, and the factor $\\exp(-G(u))$ updates the prior with information about future consequences of a selected policy $u$. \n",
    "\n",
    "- The function \n",
    "$$G(u) = \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p(x,s|u)}$$ \n",
    "is called the **Expected Free Energy** (EFE) for policy $u$. \n",
    "\n",
    "- The FEP takes the following stance: if FE minimization is all that an agent does, then the only consistent and appropriate behavior for an agent is to select actions that minimize the **expected** Free Energy in the future (where expectation is taken over current beliefs about future observations). \n",
    "\n",
    "- Note that, since $q^*(u) \\propto p(u)\\exp(-G(u))$, the probability $q^*(u)$ for selecting a policy $u$ increases when EFE $G(u)$ gets smaller. \n",
    "\n",
    "- Once the policy (control) variables have been updated, in simulated environments, it is common to assume that the next action $a_{t+1}$ (an action is the _observed_ control variable by the environment) gets selected in proportion to the probability of the related control variable (see [AIF Agent Algorithm](#AIF-algorithm), step 4, above), i.e.,\n",
    "$$\\begin{aligned}\n",
    "\\hat{u}_{t+1} &\\sim q(u_{t+1}) \\qquad &&\\text{(select control realization by sampling)} \\\\\n",
    "a_{t+1} &= \\hat{u}_{t+1} \\qquad &&\\text{(transfer to environment)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Next, we analyze some properties of the EFE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Inference Analysis: exploitation-exploration dilemma \n",
    "\n",
    "- Consider the following decomposition of EFE:\n",
    "$$\\begin{aligned}\n",
    "G(u) &= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p(x,s|u)} \\\\\n",
    "&= \\sum_{x,s} q(x,s|u) \\log \\frac{1}{p(x)} + \\sum_{x,s} q(x,s|u) \\log \\frac{q(s|u)}{p(s|x,u)}\\frac{q(s|x)}{q(s|x)} \\\\\n",
    "&= \\sum_x q(x|u) \\log \\frac{1}{p(x)} + \\sum_{x,s} q(x,s|u) \\log \\frac{q(s|u)}{q(s|x)} + \\underbrace{\\sum_{x,s} q(x,s|u) \\log \\frac{q(s|x)}{p(s|x,u)}}_{E\\left[ D_{\\text{KL}}[q(s|x),p(s|x,u)] \\right]\\geq 0} \\\\\n",
    "&\\geq \\underbrace{\\sum_x q(x|u) \\log \\frac{1}{p(x)}}_{\\substack{\\text{goal-seeking behavior} \\\\ \\text{(exploitation)}}} - \\underbrace{\\sum_{x,s} q(x,s|u) \\log \\frac{q(s|x)}{q(s|u)}}_{\\substack{\\text{information-seeking behavior}\\\\ \\text{(exploration)}}} \n",
    "\\end{aligned}$$ \n",
    "\n",
    "- Apparently, minimization of EFE leads to selection of policies that balances the following two imperatives: \n",
    "\n",
    "  1. minimization of the first term of $G(u)$, i.e. minimizing $\\sum_x q(x|u) \\log \\frac{1}{p(x)}$, leads to policies ($u$) that align the inferred observations $q(x|u)$ under policy $u$ (i.e., predicted future observations under policy $u$) with a prior $p(x)$ on future observations. We are in control to choose any prior $p(x)$ and usually we choose a prior that aligns with desired (goal) observations. Hence, policies with low EFE leads to **<a id=\"goal-seeking\">goal-seeking behavior</a>** (a.k.a. pragmatic behavior or exploitation). [In the OPTIONAL SLIDES](#ambiguity-plus-risk), we derive an alternative (perhaps clearer) expression to support this interpretation]. \n",
    "  \n",
    "  1. minimization of $G(u)$ maximizes the second term \n",
    "  $$\\begin{aligned}\n",
    "  \\sum_{x,s} q(x,s|u) \\log \\frac{q(s|x)}{q(s|u)} &= \\sum_{x,s} q(x,s|u) \\log \\frac{q(s|x)}{q(s|u)}\\frac{q(x|u)}{q(x|u)} \\\\\n",
    "  &= \\underbrace{\\sum_{x,s} q(x,s|u) \\log \\frac{q(x,s|u)}{q(x|u)q(s|u)}}_{\\text{(conditional) mutual information }I[x,s|u]}\n",
    "  \\end{aligned}$$ \n",
    "  which is the (conditional) [__mutual information__](https://en.wikipedia.org/wiki/Mutual_information) between (posteriors on) future observations and states, for a given policy $u$. Thus, maximizing this term leads to actions that maximize statistical dependency between future observations and states. In other words, a policy with low EFE also leads to **information-seeking behavior** (a.k.a. epistemic behavior or exploration). \n",
    "\n",
    "- (The third term $\\sum_{x,s} q(x,s|u) \\log \\frac{q(s|x)}{p(s|x)}$ is an (expected) KL divergence between posterior and prior on the states. This can be interpreted as a complexity/regularization term and $G(u)$ minimization will drive this term to zero.)   \n",
    "\n",
    "- Seeking actions that balance goal-seeking behavior (exploitation) and information-seeking behavior (exploration) is a [fundamental problem in the Reinforcement Learning literature](http://tomstafford.staff.shef.ac.uk/?p=48). \n",
    "\n",
    "- **Active Inference solves the exploration-exploitation dilemma**. Both objectives are served by EFE minimization without need for any tuning parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIF Agents learn both the Problem and Solution\n",
    "\n",
    "- We highlight another great feature of FE minimizing agents. Consider an AIF agent ($m$) with generative model $p(x,s,u|m)$.\n",
    "\n",
    "- Consider the Divergence-Evidence decomposition of the FE again:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "F[q] &= \\sum_{s,u} q(s,u) \\log \\frac{q(s,u)}{p(x,s,u|m)} \\\\\n",
    "&= \\underbrace{-\\log p(x|m)}_{\\substack{\\text{problem} \\\\ \\text{representation costs}}} + \\underbrace{\\sum_{s,u} q(s,u) \\log \\frac{q(s,u)}{p(s,u|x,m)}}_{\\text{solution costs}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- The first term, $-\\log p(x|m)$, is the (negative log-) evidence for model $m$, given recorded data $x$. \n",
    "\n",
    "- Minimization of FE maximizes the evidence for the given model. The model captures the  **problem representation**. A model with high evidence predicts the data well and therefore \"understands the world\".  \n",
    "\n",
    "- The second term scores the cost of inference. In almost all cases, the solution to a problem can be phrased as an inference task on the generative model. Hence, the second term **scores the accuracy of the inferred solution**, for the given model. \n",
    "\n",
    "- FE minimization optimizes a balanced trade-off between a good-enough problem representation and a good-enough solution proposal for that model. Since FE comprises both a cost for solution _and_ problem representation, it is a neutral criterion that applies across a very wide set of problems. \n",
    "\n",
    "- A good solution to the wrong problem is not good enough. A poor solution to a great problem statement is not sufficient either.  In order to solve a problem well, we need both to represent the problem correctly (high model evidence) and we need to solve it well (low inference costs). \n",
    "\n",
    "<!---\n",
    "- Question: does this argument suggest that FE is even more fundamental as a model performance criterion than the model evidence by itself?\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Brain's Action-Perception Loop by FE Minimization\n",
    "\n",
    "- The above derivations are not trivial, but we have just shown that FE minimizing agents accomplish variational Bayesian perception (a la Kalman filtering), and a balanced exploration-exploitation trade-off for policy selection. \n",
    "\n",
    "- Moreover, the FE by itself serves as a proper objective across a very wide range of problems, since it scores both the cost of the problem statement and the cost of inferring the solution. \n",
    "\n",
    "- The current FEP theory claims that minimization of FE (and EFE) is all that brains do, i.e., FE minimization leads to perception, policy selection, learning, structure adaptation, attention, learning of problems and solutions, etc.\n",
    "\n",
    "<p style=\"text-align:center;\"><img style=\"border:2px solid #000000; width:auto\" src=\"./figures/brain-design-cycle.png\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Engineering Challenge: Synthetic AIF Agents \n",
    "\n",
    "- So we have here an AI framework (minimization of FE and associated EFE) that\n",
    "  - leads to optimal (Bayesian) information processing, including balancing accuracy vs complexity.\n",
    "  - leads to balanced and continual learning of both problem representation and solution proposal\n",
    "  - actively selects data in-the-field under situated conditions (no dependency on large data base)\n",
    "  - pursues a optimal trade-off between exploration (information-seeking) and exploitation (goal-seeking) behavior\n",
    "  - needs no external tuning parameters (such as step sizes, thresholds, etc.)\n",
    "  - has a strong foundations in how nature/life self-organizes\n",
    "\n",
    "- Clearly, the FEP, and synthetic AIF agents as a realization of FEP, comprise a very attractive framework for all things relating to AI and AI agents. \n",
    "\n",
    "- A current big AI challenge is to design synthetic AIF agents based solely on FE/EFE minimization.\n",
    "\n",
    "<p style=\"text-align:center;\"><img style=\"border:2px solid #000000; width:auto\" src=\"./figures/Synthetic-FEP-agent.png\"></p> \n",
    "\n",
    "- Executing a synthetic AIF agent often poses a large computational problem because of the following reasons: \n",
    "   1. For interesting problems (e.g. speech recognition, scene analysis), generative models may contain thousands of latent variables. \n",
    "   2. The FE function is a time-varying function, since it is also a function of observable variables. \n",
    "   3. An AIF agent must execute inference in real-time if it is engaged and embedded in a real world environment.\n",
    "   \n",
    "- So, in practice, executing a synthetic AIF agent may lead to a **task of minimizing a time-varying FE function of thousands of variables in real-time**!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Factor Graph Approach to Modeling of an Active Inference Agent\n",
    "\n",
    "- How to specify and execute a synthetic AIF agent is an active area of research. \n",
    "\n",
    "- There is no definitive solution approach to AIF agent modeling yet; we ([BIASlab](http://biaslab.org)) think that (reactive) message passing in a factor graph representation provides a promising path. \n",
    "\n",
    "- After selecting an action $a_t$ and making an observation $x_t$, the FFG for the rolled-out generative model is given by the following FFG:\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/fig-active-inference-model-specification.png\" width=\"600px\"></p>\n",
    "\n",
    "- The open red nodes for $p(x_{t+k})$ specify __desired future observations__, whereas the open black boxes for $p(s_k|s_{k-1},u_k)$ and $p(x_k|s_k)$ reflect the agent's beliefs about how the world actually evolves (ie, the __veridical model__). \n",
    "\n",
    "- The (brown) dashed box is the agent's Markov blanket. Given the states on the Markov blanket, the internal states of the agent are independent of the state of the world.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to minimize FE: Online Active Inference\n",
    "\n",
    "- [Online active inference proceeds by iteratively executing three stages](https://www.frontiersin.org/articles/10.3389/frobt.2019.00020/full): \n",
    "  1. act-execute-observe     \n",
    "  2. update the latent variables and select an action    \n",
    "  3. slide forward\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/fig-online-active-inference.png\" width=\"700px\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Cart Parking Problem Revisited\n",
    "\n",
    "Here we solve the cart parking problem as stated at the beginning of this lesson. We first specify a generative model for the agent's environment (which is the observed noisy position of the cart) and then constrain future observations by a prior distribution that is located on the target parking spot. Next, we schedule a message passing-based inference algorithm for the next action. This is followed by executing the _Act-execute-observe --> infer --> slide_ procedure to infer a sequence of consecutive actions. Finally, the position of the cart over time is plotted. Note that the cart converges to the target spot.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg;Pkg.activate(\"probprog/workspace\");Pkg.instantiate()\n",
    "IJulia.clear_output();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using PyPlot, LinearAlgebra, ForneyLab\n",
    "# Load helper functions. Feel free to explore these\n",
    "include(\"ai_agent/environment_1d.jl\")\n",
    "include(\"ai_agent/helpers_1d.jl\")\n",
    "include(\"ai_agent/agent_1d.jl\")\n",
    "\n",
    "# Internal model perameters\n",
    "gamma   = 100.0 # Transition precision\n",
    "phi     = 10.0 # Observation precision\n",
    "upsilon = 1.0 # Control prior variance\n",
    "sigma   = 1.0 # Goal prior variance\n",
    "\n",
    "T = 10 # Lookahead\n",
    "\n",
    "# Build internal model\n",
    "fg = FactorGraph()\n",
    "\n",
    "o = Vector{Variable}(undef, T) # Observed states\n",
    "s = Vector{Variable}(undef, T) # internal states\n",
    "u = Vector{Variable}(undef, T) # Control states\n",
    "\n",
    "@RV s_t_min ~ GaussianMeanVariance(placeholder(:m_s_t_min),\n",
    "                                   placeholder(:v_s_t_min)) # Prior  state\n",
    "u_t = placeholder(:u_t)\n",
    "@RV u[1] ~ GaussianMeanVariance(u_t, tiny)\n",
    "@RV s[1] ~ GaussianMeanPrecision(s_t_min + u[1], gamma) \n",
    "@RV o[1] ~ GaussianMeanPrecision(s[1], phi)\n",
    "placeholder(o[1], :o_t)\n",
    "\n",
    "s_k_min = s[1]\n",
    "for k=2:T\n",
    "    @RV u[k] ~ GaussianMeanVariance(0.0, upsilon) # Control prior\n",
    "    @RV s[k] ~ GaussianMeanPrecision(s_k_min + u[k], gamma) # State transition model\n",
    "    @RV o[k] ~ GaussianMeanPrecision(s[k], phi) # Observation model\n",
    "    GaussianMeanVariance(o[k], \n",
    "                         placeholder(:m_o, var_id=:m_o_*k, index=k-1),\n",
    "                         placeholder(:v_o, var_id=:v_o_*k, index=k-1)) # Goal prior\n",
    "    s_k_min = s[k]\n",
    "end\n",
    "\n",
    "# Schedule message passing algorithm\n",
    "algo = messagePassingAlgorithm(u[2]) # Infer internal states\n",
    "source_code = algorithmSourceCode(algo)\n",
    "eval(Meta.parse(source_code)) # Loads the step!() function for inference\n",
    "\n",
    "s_0 = 2.0 # Initial State\n",
    "\n",
    "N = 20 # Total simulation time\n",
    "\n",
    "(execute, observe)  = initializeWorld() # Let there be a world\n",
    "(infer, act, slide) = initializeAgent() # Let there be an agent\n",
    "\n",
    "# Step through action-perception loop\n",
    "u_hat = Vector{Float64}(undef, N) # Actions\n",
    "o_hat = Vector{Float64}(undef, N) # Observations\n",
    "for t=1:N\n",
    "    u_hat[t] = act() # Evoke an action from the agent\n",
    "               execute(u_hat[t]) # The action influences hidden external states\n",
    "    o_hat[t] = observe() # Observe the current environmental outcome (update p)\n",
    "               infer(u_hat[t], o_hat[t]) # Infer beliefs from current model state (update q)\n",
    "               slide() # Prepare for next iteration\n",
    "end\n",
    "\n",
    "# Plot active inference results\n",
    "plotTrajectory(u_hat, o_hat)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video\n",
    "\n",
    "- See [this video to verfiy that the robot will be steered to the correct parking spot even after an \"adversarial\" intervention](https://youtu.be/0ABZJJ9r4Dw) :). (This project was completed by [Burak Ergul](https://www.linkedin.com/in/burak-ergul-/) as part of this MSc graduation project). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Extensions and Comments\n",
    "\n",
    "- If interested, here is a link to [a more detailed version of the 1D parking problem](ai_agent/robot_car_1d.ipynb). \n",
    "\n",
    "- We also have a [2D version of this cart parking problem implemented on Raspberry Pi-based robot](ai_agent/robot_car_2d.ipynb). (Credits for this implemention to [Thijs van de Laar](https://biaslab.github.io/member/thijs) and [Burak Ergul](https://www.linkedin.com/in/burak-ergul-/)). \n",
    "\n",
    "- Just to be sure, you don't need to memorize all FE/EFE decompositions nor are you expected to derive them on-the-spot. We present these decompositions only to provide insight into the multitude of forces that underlie FEM-based action selection.\n",
    "\n",
    "- In a sense, the FEP is an umbrella for describing the mechanics and self-organization of intelligent behavior, in man and machines. Lots of sub-fields in AI, such as reinforcement learning, can be interpreted as a special case of active inference under the FEP, see e.g., [Friston et al., 2009](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0006421). \n",
    "\n",
    "- Is EFE minimization really different from \"regular\" FE minimization? Not really, it appears that [EFE minimization can be reformulated as a special case of FE minimization](https://link.springer.com/article/10.1007/s00422-019-00805-w). In other words, FE minimization is still the only game in town.\n",
    "\n",
    "- Active inference also completes the \"scientific loop\" picture. Under the FEP, experimental/trial design is driven by EFE minimization. Bayesian probability theory (and FEP) contains all the equations for running scientific inquiry.\n",
    "<p style=\"text-align:center;\"><img style=\"width:auto\" src=\"./figures/scientific-inquiry-loop-complete.png\"></p>\n",
    "\n",
    "- The big engineering challenge remains the computational load of AIF. The human brain consumes about 25 Watt and the neocortex only about 4 Watt (which is about the power consumption of a bicycle light). This is multiple orders of magnitude cheaper than what we can engineer on silicon for similar tasks.    \n",
    "\n",
    "<!--- \n",
    "- In the next class, we make a lightweight excursion to brains and life. What is life? \n",
    "--->\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Thoughts\n",
    "\n",
    "- In the end, all the state inference, parameter estimation, etc., in this lecture series could have been implemented by FE minimization in an appropriately specified generative probabilistic model. However, the Free Energy Principle extends beyond state and parameter estimation. Driven by FE minimization, brains change their structure as well over time. In fact, the FEP extends beyond brains to a general theory for biological self-organization, e.g., [Darwin's natural selection process](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5857288/) may be interpreted as a FE minimization-driven model optimization process, and here's an article on [FEP for predictive processing in plants](https://royalsocietypublishing.org/doi/10.1098/rsif.2017.0096). Moreover, Constrained-FE minimization (rephrased as the Principle of Maximum Relative Entropy) provides an elegant framework to derive most (if not all) physical laws, as Caticha exposes in his [brilliant monograph](./files/Caticha-2012-Entropic-Inference-and-the-Foundations-of-Physics) on Entropic Physics. Indeed, the framework of FE minimization is known in the physics community as the very fundamental [Principle of Least Action](https://en.wikipedia.org/wiki/Stationary-action_principle) that governs the equations-of-motion in nature. \n",
    "\n",
    "- So, the FEP is very fundamental and extends way beyond applications to machine learning. At [our research lab](http://biaslab.org) at TU/e, we work on developing FEP-based intelligent agents that go out into the world and autonomously learn to accomplish a pre-determined task, such as learning-to-walk or learning-to-process-noisy-speech-signals. Free free to approach us if you want to know more about that effort.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <center>OPTIONAL SLIDES</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\require{cancel}$$ \n",
    "\n",
    "### In an AIF Agent, Actions fulfill Desired Expectations about the Future\n",
    "\n",
    "- In the [derivations above](#goal-seeking), we decomposed the EFE into an upperbound on the sum of a goal-seeking and information-seeking term. Here, we derive an alternative (exact) decomposition that more clearly reveals the goal-seeking objective</a>.\n",
    "\n",
    "- We consider again the EFE and factorize the generative model $p(x,s|u) = p^\\prime(x) p(s|x,u)$ as a product of a __target prior__ $p^\\prime(x)$ on observations and a __veridical__ state model $p(s|x,u)$. \n",
    "\n",
    "- Through the __target prior__ $p^\\prime(x)$, the agent declares which observations it **wants** to observe in the future. (The prime is just to distinguish the semantics of a desired future from the model for the actual future).\n",
    "\n",
    "- Through the __veridical__ state model $p(s|x,u)$ , the agent implicitly declares its beliefs about how the world will **actually** generate observations.\n",
    "  - In particular, note that through the equality (by Bayes rule)\n",
    "$$p(s|x,u) = \\frac{p(x|s)p(s|u)}{p(x|u)} = \\frac{p(x|s)p(s|u)}{\\sum_s p(x|s)p(s|u)}\\,,$$ \n",
    "it follows that in practice the agent may specify $p(s|x,u)$ implicitly by explicitly specifying a state transition model $p(s|u)$ and observation model $p(x|s)$. \n",
    "\n",
    "- Hence, an AIF agent holds both a model for its beliefs about how the world will actually evolve AND a model for its beliefs about how it desires the world to evolve!! \n",
    "\n",
    "- <a id=\"ambiguity-plus-risk\"></a> To highlight the role of these two models in the EFE, consider the following alternative EFE decomposition:\n",
    "$$\\begin{aligned}\n",
    "G(u) &= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p^\\prime(x)p(s|x,u)} \\\\\n",
    "&= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p^\\prime(x)} \\frac{1}{p(s|x,u)}\\\\\n",
    "&= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p^\\prime(x)} \\frac{p(x|u)}{p(x|s)p(s|u)} \\quad \\text{(use Bayes)}\\\\\n",
    "&= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p(x|s)p(s|u)} \\frac{p(x|u)}{p^\\prime(x)} \\\\\n",
    "&= \\sum_{x,s}  q(x,s|u) \\log \\frac{q(s|u)}{p(x|s)p(s|u)} + \\sum_{x,s} q(x,s|u) \\log \\frac{p(x|u)}{p^\\prime(x)} \\\\\n",
    "&= \\sum_{x,s}  p(s|u) p(x|s) \\log \\frac{\\cancel{p(s|u)}}{p(x|s)\\cancel{p(s|u)}} + \\sum_{x,s} p(s|u) p(x|s) \\log \\frac{p(x|u)}{p^\\prime(x)} \\quad \\text{( assume }q(x,s|u)=p(x|s)p(s|u)\\text{ )}\\\\\n",
    "&= \\sum_{s}  p(s|u) \\sum_x p(x|s) \\log \\frac{1}{p(x|s)} + \\sum_x p(x|u) \\log \\frac{p(x|u)}{p^\\prime(x)} \\\\\n",
    "&= \\underbrace{E_{p(s|u)}\\left[ H[p(x|s)]\\right]}_{\\text{ambiguity}} + \\underbrace{D_{\\text{KL}}\\left[ p(x|u), p^\\prime(x)\\right]}_{\\text{risk}}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- In this derivation, we have assumed that we can use the generative model to make inferences in the \"forward\" direction. Hence, $q(s|u)=p(s|u)$ and $q(x|s)=p(x|s)$.  \n",
    "\n",
    "- The terms \"ambiguity\" and \"risk\" have their origin in utility theory for behavioral ecocomics. Minimization of EFE leads to minimizing both ambiguity and risk.\n",
    "\n",
    "- Ambiguous (future) states are states that map to large uncertainties about (future) observations. We want to avoid those ambiguous states since it implies that the model is not capable to predict how the world evolves. Ambiguity can be resolved by selecting information-seeking (epistemic) actions. \n",
    "\n",
    "- Minimization of the second term (risk) leads to choosing actions ($u$) that align **predicted** future observations (represented by $p(x|u)$) with **desired** future observations (represented by $p^\\prime(x)$). Agents minimize risk by selecting pragmatic (goal-seeking) actions.\n",
    "\n",
    "- $\\Rightarrow$ **Actions fulfill desired expectations about the future!**\n",
    "\n",
    "- ([return to related cell in main text](#goal-seeking))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof $q^*(u) = \\arg\\min_q H[q] \\propto p(u)\\exp(-G(u))$ \n",
    "\n",
    "- <a id='q-star'></a>Consider the following decomposition:\n",
    "$$\\begin{aligned}\n",
    "H[q] &= \\sum_{x,s,u} q(x,s,u) \\log \\frac{q(s,u)}{p(x,s,u)} \\\\\n",
    "&= \\sum_{x,s,u} q(x,s|u) q(u) \\log \\frac{q(s|u) q(u)}{p(x,s|u) p(u)} \\\\\n",
    "&= \\sum_{u} q(u) \\bigg(\\sum_{x,s} q(x,s|u) \\log \\frac{q(s|u) q(u)}{p(x,s|u) p(u)}\\bigg) \\\\\n",
    "&= \\sum_{u} q(u) \\bigg( \\log q(u) + \\log \\frac{1}{p(u)}+ \\underbrace{\\sum_{x,s} q(x,s|u) \\log \\frac{q(s|u)}{p(x,s|u)}}_{G(u)}\\bigg) \\\\\n",
    "&= \\sum_{u} q(u) \\log \\frac{q(u)}{p(u)\\exp\\left(- G(u)\\right) }\n",
    "\\end{aligned}$$\n",
    "\n",
    "- This is a KL-divergence. Minimization of $H[q]$ leads to the following posterior for the policy:\n",
    "$$\\begin{aligned}\n",
    "q^*(u) &= \\arg\\min_q H[q] \\\\\n",
    "&= \\frac{1}{Z}p(u)\\exp(-G(u))\n",
    "\\end{aligned}$$\n",
    "\n",
    "- [(click to return to linked cell in the main text.)](#q-star-main-cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What Makes a Good Agent? [The Good Regulator Theorem](https://en.wikipedia.org/wiki/Good_regulator)\n",
    "\n",
    "- <a id=\"good-regulator-theorem\"></a> According to Friston, an ``intelligent'' agent like a brain minimizes a variational free energy functional, which, in general, is a functional of a probability distribution $p$ and a variational posterior $q$. \n",
    "\n",
    "- What should the agent's model $p$ be modeling? This question was (already) answered by [Conant and Ashby (1970)](https://www.tandfonline.com/doi/abs/10.1080/00207727008920220) as the Good Regulator Theorem: **every good regulator of a system must be a model of that system**. \n",
    "  \n",
    "- A Quote from Conant and Ashby's paper (this statement was later finessed by [Friston (2013)](https://royalsocietypublishing.org/doi/full/10.1098/rsif.2013.0475)): \n",
    "> \"The theory has the interesting corollary that the living brain, insofar as it is successful and efficient as a regulator for survival, *must* proceed, in learning, by the formation of a model (or models) of its environment.\"\n",
    "\n",
    "<p style=\"text-align:center;\"><img style=\"border:2px solid #000000; width:500px\" src=\"./figures/good-regulator.png\"></p>\n",
    "\n",
    "- ([Return to related cell in main text](#model-specification)).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\r\n",
       "This HTML file contains custom styles and some javascript.\r\n",
       "Include it a Jupyter notebook for improved rendering.\r\n",
       "-->\r\n",
       "\r\n",
       "<!-- Fonts -->\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\r\n",
       "\r\n",
       "<!-- Custom style -->\r\n",
       "<style>\r\n",
       "\r\n",
       "@font-face {\r\n",
       "    font-family: \"Computer Modern\";\r\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\r\n",
       "}\r\n",
       "\r\n",
       "#notebook_panel { /* main background */\r\n",
       "    background: rgb(245,245,245);\r\n",
       "}\r\n",
       "\r\n",
       "div.container {\r\n",
       "    min-width: 960px;\r\n",
       "}\r\n",
       "\r\n",
       "div #notebook { /* centre the content */\r\n",
       "    background: #fff; /* white background for content */\r\n",
       "    margin: auto;\r\n",
       "    padding-left: 0em;\r\n",
       "}\r\n",
       "\r\n",
       "#notebook li { /* More space between bullet points */\r\n",
       "    margin-top:0.8em;\r\n",
       "}\r\n",
       "\r\n",
       "/* draw border around running cells */\r\n",
       "div.cell.border-box-sizing.code_cell.running {\r\n",
       "    border: 1px solid #111;\r\n",
       "}\r\n",
       "\r\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\r\n",
       "div.cell.code_cell {\r\n",
       "    background-color: rgb(256,256,256);\r\n",
       "    border-radius: 0px;\r\n",
       "    padding: 0.5em;\r\n",
       "    margin-left:1em;\r\n",
       "    margin-top: 1em;\r\n",
       "}\r\n",
       "\r\n",
       "div.text_cell_render{\r\n",
       "    font-family: 'Alegreya Sans' sans-serif;\r\n",
       "    line-height: 140%;\r\n",
       "    font-size: 125%;\r\n",
       "    font-weight: 400;\r\n",
       "    width:800px;\r\n",
       "    margin-left:auto;\r\n",
       "    margin-right:auto;\r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "/* Formatting for header cells */\r\n",
       ".text_cell_render h1 {\r\n",
       "    font-family: 'Nixie One', serif;\r\n",
       "    font-style:regular;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 45pt;\r\n",
       "    line-height: 100%;\r\n",
       "    color: rgb(0,51,102);\r\n",
       "    margin-bottom: 0.5em;\r\n",
       "    margin-top: 0.5em;\r\n",
       "    display: block;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h2 {\r\n",
       "    font-family: 'Nixie One', serif;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 30pt;\r\n",
       "    line-height: 100%;\r\n",
       "    color: rgb(0,51,102);\r\n",
       "    margin-bottom: 0.1em;\r\n",
       "    margin-top: 0.3em;\r\n",
       "    display: block;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h3 {\r\n",
       "    font-family: 'Nixie One', serif;\r\n",
       "    margin-top:16px;\r\n",
       "    font-size: 22pt;\r\n",
       "    font-weight: 600;\r\n",
       "    margin-bottom: 3px;\r\n",
       "    font-style: regular;\r\n",
       "    color: rgb(102,102,0);\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h4 {    /*Use this for captions*/\r\n",
       "    font-family: 'Nixie One', serif;\r\n",
       "    font-size: 14pt;\r\n",
       "    text-align: center;\r\n",
       "    margin-top: 0em;\r\n",
       "    margin-bottom: 2em;\r\n",
       "    font-style: regular;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\r\n",
       "    font-family: 'Nixie One', sans-serif;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 16pt;\r\n",
       "    color: rgb(163,0,0);\r\n",
       "    font-style: italic;\r\n",
       "    margin-bottom: .1em;\r\n",
       "    margin-top: 0.8em;\r\n",
       "    display: block;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h6 { /*use this for copyright note*/\r\n",
       "    font-family: 'PT Mono', sans-serif;\r\n",
       "    font-weight: 300;\r\n",
       "    font-size: 9pt;\r\n",
       "    line-height: 100%;\r\n",
       "    color: grey;\r\n",
       "    margin-bottom: 1px;\r\n",
       "    margin-top: 1px;\r\n",
       "}\r\n",
       "\r\n",
       ".CodeMirror{\r\n",
       "    font-family: \"PT Mono\";\r\n",
       "    font-size: 90%;\r\n",
       "}\r\n",
       "\r\n",
       ".boxed { /* draw a border around a piece of text */\r\n",
       "  border: 1px solid blue ;\r\n",
       "}\r\n",
       "\r\n",
       "h4#CODE-EXAMPLE,\r\n",
       "h4#END-OF-CODE-EXAMPLE {\r\n",
       "    margin: 10px 0;\r\n",
       "    padding: 10px;\r\n",
       "    background-color: #d0f9ca !important;\r\n",
       "    border-top: #849f81 1px solid;\r\n",
       "    border-bottom: #849f81 1px solid;\r\n",
       "}\r\n",
       "\r\n",
       ".emphasis {\r\n",
       "    color: red;\r\n",
       "}\r\n",
       "\r\n",
       ".exercise {\r\n",
       "    color: green;\r\n",
       "}\r\n",
       "\r\n",
       ".proof {\r\n",
       "    color: blue;\r\n",
       "}\r\n",
       "\r\n",
       "code {\r\n",
       "  padding: 2px 4px !important;\r\n",
       "  font-size: 90% !important;\r\n",
       "  color: #222 !important;\r\n",
       "  background-color: #efefef !important;\r\n",
       "  border-radius: 2px !important;\r\n",
       "}\r\n",
       "\r\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\r\n",
       "   as they will be removed through some other method */\r\n",
       "@media not print {\r\n",
       "  .cell:nth-last-child(-n+2) {\r\n",
       "    display: none;\r\n",
       "  }\r\n",
       "}\r\n",
       "\r\n",
       "footer.hidden-print {\r\n",
       "    display: none !important;\r\n",
       "}\r\n",
       "    \r\n",
       "</style>\r\n",
       "\r\n",
       "<!-- MathJax styling -->\r\n",
       "<script>\r\n",
       "    MathJax.Hub.Config({\r\n",
       "                        CommonHTML: {\r\n",
       "                            scale: 200\r\n",
       "                        },\r\n",
       "                TeX: {\r\n",
       "                    extensions: [\"AMSmath.js\"],\r\n",
       "                    equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\r\n",
       "                },\r\n",
       "                tex2jax: {\r\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\r\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\r\n",
       "                },\r\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\r\n",
       "                \"HTML-CSS\": {\r\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\r\n",
       "                }\r\n",
       "        });\r\n",
       "</script>\r\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", read(f,String))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
