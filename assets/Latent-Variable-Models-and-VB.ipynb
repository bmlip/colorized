{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Variable Models and Variational Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to latent variable models and variational inference    \n",
    "- Materials\n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional \n",
    "    - Bishop pp. 55-57 for Jensen's inequality\n",
    "  - references\n",
    "    - Blei et al. (2017), [Variational Inference: A Review for Statisticians](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustrative Example\n",
    "\n",
    "- You're now asked to build a density model for a data set ([Old Faithful](https://en.wikipedia.org/wiki/Old_Faithful), Bishop pg. 681) that clearly is not distributed as a single Gaussian:\n",
    "\n",
    "<img src=\"./figures/fig-Bishop-A5-Old-Faithfull.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Unobserved Classes\n",
    "\n",
    "Consider again a set of observed data $D=\\{x_1,\\dotsc,x_N\\}$\n",
    "\n",
    "- This time we suspect that there are _unobserved_ class labels that would help explain (or predict) the data, e.g.,\n",
    "  - the observed data are the color of living things; the unobserved classes are animals and plants.\n",
    "  - observed are wheel sizes; unobserved categories are trucks and personal cars.\n",
    "  - observed is an audio signal; unobserved classes include speech, music, traffic noise, etc.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   \n",
    "- Classification problems with unobserved classes are called **Clustering** problems. The learning algorithm needs to **discover the classes from the observed data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Gaussian Mixture Model\n",
    "\n",
    "- Let $D=\\{x_n\\}$ be a set of observations. We associate a one-hot coded hidden class label $z_n$ with each observation:\n",
    "$$\n",
    "z_{nk} = \\begin{cases} 1 & \\text{if } x_n \\in \\mathcal{C}_k \\text{ (the k-th class)}\\\\\n",
    "                       0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "- We consider the same model as for the generative classification model:\n",
    "$$\\begin{align*}\n",
    "p(x_n | z_{nk}=1) &= \\mathcal{N}\\left( x_n | \\mu_k, \\Sigma_k\\right)\\\\\n",
    "p(z_{nk}=1) &= \\pi_k\n",
    "\\end{align*}$$\n",
    "which can be summarized with the one-hot coded selection variables $z_{nk}$ as\n",
    "$$\\begin{align*}\n",
    "p(x_n,z_n) &= p(x_n | z_n) p(z_n) = \\prod_{k=1}^K \\left( \\pi_k \\cdot \\mathcal{N}\\left( x_n | \\mu_k, \\Sigma_k\\right) \\right)^{z_{nk}} \n",
    "\\end{align*}$$\n",
    "- The model for an _observed_ data point $x_n$ is now given by (<font color=green>exercise B-9.3</font>)\n",
    "$$\\begin{align*}{}\n",
    "p(x_n) &= \\sum_{z_n} p(x_n,z_n)  \\\\\n",
    "  &= \\sum_{k=1}^K \\pi_k \\cdot \\mathcal{N}\\left( x_n | \\mu_k, \\Sigma_k \\right) \\tag{B-9.12}\n",
    "\\end{align*}$$\n",
    "- This model is known as a <font color=red>Gaussian Mixture Model</font> (GMM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  GMM is a very flexible model\n",
    "\n",
    "- GMMs are very popular models. They have decent computational properties and are **universal approximators of densities** (as long as there are enough Gaussians of course)\n",
    "\n",
    "<img src=\"./figures/fig-ZoubinG-GMM-universal-approximation.png\" width=\"600\">\n",
    "\n",
    "- (In the above figure, the Gaussian components are shown in <font color=red>red</font> and the pdf of the mixture models in <font color=blue>blue</font>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  The Kullback-Leibler Divergence\n",
    "\n",
    "- In order to prove that the EM algorithm works, we will need [Gibbs inequality](https://en.wikipedia.org/wiki/Gibbs%27_inequality), which is a famous theorem in information theory.\n",
    "\n",
    "- Definition: the **Kullback-Leibler divergence** (a.k.a. **relative entropy**) is a distance measure between two distributions $q$ and $p$ and is defined as\n",
    "\n",
    "$$\n",
    "D_{\\text{KL}}(q \\parallel p) \\triangleq \\sum_z q(z) \\log \\frac{q(z)}{p(z)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Free Energy Functional\n",
    "\n",
    "- Consider a generative model specified by $$p(x,z) = p(x|z)p(z)\\,,$$ where $x$ and $z$ represent the observed and unobserved variables, respectively.\n",
    "- Assume that $x$ has been observed and we are interested in performing Bayesian inference, i.e., we want to compute the posterior $p(z|x)$ for the latent variables and the evidence $p(x)$.\n",
    "- Consider the following loss functional (the **variational free energy** or shortly **free energy** (FE))\n",
    "$$\\begin{align}\n",
    "F[q] &\\triangleq \\sum_z q(z) \\log \\frac{q(z)}{p(x,z)} \\tag{B-10.3}\n",
    "\\end{align}$$\n",
    "  - (As an aside), note that Bishop introduces an _Evidence Lower BOund_ (in modern literature abbreviated as ELBO) $\\mathcal{L}(z)$ that equals the _negative_ FE. We prefer to discuss variational inference in terms of a free energy (but it is the same story as he discusses with the ELBO).  \n",
    "\n",
    "- <a id='fe-decompositions'></a>Note that the FE can be decomposed as (link to [exercise](#exercises))\n",
    "$$\\begin{align}\n",
    "F[q] &= \\underbrace{-\\sum_z q(z) \\log p(x,z)}_{\\text{energy}} - \\underbrace{\\sum_z q(z) \\log \\frac{1}{q(z)}}_{\\text{entropy}} \\tag{EE}\\\\\n",
    "  &= \\underbrace{\\sum_z q(z) \\log \\frac{q(z)}{p(z|x)}}_{\\text{divergence}} - \\underbrace{\\log p(x)}_{\\text{log-evidence}}  \\tag{DE}\\\\\n",
    "  &=  \\underbrace{\\sum_z q(z)\\log\\frac{q(z)}{p(z)}}_{\\text{complexity}} - \\underbrace{\\sum_z q(z) \\log p(x|z)}_{\\text{accuracy}}  \\tag{IC}\n",
    "\\end{align}$$\n",
    "  - These decompositions are very insightful and we will label them respectively as _energy-entropy_ (EE), _divergence-evidence_ (DE), and _complexity-accuracy_ (CA) decompositions. \n",
    "- The RHS of the DE decomposition does not depend on $q$. Hence, the global minimum $$q^* \\triangleq \\arg\\max_q F[q]$$ is obtained for $q^*(z) = p(z|x)$, which is the Bayesian posterior.\n",
    "- Furthermore, the minimal free energy $F^* \\triangleq F[q^*] = -\\log p(x)$ equals minus-log-evidence.  \n",
    "\n",
    "\n",
    "$\\Rightarrow$ <font color=red>It follows that Bayesian inference (computation of posterior and evidence) can be executed by FE minimization.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Solving FE Minimization through Mean-field Factorization\n",
    "\n",
    "- Note that the FE is a _functional_ , i.e., the FE is a function ($F$) of a function ($q$). We are looking for a _function_ $q^*(z)$ that minimizes the FE. \n",
    "- The mathematics of minimizing funstionals is described by _variational calculus_ , see Bishop app.D and Lanczos. (Optional reading).\n",
    "- Generally speaking, it is not possible to solve the variational FE minimization problem without adding some constraints on $q(z)$.\n",
    "- We will discuss three important cases of setting constraints on $q(z)$\n",
    "  1. mean-field factorization\n",
    "  2. parameterization\n",
    "  3. the EM algorithm\n",
    "  \n",
    "- One important constraint is the so-called _mean-field_ constraint, given by\n",
    "$$\n",
    "q(z) = \\prod_{i=1}^m q_i(z_i)\\,, \\tag{B-10.5}\n",
    "$$\n",
    "i.e., the posterior factorizes into $m$ _independent_ factors $q_i(z_i)$.\n",
    "\n",
    "- Given the mean-field constraint, it is possible to derive an expression for the optimal solutions $q_j^*(z_j)$, for $j=1,\\ldots,m$, which is given by (see Bishop, p.466, eq.10.9)\n",
    "$$\\begin{align} \n",
    "\\log q_j^*(z_j) &\\propto \\mathrm{E}_{q_{-j}^*}\\left[ \\log p(x,z) \\right] \\tag{B-10.9}\\\\\n",
    "  &=\\sum_{z_{-j}} q_{-j}^*(z_{-j}) \\log p(x,z) \\notag\n",
    "\\end{align}$$\n",
    "where we defined $q_{-j}^*(z_{-j}) \\triangleq q_1^*(z_1)q_2^*(z_2)\\cdots q_{j-1}^*(z_{j-1})q_{j+1}^*(z_{j+1})\\cdots q_m^*(z_m)$.\n",
    "  - <font color=green>Proof (from Blei (2017)): We first rewrite the energy-entropy decomposition of the FE as a function of $q_j(z_j)$ only: \n",
    "  $$ F[q_j] = \\mathbb{E}_{q_{j}}\\left[ \\mathbb{E}_{q_{-j}}\\left[ \\log p(x,z_j,z_{-j})\\right]\\right] - \\mathbb{E}_{q_j}\\left[ \\log q_j(z_j)\\right] + \\mathtt{const.}\\,,$$\n",
    "  where the constant holds all terms that do not depend on $q_j(z_j)$. This expression can be written as \n",
    "  $$ F[q_j] = \\sum_{z_j} q_j(z_j) \\log \\frac{q_j(z_j)}{\\exp\\left( \\mathbb{E}_{q_{-j}}\\left[ \\log p(x,z_j,z_{-j})\\right]\\right)}$$\n",
    "  which is a KL-divergence that is minimized by Eq. B-10.9. </font> \n",
    "  \n",
    "- This is not yet a full solution to the FE minimization task since the solution $q_j^*(z_j)$ depends on expectations that involve $q_{i\\neq j}^*(z_{i \\neq j})$, and each of the solutions $q_{i\\neq j}^*(z_{i \\neq j})$ depends on an expection that involves $q_j^*(z_j)$. \n",
    "- In practice, we solve this chicken-and-egg problem by an iterative approach: we first initialize all $q_j(z_j)$ (for $j=1,\\ldots,m$) to an appropriate initial distribution and then cycle through the factors in turn by solving eq.10.9 and update $q_{-j}^*(z_{-j})$ with the latest estimates. (See Blei (2017), Algorithm 1, p864).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: FE Minimization by Mean-field Factorization\n",
    "\n",
    "- Let's work out an FE minimization task for a simple Gaussian estimation problem. Bishop section 10.1.3\n",
    "- <font color=red> TBD <font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FEM for the Gaussian Mixture Model\n",
    "\n",
    "##### model specification\n",
    "\n",
    "\n",
    "- We consider a Gaussian Mixture Model, specified by \n",
    "$$\\begin{align*}\n",
    "p(x,z|\\theta) &= p(x|z,\\mu,\\Lambda)p(z|\\pi) \\\\\n",
    "  &= \\prod_{n=1}^N \\prod_{k=1}^K \\pi_k^{z_{nk}}\\cdot \\mathcal{N}\\left( x_n | \\mu_k, \\Lambda_k^{-1}\\right)^{z_{nk}} \\tag{B-10.37,38}\n",
    "\\end{align*}$$\n",
    "with tuning parameters $\\theta=\\{\\pi_k, \\mu_k,\\Lambda_k\\}$. \n",
    "- Let us introduce some priors for the parameters. We factorize the prior as\n",
    "$$\n",
    "p(\\theta) = p(\\pi,\\mu,\\Lambda) = p(\\pi) p(\\mu|\\Lambda) p(\\Lambda)\n",
    "$$\n",
    "with \n",
    "$$\\begin{align*}\n",
    "p(\\pi) &= \\mathrm{Dir}(\\pi|\\alpha_0) = C(\\alpha_0) \\prod_k \\pi_k^{\\alpha_0-1} \\tag{B-10.39}\\\\\n",
    "p(\\mu|\\Lambda) &= \\prod_k \\mathcal{N}\\left(\\mu_k | m_0, \\left( \\beta_0 \\Lambda_k\\right)^{-1} \\right) \\tag{B-10.40}\\\\\n",
    "p(\\Lambda) &= \\prod_k \\mathcal{W}\\left( \\Lambda_k | W_0, \\nu_0 \\right) \\tag{B-10.40}\n",
    "\\end{align*}$$\n",
    "\n",
    "- The full generative model is now specified by\n",
    "$$\n",
    "p(x,z,\\pi,\\mu,\\Lambda) = p(x|z,\\mu,\\Lambda) p(z|\\pi) p(\\pi) p(\\mu|\\Lambda) p(\\Lambda) \\tag{B-10.41}\n",
    "$$\n",
    "with hyperparameters $\\{ \\alpha_0, m_0, \\beta_0, W_0, \\nu_0\\}$.\n",
    "\n",
    "##### inference\n",
    "\n",
    "- Assume that we have observed $D = \\left\\{x_1, x_2, \\ldots, x_N\\right\\}$ and are interested to infer the posterior distribution for the tuning parameters: \n",
    "$$\n",
    "p(\\theta|D) = p(\\pi,\\mu,\\Lambda|D)\n",
    "$$\n",
    "\n",
    "- The (perfect) Bayesian solution is \n",
    "$$\n",
    "p(\\theta|D) = \\frac{p(x=D,\\theta)}{p(D)}\\frac{\\sum_z p(x=D,z,\\pi,\\mu,\\Lambda)}{\\sum_z \\sum_{\\pi} \\iint p(x=D,z,\\pi,\\mu,\\Lambda) \\,\\mathrm{d}\\mu\\mathrm{d}\\Lambda}\n",
    "$$\n",
    "but this is intractable (See Blei (2017), p861, eqs. 8 and 9).\n",
    "\n",
    "- Bishop shows that the the mean-field optimal equations (Eq. B-10.9) are analytically solvable for the GMM as specified above, with the following solution: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code Example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### FE Minimization by the Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "- The EM algorithm is a special case of FE minimization that focusses on Maximum-Likelihood estimation for models with latent variables. \n",
    "- Consider a model $$p(x,z,\\theta)$$ with observations $x = \\{x_n\\}$, latent variables $z=\\{z_n\\}$ and parameters $\\theta$.\n",
    "\n",
    "- We can write the following FE functional for this model:\n",
    "$$\\begin{align}\n",
    "F[q] =  \\sum_z \\sum_\\theta q(z) q(\\theta) \\log \\frac{q(z) q(\\theta)}{p(x,z,\\theta)} \n",
    "\\end{align}$$\n",
    "\n",
    "- The EM algorithm makes the following simplifying assumptions:\n",
    "  1. The prior for the parameters is uninformative (uniform). This implies that \n",
    "  $$p(x,z,\\theta) = p(x,z|\\theta) p(\\theta) \\propto p(x,z|\\theta)$$\n",
    "  2. The posterior for the parameters is a delta function:\n",
    "  $$q(\\theta) = \\delta(\\theta - \\hat{\\theta})$$\n",
    "  \n",
    "- Basically, these two assumptions turn FE minimization into maximum likelihood estimation for the parameters $\\theta$ and the FE simplifies to \n",
    "$$\\begin{align}\n",
    "F[q,\\theta] =  \\sum_z q(z) \\log \\frac{q(z)}{p(x,z|\\theta)} \n",
    "\\end{align}$$\n",
    "\n",
    "- The EM algorithm minimizes this FE by iterating (iteration counter: $i$) over \n",
    "\n",
    "<font color=red>$$\\begin{align*}\n",
    "q(z) &= p(z|x,\\theta^{(i-1)}) \\tag{the E-step}\\\\\n",
    "\\theta^{(i)} &= \\arg\\max_\\theta \\sum_z q(z) \\log p(x,z|\\theta^{(i-1)}) \\tag{the M-step}\n",
    "\\end{align*}$$</font>\n",
    "\n",
    "- These choices are optimal for the given FE functional. In order to see this, consider the two decompositions\n",
    "$$\\begin{align}\n",
    "F[q,\\theta] &= \\underbrace{-\\sum_z q(z) \\log p(x,z|\\theta)}_{\\text{energy}} - \\underbrace{\\sum_z q(z) \\log \\frac{1}{q(z)}}_{\\text{entropy}} \\tag{EE}\\\\\n",
    "  &= \\underbrace{\\sum_z q(z) \\log \\frac{q(z)}{p(z|x,\\theta)}}_{\\text{divergence}} - \\underbrace{\\log p(x|\\theta)}_{\\text{log-likelihood}}  \\tag{DE}\n",
    "\\end{align}$$\n",
    "\n",
    "- The DE decomposition shows that the FE is minimized for the choice $q(z) := p(z|x,\\theta)$. Also, for this choice, the FE equals the (negative) log-evidence. \n",
    "\n",
    "- The EE decomposition shows that the FE is minimized wrt $\\theta$ by minimizing the energy term (which is indeed the choice in the M-step of the EM algorithm).\n",
    "  - (In the EM literature, this term is called the _expected complete-data log-likelihood_.)\n",
    "\n",
    "- In order to execute the EM algorithm, it is assumed that we can analytically execute the E- and M-steps. For a large set of models (including models whose distributions belong to the exponential family of distributions), this is indeed the case and hence the large popularity of the EM algorithm. \n",
    "\n",
    "- The EM algorihm imposes rather severe assumptions on the FE. Over the past few years, the rise of Probabilistic Programming languages has dramatically increased the range of models for which the parameters can by estimated autmatically by (approximate) Bayesian inference, so the popularity of EM is slowly waning. (More on this in the Probabilistic Programming lessons). \n",
    "\n",
    "- Bishop (2006) works out EM for the GMM in section 9.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  Theorem: **Gibbs Inequality** ([proof](https://en.wikipedia.org/wiki/Gibbs%27_inequality#Proof) uses Jensen inquality):    \n",
    "$$\\boxed{ D_{\\text{KL}}(q \\parallel p) \\geq 0 }$$\n",
    "with equality only iff $p=q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the KL divergence is an asymmetric distance measure, i.e. in general \n",
    "\n",
    "$$D_{\\text{KL}}(q \\parallel p) \\neq D_{\\text{KL}}(p \\parallel q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  EM by maximizing a Lower Bound on the Log-Likelihood\n",
    "\n",
    "- Consider a model for observations $x$, hidden variables $z$ and tuning parameters $\\theta$. Note that, for **any** distribution $q(z)$, we can expand the log-likelihood as follows: \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathrm{L}(\\theta) &\\triangleq \\log p(x|\\theta)  \\\\\n",
    "  &= \\sum_z q(z) \\log p(x|\\theta) \\\\\n",
    "  &= \\sum_z q(z) \\left( \\log p(x|\\theta) - \\log \\frac{q(z)}{p(z|x,\\theta)}\\right) +  \\sum_z q(z) \\log \\frac{q(z)}{p(z|x,\\theta)} \\\\\n",
    "  &= \\sum_z q(z) \\log \\frac{p(x,z|\\theta)}{q(z)} + \\underbrace{D_{\\text{KL}}\\left( q(z) \\parallel p(z|x,\\theta) \n",
    "\\right)}_{\\text{Kullback-Leibler div.}} \\tag{1} \\\\\n",
    "  &\\geq \\sum_z q(z) \\log \\frac{p(x,z|\\theta)}{q(z)} \\quad \\text{(use Gibbs inequality)}  \\\\\n",
    "  &= \\underbrace{\\sum_z q(z) \\log p(x,z|\\theta)}_{\\text{expected complete-data log-likelihood}} + \\underbrace{\\mathcal{H}\\left[ q\\right]}_{\\text{entropy of }q}  \\\\\n",
    "&\\triangleq \\mathrm{LB}(q,\\theta) \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Hence, $\\mathrm{LB}(q,\\theta)$ is a **lower bound** on the log-likelihood $\\log p(x|\\theta)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Technically, the Expectation-Maximization (EM) algorithm is defined by coordinate ascent on $\\mathrm{LB}(q,\\theta)$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "  &\\textrm{ } \\\\\n",
    "  &\\textrm{Initialize }: \\theta^{(0)}\\\\\n",
    "  &\\textrm{for }m = 1,2,\\ldots \\textrm{until convergence}\\\\\n",
    "    &\\quad q^{(m+1)} = \\arg\\max_q \\mathrm{LB}(q,\\theta^{(m)}) &\\quad \\text{% update responsibilities} \\\\\n",
    "    &\\quad \\theta^{(m+1)} = \\arg\\max_\\theta \\mathrm{LB}(q^{(m+1)},\\theta) &\\quad \\text{% re-estimate parameters}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Since $\\mathrm{LB}(q,\\theta) \\leq \\mathrm{L}(\\theta)$ (for all choices of $q(z)$), maximizing the lower-bound $\\mathrm{LB}$ will also maximize the log-likelihood wrt $\\theta$. The _reason_ to maximize $\\mathrm{LB}$ rather than log-likelihood $\\mathrm{L}$ directly is that $\\arg\\max \\mathrm{LB}$ often leads to easier expressions. E.g., see this illustrative figure (Bishop p.453):\n",
    "\n",
    "<img src=\"./figures/Bishop-Figure914.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### EM Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Maximizing $\\mathrm{LB}$ w.r.t. $q$ \n",
    "\n",
    "- Note that\n",
    "$$\n",
    "\\mathrm{LB}(q,\\theta) =  \\mathrm{L}(\\theta)  - D_{\\text{KL}}\\left( q(z) \\parallel p(z|x,\\theta) \\right)\n",
    "$$\n",
    "and consequenty, maximizing $\\mathrm{LB}$ over $q$ leads to minimization of the KL-divergence. Specifically, it follows from Gibbs inequality that\n",
    "$$\n",
    " q^{(m+1)}(z) := p(z|x,\\theta^{(m)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Maximizing $\\mathrm{LB}$ w.r.t. $\\theta$\n",
    "\n",
    "- It also follows from (the last line of) the multi-line derivation above that maximizing $\\mathrm{LB}$ w.r.t. $\\theta$ amounts to maximization of the _expected complete-data log-likelihood_ (where the complete data set is defined as $\\{(x_i,z_i)\\}_{i=1}^N$). Hence, the EM algorithm comprises iterations of\n",
    "$$\n",
    "\\boxed{\\textbf{EM}:\\, \\theta^{(m+1)} := \\underbrace{\\arg\\max_\\theta}_{\\text{M-step}} \\underbrace{\\sum_z  \\overbrace{p(z|x,\\theta^{(m)})}^{=q^{(m+1)}(z)} \\log p(x,z|\\theta)}_{\\text{E-step}} }\n",
    "$$\n",
    "\n",
    "<!---\n",
    "- Compare this to regular log-likelihood maximization:\n",
    "$$\n",
    "\\boxed{\\textbf{ML}:\\, \\hat \\theta:= \\arg\\max_\\theta \\log p(x|\\theta)}\n",
    "$$\n",
    "--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### <span style=\"color:green\">Homework exercise: EM for GMM Revisited</span>\n",
    "\n",
    "##### E-step\n",
    "- Write down the GMM generative model\n",
    "- The complete-data set is $D_c=\\{x_1,z_1,x_2,z_2,\\ldots,x_n,z_n\\}$. Write down the _complete-data_ likelihood $p(D_c|\\theta)$\n",
    "- Write down the complete-data _log_-likelihood $\\log p(D_c|\\theta)$\n",
    "- Write down the _expected_ complete-data log-likelihood $\\mathrm{E}_Z\\left[ \\log p(D_c|\\theta) \\right]$\n",
    "\n",
    "\n",
    "##### M-step\n",
    "- Maximize $\\mathrm{E}_Z\\left[ \\log p(D_c|\\theta) \\right]$ w.r.t. $\\theta=\\{\\pi,\\mu,\\Sigma\\}$\n",
    "\n",
    "- Verify that your solution is the same as the 'intuitive' solution of the previous lesson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  <span style=\"color:green\">Homework Exercise: EM for Three Coins problem</span>\n",
    "\n",
    "- You have three coins in your pocket. For each coin, outcomes $\\in \\{\\mathrm{H},\\mathrm{T}\\}$.\n",
    "$$\n",
    "p(\\mathrm{H}) = \\begin{cases} \\lambda & \\text{for coin }0 \\\\\n",
    " \\rho & \\text{for coin }1 \\\\\n",
    " \\theta & \\text{for coin }2 \\end{cases}\n",
    "$$\n",
    "\n",
    "    \n",
    "-  **Scenario**. Toss coin $0$. If Head comes up, toss three times with coin $1$; otherwise, toss three times with coin $2$.\n",
    "\n",
    "- The observed sequences **after** each toss with coin $0$ were $\\langle \\mathrm{HHH}\\rangle$, $\\langle \\mathrm{HTH}\\rangle$, $\\langle \\mathrm{HHT}\\rangle$, and $\\langle\\mathrm{HTT}\\rangle$\n",
    "\n",
    "- **Task**. Use EM to estimate most the likely values for $\\lambda$, $\\rho$ and $\\theta$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Report Card on EM\n",
    "\n",
    "-  EM is a general procedure for learning in the presence of unobserved variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  In a sense, it is a **family of algorithms**. The update rules you will derive depend on the probability model assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  (Good!) **No tuning parameters** such a learning rate, unlike gradient descent-type algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  (Bad). EM is an iterative procedure that is very sensitive to initial\n",
    "conditions! EM converges to a **local optimum**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  Start from trash $\\rightarrow$ end up with trash. Hence, we need a good and fast initialization procedure (often used: K-Means, see Bishop p.424)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  Also used to train HMMs, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:red\">(OPTIONAL SLIDE)</span> The Free-Energy Principle\n",
    "\n",
    "- The negative lower-bound $\\mathrm{F}(q,\\theta) \\triangleq -\\mathrm{LB}(q,\\theta)$ also appears in various scientific disciplines. In statistical physics and variational calculus, $F$ is known as the **free energy** functional. Hence, the EM algorithm is a special case of **free energy minimization**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It follows from Eq.1 above that\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathrm{F}(q,\\theta) &\\triangleq \\sum_z q(z) \\log \\frac{q(z)}{p(x,z|\\theta)} \\\\\n",
    "  &= \\underbrace{- \\mathrm{L}(\\theta)}_{-\\text{log-likelihood}} + \\underbrace{D_{\\text{KL}}\\left( q(z) \\parallel p(z|x,\\theta) \n",
    "\\right)}_{\\text{divergence}}\n",
    "\\end{align*}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The **Free-Energy Principle** (FEP) is an influential [neuroscientific theory](https://en.wikipedia.org/wiki/Free_energy_principle) that claims that information processing in the brain is also an example of free-energy minimization, see [Friston, 2009](./files/Friston-2009-The-free-energy-principle-a-rough-guide-to-the-brain.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- According to FEP, the brain contains a generative model $p(x,z,a,\\theta)$ for its environment. Here, $x$ are the sensory signals (observations); $z$ corresponds to (hidden) environmental causes of the sensorium; $a$ represents our actions (control signals for movement) and $\\theta$ describes the fixed 'physical rules' of the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:red\">(OPTIONAL SLIDE)</span> The Free-Energy Principle, cont'd\n",
    "\n",
    "- Solely through free-energy minimization, the brain infers an approximate posterior $q(z,a,\\theta)$, thus inferring our **perception**, **actions**, and **learning** equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Free-energy can be interpreted as (a generalized notion of sum of) prediction errors. Free-energy minimization aims to minimize prediction errors through perception, actions, and learning. (The next picture  is from a [2012 tutorial presentation](http://slideplayer.com/slide/9925565/) by Karl Friston) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"./figures/friston-fep.jpg\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <span style=\"color:red\">(OPTIONAL SLIDE)</span> The Free-Energy Principle, cont'd\n",
    "\n",
    "- $\\Rightarrow$ The brain is \"nothing but\" an approximate Bayesian agent that tries to build a model for its world. Actions (behavior) are selected to fulfill prior expectations about the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In our [BIASlab research team](http://biaslab.org) in the Signal Processing Systems group (FLUX floor 7), we work on developing (approximately Bayesian) **artificial** intelligent agents that learn purposeful behavior through interactions with their environments, inspired by the free-energy principle. Applications include\n",
    "  - robotics\n",
    "  - self-learning of new signal processing algorithms, e.g., for hearing aids\n",
    "  - self-learning how to drive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let me know (email bert.de.vries@tue.nl) if you're interested in developing machine learning applications that are directly inspired by how the brain computes. We often have open traineeships or MSc thesis projects available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <a id='exercises'></a>Exercises\n",
    "\n",
    "1. Given the free energy , proof the [EE, DE and AC decompositions](#fe-decompositions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\r\n",
       "This HTML file contains custom styles and some javascript.\r\n",
       "Include it a Jupyter notebook for improved rendering.\r\n",
       "-->\r\n",
       "\r\n",
       "<!-- Fonts -->\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\r\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\r\n",
       "\r\n",
       "<!-- Custom style -->\r\n",
       "<style>\r\n",
       "\r\n",
       "@font-face {\r\n",
       "    font-family: \"Computer Modern\";\r\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\r\n",
       "}\r\n",
       "\r\n",
       "#notebook_panel { /* main background */\r\n",
       "    background: rgb(245,245,245);\r\n",
       "}\r\n",
       "\r\n",
       "div.container {\r\n",
       "    min-width: 960px;\r\n",
       "}\r\n",
       "\r\n",
       "div #notebook { /* centre the content */\r\n",
       "    background: #fff; /* white background for content */\r\n",
       "    margin: auto;\r\n",
       "    padding-left: 0em;\r\n",
       "}\r\n",
       "\r\n",
       "#notebook li { /* More space between bullet points */\r\n",
       "    margin-top:0.8em;\r\n",
       "}\r\n",
       "\r\n",
       "/* draw border around running cells */\r\n",
       "div.cell.border-box-sizing.code_cell.running {\r\n",
       "    border: 1px solid #111;\r\n",
       "}\r\n",
       "\r\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\r\n",
       "div.cell.code_cell {\r\n",
       "    background-color: rgb(256,256,256);\r\n",
       "    border-radius: 0px;\r\n",
       "    padding: 0.5em;\r\n",
       "    margin-left:1em;\r\n",
       "    margin-top: 1em;\r\n",
       "}\r\n",
       "\r\n",
       "div.text_cell_render{\r\n",
       "    font-family: 'Alegreya Sans' sans-serif;\r\n",
       "    line-height: 140%;\r\n",
       "    font-size: 125%;\r\n",
       "    font-weight: 400;\r\n",
       "    width:800px;\r\n",
       "    margin-left:auto;\r\n",
       "    margin-right:auto;\r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "/* Formatting for header cells */\r\n",
       ".text_cell_render h1 {\r\n",
       "    font-family: 'Nixie One', serif;\r\n",
       "    font-style:regular;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 45pt;\r\n",
       "    line-height: 100%;\r\n",
       "    color: rgb(0,51,102);\r\n",
       "    margin-bottom: 0.5em;\r\n",
       "    margin-top: 0.5em;\r\n",
       "    display: block;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h2 {\r\n",
       "    font-family: 'Nixie One', serif;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 30pt;\r\n",
       "    line-height: 100%;\r\n",
       "    color: rgb(0,51,102);\r\n",
       "    margin-bottom: 0.1em;\r\n",
       "    margin-top: 0.3em;\r\n",
       "    display: block;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h3 {\r\n",
       "    font-family: 'Nixie One', serif;\r\n",
       "    margin-top:16px;\r\n",
       "    font-size: 22pt;\r\n",
       "    font-weight: 600;\r\n",
       "    margin-bottom: 3px;\r\n",
       "    font-style: regular;\r\n",
       "    color: rgb(102,102,0);\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h4 {    /*Use this for captions*/\r\n",
       "    font-family: 'Nixie One', serif;\r\n",
       "    font-size: 14pt;\r\n",
       "    text-align: center;\r\n",
       "    margin-top: 0em;\r\n",
       "    margin-bottom: 2em;\r\n",
       "    font-style: regular;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\r\n",
       "    font-family: 'Nixie One', sans-serif;\r\n",
       "    font-weight: 400;\r\n",
       "    font-size: 16pt;\r\n",
       "    color: rgb(163,0,0);\r\n",
       "    font-style: italic;\r\n",
       "    margin-bottom: .1em;\r\n",
       "    margin-top: 0.8em;\r\n",
       "    display: block;\r\n",
       "}\r\n",
       "\r\n",
       ".text_cell_render h6 { /*use this for copyright note*/\r\n",
       "    font-family: 'PT Mono', sans-serif;\r\n",
       "    font-weight: 300;\r\n",
       "    font-size: 9pt;\r\n",
       "    line-height: 100%;\r\n",
       "    color: grey;\r\n",
       "    margin-bottom: 1px;\r\n",
       "    margin-top: 1px;\r\n",
       "}\r\n",
       "\r\n",
       ".CodeMirror{\r\n",
       "    font-family: \"PT Mono\";\r\n",
       "    font-size: 90%;\r\n",
       "}\r\n",
       "\r\n",
       ".boxed { /* draw a border around a piece of text */\r\n",
       "  border: 1px solid blue ;\r\n",
       "}\r\n",
       "\r\n",
       "h4#CODE-EXAMPLE,\r\n",
       "h4#END-OF-CODE-EXAMPLE {\r\n",
       "    margin: 10px 0;\r\n",
       "    padding: 10px;\r\n",
       "    background-color: #d0f9ca !important;\r\n",
       "    border-top: #849f81 1px solid;\r\n",
       "    border-bottom: #849f81 1px solid;\r\n",
       "}\r\n",
       "\r\n",
       ".emphasis {\r\n",
       "    color: red;\r\n",
       "}\r\n",
       "\r\n",
       ".exercise {\r\n",
       "    color: green;\r\n",
       "}\r\n",
       "\r\n",
       ".proof {\r\n",
       "    color: blue;\r\n",
       "}\r\n",
       "\r\n",
       "code {\r\n",
       "  padding: 2px 4px !important;\r\n",
       "  font-size: 90% !important;\r\n",
       "  color: #222 !important;\r\n",
       "  background-color: #efefef !important;\r\n",
       "  border-radius: 2px !important;\r\n",
       "}\r\n",
       "\r\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\r\n",
       "   as they will be removed through some other method */\r\n",
       "@media not print {\r\n",
       "  .cell:nth-last-child(-n+2) {\r\n",
       "    display: none;\r\n",
       "  }\r\n",
       "}\r\n",
       "\r\n",
       "footer.hidden-print {\r\n",
       "    display: none !important;\r\n",
       "}\r\n",
       "    \r\n",
       "</style>\r\n",
       "\r\n",
       "<!-- MathJax styling -->\r\n",
       "<script>\r\n",
       "    MathJax.Hub.Config({\r\n",
       "                        TeX: {\r\n",
       "                           extensions: [\"AMSmath.js\"],\r\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\r\n",
       "                           },\r\n",
       "                tex2jax: {\r\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\r\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\r\n",
       "                },\r\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\r\n",
       "                \"HTML-CSS\": {\r\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\r\n",
       "                }\r\n",
       "        });\r\n",
       "</script>\r\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", read(f, String))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
