{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Latent Variable Models and Variational Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to latent variable models and variational inference by Free energy minimization   \n",
    "- Materials\n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "    - Ariel Caticha (2010), [Entropic Inference](https://arxiv.org/abs/1011.0723)\n",
    "      - tutorial on entropic inference, which is a generalization to Bayes rule and provides a foundation for variational inference.\n",
    "  - Optional \n",
    "    - Bishop (2016), pp. 461-486 (sections 10.1, 10.2 and 10.3) \n",
    "  - references <a id='references'></a>\n",
    "    - Blei et al. (2017), [Variational Inference: A Review for Statisticians](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773) \n",
    "    - Lanczos (1961), [The variational principles of mechanics](https://www.amazon.com/Variational-Principles-Mechanics-Dover-Physics/dp/0486650677)\n",
    "    - Senoz et al. (2021), [Variational Message Passing and Local Constraint Manipulation in Factor Graphs](https://www.mdpi.com/1099-4300/23/7/807)\n",
    "    - Dauwels (2007), [On variational message passing on factor graphs](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Dauwels-2007-on-variational-message-passing-on-factor-graphs)\n",
    "    - Caticha (2010), [Entropic Inference](https://arxiv.org/abs/1011.0723)\n",
    "    - Shore and Johnson (1980), [Axiomatic Derivation of the Principle of Maximum Entropy and the Principle of Minimum Cross-Entropy](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/ShoreJohnson-1980-Axiomatic-Derivation-of-the-Principle-of-Maximum-Entropy.pdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustrative Example <a id=\"illustrative-example\"></a>: Density Modeling for the Old Faithful Data Set\n",
    "\n",
    "- You're now asked to build a density model for a data set ([Old Faithful](https://en.wikipedia.org/wiki/Old_Faithful), Bishop pg. 681) that clearly is not distributed as a single Gaussian:\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/fig-Bishop-A5-Old-Faithfull.png\" width=\"500\"></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Unobserved Classes\n",
    "\n",
    "Consider again a set of observed data $D=\\{x_1,\\dotsc,x_N\\}$\n",
    "\n",
    "- This time we suspect that there are _unobserved_ class labels that would help explain (or predict) the data, e.g.,\n",
    "  - the observed data are the color of living things; the unobserved classes are animals and plants.\n",
    "  - observed are wheel sizes; unobserved categories are trucks and personal cars.\n",
    "  - observed is an audio signal; unobserved classes include speech, music, traffic noise, etc.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   \n",
    "- Classification problems with unobserved classes are called **Clustering** problems. The learning algorithm needs to **discover the classes from the observed data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Gaussian Mixture Model\n",
    "\n",
    "- The spread of the data in the illustrative example looks like it could be modeled by two Gaussians. Let's develop a model for this data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let $D=\\{x_n\\}$ be a set of observations. We associate a one-hot coded hidden class label $z_n$ with each observation:\n",
    "\n",
    "$$\\begin{equation*}\n",
    "z_{nk} = \\begin{cases} 1 & \\text{if } x_n \\in \\mathcal{C}_k \\text{ (the k-th class)}\\\\\n",
    "                       0 & \\text{otherwise} \\end{cases}\n",
    "\\end{equation*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We consider the same model as we did in the [generative classification lesson](https://nbviewer.jupyter.org/github/bertdv/BMLIP/blob/master/lessons/notebooks/Generative-Classification.ipynb#GDA):\n",
    "$$\\begin{align*}\n",
    "p(x_n | z_{nk}=1) &= \\mathcal{N}\\left( x_n | \\mu_k, \\Sigma_k\\right)\\\\\n",
    "p(z_{nk}=1) &= \\pi_k\n",
    "\\end{align*}$$\n",
    "which can be summarized with the selection variables $z_{nk}$ as\n",
    "$$\\begin{align*}\n",
    "p(x_n,z_n) &= p(x_n | z_n) p(z_n) = \\prod_{k=1}^K (\\underbrace{\\pi_k \\cdot \\mathcal{N}\\left( x_n | \\mu_k, \\Sigma_k\\right) }_{p(x_n,z_{nk}=1)})^{z_{nk}} \n",
    "\\end{align*}$$\n",
    "\n",
    "- *Again*, this is the same model  as we defined for the generative classification model: A Gaussian-Categorical model but now with unobserved classes). \n",
    "\n",
    "- This model (with **unobserved class labels**) is known as a **Gaussian Mixture Model** (GMM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Marginal Distribution for the GMM\n",
    "\n",
    "- In the literature, the GMM is often introduced by the marginal distribution for an _observed_ data point $x_n$, given by\n",
    "\n",
    "$$\\begin{align*}{}\n",
    "p(x_n) &= \\sum_{z_n} p(x_n,z_n)  \\\\\n",
    "  &= \\sum_{k=1}^K \\pi_k \\cdot \\mathcal{N}\\left( x_n | \\mu_k, \\Sigma_k \\right) \\tag{B-9.12}\n",
    "\\end{align*}$$\n",
    "\n",
    "- Full proof as an [exercise](https://nbviewer.org/github/bertdv/BMLIP/blob/master/lessons/exercises/Exercises-Latent-Variable-Models-and-VB.ipynb). \n",
    "\n",
    "- Note that Eq. B-9.12 is not the generative model. The generative model is the joint distribution $p(x,z)$ over all variables, including the latent variables. \n",
    "\n",
    "- Eq. B-9.12 reveals the link to the name Gaussian *mixture model*. The priors $\\pi_k$ are also called **mixture coefficients**. \n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  GMM is a very flexible model\n",
    "\n",
    "- GMMs are very popular models. They have decent computational properties and are **universal approximators of densities** (as long as there are enough Gaussians of course)\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/fig-ZoubinG-GMM-universal-approximation.png\" width=\"600\"></p>\n",
    "\n",
    "- (In the above figure, the Gaussian components are shown in <font color=red>red</font> and the pdf of the mixture models in <font color=blue>blue</font>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Latent Variable Models\n",
    "\n",
    "- The GMM contains both _observed_ variables $\\{x_n\\}$, (unobserved) _parameters_ $\\theta= \\{\\pi_k,\\mu_k, \\Sigma_k\\}$ _and_ unobserved (synonym: latent, hidden) variables $\\{z_{nk}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- From a Bayesian viewpoint, both latent variables $\\{z_{nk}\\}$ and parameters $\\theta$ are just unobserved variables for which we can set a prior and compute a posterior by Bayes rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that $z_{nk}$ has a subscript $n$, hence its value depends not only on the cluster ($k$) but also on the $n$-th observation (in contrast to parameters). These observation-dependent latent variables are generally a useful tool to encode structure in the model. Here (in the GMM), the latent variables $\\{z_{nk}\\}$ encode (unobserved) class membership. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By adding model structure through (equations among) latent variables, we can build very complex models. Unfortunately, inference in latent variable models can also be much more complex than for fully observed models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference for GMM is Difficult\n",
    "\n",
    "\n",
    "- We recall here the log-likelihood for the Gaussian-Categorial Model, see [generative classification lesson)](https://nbviewer.jupyter.org/github/bertdv/BMLIP/blob/master/lessons/notebooks/Generative-Classification.ipynb)\n",
    "\n",
    "$$\n",
    "\\log\\, p(D|\\theta) =  \\sum_{n,k} y_{nk} \\underbrace{ \\log\\mathcal{N}(x_n|\\mu_k,\\Sigma) }_{ \\text{Gaussian} } + \\underbrace{ \\sum_{n,k} y_{nk} \\log \\pi_k }_{ \\text{multinomial} } \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since the class labels $y_{nk} \\in \\{0,1\\}$ were given, this expression decomposed into a set of simple updates for the Gaussian and multinomial distributions. For both distributions, we have conjugate priors, so inference is easily solved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- However, for the Gaussian mixture model (same log-likelihood function with $z_{nk}$ replacing $y_{nk}$), the class labels $\\{z_{nk}\\}$ are _unobserved_ and need to estimated alongside with the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There is no conjugate prior on the latent variables for the GMM likelihood function $p(\\underbrace{D}_{\\{x_n\\}}\\,|\\,\\underbrace{\\{z_{nk}\\},\\{\\mu_k,\\Sigma_k,\\pi_k\\}}_{\\text{all latent variables}})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In this lesson, we introduce an approximate Bayesian inference method known as **Variational Bayes** (VB) (also known as **Variational Inference**) that can be used for Bayesian inference in models with latent variables. Later in this lesson, we will use VB to do inference in the GMM.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As a motivation for variational inference, we first discuss the foundation of inference by the **Method of Maximum Relative Entropy**, [(Caticha, 2010)](https://arxiv.org/abs/1011.0723). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference When Information is in the Form of Constraints \n",
    "\n",
    "- In the [probability theory lesson](https://nbviewer.jupyter.org/github/bertdv/BMLIP/blob/master/lessons/notebooks/Probability-Theory-Review.ipynb#Bayes-rule), we recognized Bayes rule as the fundamental rule for learning from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We will now generalize this notion and consider learning as a process that updates a prior into a posterior distribution whenever new information becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In this context, *new information* is not necessarily a new observation, but could (for instance) also relate to adding *constraints* on the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For example, consider a model $\\prod_n p(x_n,z) = p(z) \\prod_n p(x_n|z)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our prior beliefs about $z$ are represented by $p(z)$. In the following, we will write $q(z)$ to denote a posterior distribution for $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  We might be interested in obtaining rational posterior beliefs $q(z)$ in consideration of the following additional constraints:\n",
    "  1. *Data Constraints*: e.g., two observed data points $x_1=5$ and $x_2=3$, which lead to constraints $q(x_1) = \\delta(x_1-5)$ and $q(x_2)=\\delta(x_2-3)$.\n",
    "  2. *Form Constraints*: e.g., we only consider the Gamma distribution for $q(z) = \\mathrm{Gam}(z|\\alpha,\\beta)$.\n",
    "  3. *Factorization Constraints*:, e.g., we consider independent marginals for the posterior distribution: $q(z) = \\prod_k q(z_k)$. \n",
    "  3. *Moment Constraints*: e.g., the first moment of the posterior is given by $\\int z q(z) \\mathrm{d}z = 3$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Note that this is not \"just\" applying Bayes rule to compute a posterior, which can only deal with data constraints as specified by the likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note also that observations _can_ be rephrased as constraints on the posterior, e.g., observation $x_1=5$ can be phrased as a constraint $q(x_1)=\\delta(x_1-5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\Rightarrow$ Updating a prior to a posterior on the basis of constraints on the posterior is more general than updating based on observations alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Method of Maximum Relative Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Caticha (2010)](https://arxiv.org/abs/1011.0723) (based on earlier work by [Shore and Johnson (1980)](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/ShoreJohnson-1980-Axiomatic-Derivation-of-the-Principle-of-Maximum-Entropy.pdf)) developed the **method of maximum (relative) entropy** for rational updating of priors to posteriors when faced with new information in the form of constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Consider prior beliefs $p(z)$ about $z$. New information in the form of constraints is obtained and we are interested in the \"best update\" to a posterior $q(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In order to define what \"best update\" means, we assume a ranking function $S[q,p]$ that generates a preference score for each candidate posterior $q$ for a given $p$. The best update from $p$ to $q$ is identified as $q^* = \\arg\\max_q S[q,p]$.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Similarly to [Cox' method to deriving probability theory](https://en.wikipedia.org/wiki/Cox%27s_theorem), Caticha introduced the following mild criteria based on a rational principle (the **principle of minimal updating**, see [Caticha 2010](https://arxiv.org/abs/1011.0723)) that the ranking function needs to adhere to: \n",
    "  1. *Locality*: local information has local effects.\n",
    "  2. *Coordinate invariance*: the system of coordinates carries no information. \n",
    "  3. *Independence*: When systems are known to be independent, it should not matter whether they are treated separately or jointly. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- These three criteria **uniquely identify the Relative Entropy** (RE) as the proper ranking function: \n",
    "$$\\begin{align*}\n",
    "S[q,p] = - \\sum_z q(z) \\log \\frac{q(z)}{p(z)}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- $\\Rightarrow$ When information is supplied in the form of constaints on the posterior, we *should* select the posterior that maximizes the Relative Entropy, subject to the constraints. This is the **Method of Maximum (Relative) Entropy** (MRE). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Relative Entropy, KL Divergence and Free Energy\n",
    "\n",
    "- Note that the Relative Entropy is technically a *functional*, i.e., a function of function(s) (since it is a function of probability distributions). The calculus of functionals is also called **variational calculus**.\n",
    "  - See  [Lanczos, The variational principles of mechanics (1961)](https://www.amazon.com/Variational-Principles-Mechanics-Dover-Physics/dp/0486650677) for a great book on variational calculus. For a summary, see Bishop (2016), app.D.\n",
    "  - It is customary to use square brackets (like $S[q,p]$) for functionals rather than round brackets, which we use for functions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Also note the complimentary relation between the Maximum Relative Entropy method and Probability Theory: \n",
    "  - PT describes how to _represent_ beliefs about events and how to _calculate_ beliefs about joint and conditional events. \n",
    "  - In contrast, the MRE method describes how to _update_ beliefs when new information becomes available.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- PT and the MRE method are both needed to describe the theory of optimal information processing. (As we will see below, Bayes rule is a special case of the MRE method.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In principle, entropies can always be considered as a *relative* score against a reference distribution. For instance, the score $-\\sum_{z_i} q(z_i) \\log q(z_i)$ can be interpreted as a score against the uniform distribution, i.e., $-\\sum_{z_i} q(z_i) \\log q(z_i) \\propto -\\sum_{z_i} q(z_i) \\log \\frac{q(z_i)}{\\mathrm{Uniform(z_i)}}$. Therefore, the \"method of maximum relative entropy\" is often simply known as the \"method of maximum entropy\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The negative relative entropy is known as the **Kullback-Leibler** (KL) divergence:\n",
    "$$\n",
    "\\mathrm{KL}[q,p] \\triangleq \\sum_z q(z) \\log \\frac{q(z)}{p(z)} \\tag{B-1.113}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The [Gibbs inequality](https://en.wikipedia.org/wiki/Gibbs%27_inequality) ([proof](https://en.wikipedia.org/wiki/Gibbs%27_inequality#Proof)), is a famous theorem in information theory that states that \n",
    "$$\\mathrm{KL}[q,p] \\geq 0 $$\n",
    "with equality only iff $p=q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "  - The KL divergence can be interpreted as a \"distance\" between two probability distributions. Note however that the KL divergence is an asymmetric distance measure, i.e., in general $\\mathrm{KL}[q,p] \\neq \\mathrm{KL}[p,q]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We also introduce here the notion of a (variational) **Free Energy** (FE) functional, which is just a generalization of the KL-divergence that allows the prior to be unnormalized. Let $f(z) = Z \\cdot p(z)$ be an unnormalized distribution, then the FE is defined as\n",
    "$$\\begin{align*}\n",
    "F[q,p] &\\triangleq \\sum_z q(z) \\log \\frac{q(z)}{f(z)} \\\\\n",
    "&= \\sum_z q(z) \\log \\frac{q(z)}{Z\\cdot p(z)} \\\\\n",
    "&= \\underbrace{\\sum_z q(z) \\log \\frac{q(z)}{p(z)}}_{\\text{KL divergence }\\geq 0} - \\log Z\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since the second term ($\\log Z$) is constant, FE minimization (subject to constraints) with respect to $q$ leads to the same posteriors as KL minimization and RE maximization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If we are only interested in minimizing FE with respect to $q$, then we'll write $F[q]$ (rather than $F[q,p]$) fo brevity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this class, we prefer to discuss inference in terms of minimizing Free Energy (subject to the constraints) rather than maximizing Relative Entropy, but note that these two concepts are equivalent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example KL divergence for Gaussians\n",
    "\n",
    "The following picture illustrates the KL-divergence between two Gaussian distributions.\n",
    "\n",
    "<p style=\"text-align:center;\"><img src=\"./figures/KL-Gauss-Example.png\" width=\"600\"></p>\n",
    "\n",
    "<span style=\"font-size:40%;text-align:center;\">source: By <a href=\"https://en.wikipedia.org/wiki/User:Mundhenk\" class=\"extiw\" title=\"w:User:Mundhenk\">Mundhenk</a> at <a href=\"https://en.wikipedia.org/wiki/\" class=\"extiw\" title=\"w:\">English Wikipedia</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=75315685\">Link</a></span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Free Energy Functional and Variational Bayes\n",
    "\n",
    "- Let's get back to the issue of doing inference for models with latent variables (such as the GMM). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider a generative model specified by $p(x,z)$ where $x$ and $z$ represent the observed and unobserved variables, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Assume further that $x$ has been observed as $x=\\hat{x}$ and we are interested in performing inference, i.e., we want to compute the posterior $p(z|x=\\hat{x})$ for the latent variables and we want to compute the evidence $p(x=\\hat{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- According to the Method of Maximum Relative Entropy, in order to find the correct posterior $q(x,z)$, we should minimize\n",
    "\n",
    "$$\n",
    "\\mathrm{F}[q] = \\sum_{x,z} q(x,z) \\log \\frac{q(x,z)}{p(x,z)}\\,, \\quad \\text{subject to data constraint }x=\\hat{x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The data constraint $x=\\hat{x}$ fixes posterior $q(x) = \\delta(x-\\hat{x})$, so the Free Energy evaluates to  \n",
    "\n",
    "$$\\begin{align*}\n",
    "F[q] &= \\sum_{z} \\sum_{x}q(z|x)q(x) \\log \\frac{q(z|x) q(x)}{p(z|x) p(x)} \\\\\n",
    "  &= \\sum_{z} \\sum_{x} q(z|x)\\delta(x-\\hat{x}) \\log \\frac{q(z|x)\\delta(x-\\hat{x})}{p(z|x) p(x)} \\\\\n",
    "  &= \\sum_{z} q(z|\\hat{x}) \\log \\frac{q(z|\\hat{x})}{p(z|\\hat{x}) p(\\hat{x}) } \\\\\n",
    "  &=  \\underbrace{\\sum_{z}q(z|\\hat{x}) \\log \\frac{q(z|\\hat{x})}{p(z|\\hat{x})}}_{\\text{KL divergence }\\geq 0} - \\underbrace{\\log p(\\hat{x})}_{\\text{log-evidence}}  \\tag{B-10.2}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The log-evidence term does not depend on $q$. Hence, the global minimum $q(z|\\hat{x})^* \\triangleq \\arg\\min_q F[q]$ is obtained for $$q^*(z|\\hat{x}) = p(z|\\hat{x})\\,,$$ which is the correct **Bayesian posterior**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Furthermore, the minimal free energy $$F^* \\triangleq F[q^*] = -\\log p(\\hat{x})$$ equals minus **log-evidence**. (Or, equivalently, the evidence is given by $p(\\hat{x}) = \\exp\\left(-F[q^*] \\right)$.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  This is an amazing result!  Note that FE minimization transforms an inference problem (that involves integration) to an optimization problem! Generally, optimization problems are easier to solve than integration problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Executing inference by minimizing the variational FE functional is called **Variational Bayes** (VB) or variational inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  (As an aside), note that Bishop introduces in Eq. B-10.2 an _Evidence Lower BOund_ (in modern machine learning literature abbreviated as **ELBO**) $\\mathcal{L}(z)$ that equals the _negative_ FE. We prefer to discuss variational inference in terms of a free energy (but it is the same story as he discusses with the ELBO).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  FE Minimization as Approximate Bayesian Inference\n",
    "\n",
    "- In the rest of this lesson, we are concerned with how to execute FE minimization (FEM) w.r.t $q$ for the functional\n",
    "$$\n",
    " F[q] = \\sum_{z}q(z) \\log \\frac{q(z)}{p(z|x)} - \\log p(x)  \n",
    " $$\n",
    "where $x$ has been observed and $z$ represent all latent variables.\n",
    "  - To keep the notation uncluttered, in the following we write $x$ rather than $\\hat{x}$, and we write simply $q(z)$ (rather than $q(z|\\hat{x})$) for the posterior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Due to restrictions in our optimization algorithm, we are often unable to fully minimize the FE to the global minimum $q^*(z)$, but rather get stuck in a local minimum $\\hat{q}(z)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that, since $\\mathrm{KL}[q(z),p(z|x)]\\geq 0$ for any $q(z)$, the FE is always an upperbound on (minus) log-evidence, i.e.,\n",
    "$$\n",
    "F[q] \\geq -\\log p(x) \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In practice, even if we cannot attain the global minimum, we can still use the local minimum $\\hat{q}(z) \\approx \\arg\\min_q F[q]$ to accomplish **approximate Bayesian inference**:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{align*}\n",
    "   \\text{posterior: } \\hat{q}(z) &\\approx p(z|x) \\\\\n",
    "   \\text{evidence: } p(x) &\\approx \\exp\\left( -F[\\hat{q}]\\right) \n",
    "    \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constrained FE Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Generally speaking, it can be very helpful in the FE minimization task to add some additional constraints on $q(z)$ (i.e., in addition to the data constraints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Once we add constraints to $q$ (in addition to data constraints), we are no longer guaranteed that the minimum of the (constrained) FE coincides with the solution by Bayes rule (which only takes data as constraints). So again, at best constrained FEM is only an **approximation to Bayes rule**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There are three important cases of adding constraints to $q(z)$ that often alleviates the FE minimization task:\n",
    "\n",
    "#### 1. mean-field factorization\n",
    "- We constrain the posterior to factorize into a set of _independent_ factors, i.e., \n",
    "$$\n",
    "q(z) = \\prod_{j=1}^m q_j(z_j)\\,, \\tag{B-10.5}\n",
    "$$ \n",
    "  - Variational inference with mean-field factorization has been worked out in detail as the **Coordinate Ascent Variational Inference** (CAVI) algorithm. See the [Optional Slide on CAVI](#CAVI) for details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "####  2. fixed-form parameterization\n",
    "- We constrain the posterior to be part of a parameterized probability distribution, e.g., $$q(z) = \\mathcal{N}\\left( z | \\mu, \\Sigma \\right)\\,.$$ \n",
    "  - In this case, the functional minimization problem for $F[q]$ reduces to the minimization of a _function_ $F(\\mu,\\Sigma)$ w.r.t. its parameters. We can often use standand gradient-based optimization methods to minimize the FE.   \n",
    "  - The following image by [David Blei](https://www.cs.columbia.edu/~blei/) illustrates this approach\n",
    "  <p style=\"text-align:center;\"><img src=\"./figures/blei-variational-inference.png\" width=\"600\"></p>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 3. the Expectation-Maximization (EM) algorithm\n",
    "- We place some constraints both on the prior and posterior for $z$ ([also to be discussed in an Optional Slide](#EM-Algorithm)) that simplifies FE minimization to maximum-likelihood estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: FEM for the Gaussian Mixture Model (CAVI Approach)\n",
    "\n",
    "- Let's get back to the illustrative example at the beginning of this lesson: we want to do [density modeling for the Old Faithful data set](#illustrative-example).\n",
    "\n",
    "#### model specification\n",
    "\n",
    "\n",
    "- We consider a Gaussian Mixture Model, specified by \n",
    "$$\\begin{align*}\n",
    "p(x,z|\\theta) &= p(x|z,\\mu,\\Lambda)p(z|\\pi) \\\\\n",
    "  &= \\prod_{n=1}^N \\prod_{k=1}^K \\left(\\pi_k \\cdot \\mathcal{N}\\left( x_n | \\mu_k, \\Lambda_k^{-1}\\right)\\right)^{z_{nk}} \\tag{B-10.37,38}\n",
    "\\end{align*}$$\n",
    "with tuning parameters $\\theta=\\{\\pi_k, \\mu_k,\\Lambda_k\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let us introduce some priors for the parameters. We factorize the prior and choose conjugate distributions by\n",
    "$$\n",
    "p(\\theta) = p(\\pi,\\mu,\\Lambda) = p(\\pi) p(\\mu|\\Lambda) p(\\Lambda)\n",
    "$$\n",
    "with \n",
    "$$\\begin{align*}\n",
    "p(\\pi) &= \\mathrm{Dir}(\\pi|\\alpha_0) = C(\\alpha_0) \\prod_k \\pi_k^{\\alpha_0-1} \\tag{B-10.39}\\\\\n",
    "p(\\mu|\\Lambda) &= \\prod_k \\mathcal{N}\\left(\\mu_k | m_0, \\left( \\beta_0 \\Lambda_k\\right)^{-1} \\right) \\tag{B-10.40}\\\\\n",
    "p(\\Lambda) &= \\prod_k \\mathcal{W}\\left( \\Lambda_k | W_0, \\nu_0 \\right) \\tag{B-10.40}\n",
    "\\end{align*}$$\n",
    "where $\\mathcal{W}\\left( \\cdot \\right)$ is a [Wishart distribution](https://en.wikipedia.org/wiki/Wishart_distribution) (i.e., a multi-dimensional Gamma distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The full generative model is now specified by\n",
    "$$\n",
    "p(x,z,\\pi,\\mu,\\Lambda) = p(x|z,\\mu,\\Lambda) p(z|\\pi) p(\\pi) p(\\mu|\\Lambda) p(\\Lambda) \\tag{B-10.41}\n",
    "$$\n",
    "with hyperparameters $\\{ \\alpha_0, m_0, \\beta_0, W_0, \\nu_0\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### inference\n",
    "\n",
    "- Assume that we have observed $D = \\left\\{x_1, x_2, \\ldots, x_N\\right\\}$ and are interested to infer the posterior distribution for the tuning parameters: \n",
    "$$\n",
    "p(\\theta|D) = p(\\pi,\\mu,\\Lambda|D)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The (perfect) Bayesian solution is \n",
    "$$\n",
    "p(\\theta|D) = \\frac{p(x=D,\\theta)}{p(x=D)} = \\frac{\\sum_z p(x=D,z,\\pi,\\mu,\\Lambda)}{\\sum_z \\sum_{\\pi} \\iint p(x=D,z,\\pi,\\mu,\\Lambda) \\,\\mathrm{d}\\mu\\mathrm{d}\\Lambda}\n",
    "$$\n",
    "but this is intractable (See [Blei (2017), p861, eqs. 8 and 9](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- <a id=\"mf-constraint\"></a>Alternatively, we can use **FE minimization with factorization constraint** \n",
    "$$\\begin{equation}\n",
    "q(\\theta) = q(z) \\cdot q(\\pi,\\mu,\\Lambda) \\tag{B-10.42}\n",
    "\\end{equation}$$ \n",
    "on the posterior. For the specified model, this extra constaint leads to FE minimization wrt the hyperparameters, i.e., we need to minimize the function $F(\\alpha_0, m_0, \\beta_0, W_0, \\nu_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Bishop shows that the equations for the [optimal solutions (Eq. B-10.9)](#optimal-solutions) are analytically solvable for the GMM as specified above, leading to the following variational update equations (for $k=1,\\ldots,K$): \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\alpha_k &= \\alpha_0 + N_k  \\tag{B-10.58} \\\\\n",
    "\\beta_k &= \\beta_0 + N_k  \\tag{B-10.60} \\\\\n",
    "m_k &= \\frac{1}{\\beta_k} \\left( \\beta_0 m_0 + N_k \\bar{x}_k \\right) \\tag{B-10.61} \\\\\n",
    "W_k^{-1} &= W_0^{-1} + N_k S_k + \\frac{\\beta_0 N_k}{\\beta_0 + N_k}\\left( \\bar{x}_k - m_0\\right) \\left( \\bar{x}_k - m_0\\right)^T \\tag{B-10.62} \\\\\n",
    "\\nu_k &= \\nu_0 + N_k \\tag{B-10.63}\n",
    "\\end{align*}\n",
    "$$\n",
    "where we used\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log \\rho_{nk} &= \\mathbb{E}\\left[ \\log \\pi_k\\right] + \\frac{1}{2}\\mathbb{E}\\left[ \\log | \\Lambda_k | \\right] - \\frac{D}{2} \\log(2\\pi) \\\\ \n",
    " & \\qquad - \\frac{1}{2}\\mathbb{E}\\left[(x_k - \\mu_k)^T \\Lambda_k(x_k - \\mu_k)  \\right]  \\tag{B-10.46} \\\\\n",
    "r_{nk} &= \\frac{\\rho_{nk}}{\\sum_{j=1}^K \\rho_{nj}} \\tag{B-10.49} \\\\\n",
    "N_k &= \\sum_{n=1}^N r_{nk} x_n \\tag{B-10.51} \\\\\n",
    "\\bar{x}_k &= \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} x_n \\tag{B-10.52} \\\\\n",
    "S_k &= \\frac{1}{N_k} \\sum_{n=1}^N r_{nk} \\left( x_n - \\bar{x}_k\\right) \\left( x_n - \\bar{x}_k\\right)^T \\tag{B-10.53}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Exam guide: Working out FE minimization for the GMM to these update equations (eqs B-10.58 through B-10.63) is not something that you need to reproduce without assistance at the exam. Rather, the essence is that *it is possible* to arrive at closed-form variational update equations for the GMM. You should understand though how FEM works conceptually and in principle be able to derive variational update equations for very simple models that do not involve clever mathematical tricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code Example: FEM for GMM on Old Faithfull data set\n",
    "\n",
    "- Below we exemplify training of a Gaussian Mixture Model on the Old Faithful data set by Free Energy Minimization, using the constraints as specified above, e.g., [(B-10.42)](#mf-constraint). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n",
      "└ @ Base loading.jl:1664\n",
      "\u001b[91m\u001b[1mERROR: \u001b[22m\u001b[39mLoadError: ccall method definition: argument 1 type doesn't correspond to a C type\n",
      "Stacktrace:\n",
      " [1] top-level scope\n",
      "\u001b[90m   @ \u001b[39m\u001b[90m~/.julia/packages/SentinelArrays/iHRpO/src/\u001b[39m\u001b[90m\u001b[4mSentinelArrays.jl:209\u001b[24m\u001b[39m\n",
      " [2] \u001b[0m\u001b[1minclude\u001b[22m\n",
      "\u001b[90m   @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mBase.jl:419\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      " [3] \u001b[0m\u001b[1minclude_package_for_output\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mpkg\u001b[39m::\u001b[0mBase.PkgId, \u001b[90minput\u001b[39m::\u001b[0mString, \u001b[90mdepot_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mdl_load_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mload_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mconcrete_deps\u001b[39m::\u001b[0mVector\u001b[90m{Pair{Base.PkgId, UInt64}}\u001b[39m, \u001b[90msource\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m   @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1554\u001b[24m\u001b[39m\n",
      " [4] top-level scope\n",
      "\u001b[90m   @ \u001b[39m\u001b[90m\u001b[4mstdin:1\u001b[24m\u001b[39m\n",
      "in expression starting at /Users/bert/.julia/packages/SentinelArrays/iHRpO/src/SentinelArrays.jl:1\n",
      "in expression starting at stdin:1\n",
      "\u001b[91m\u001b[1mERROR: \u001b[22m\u001b[39mLoadError: Failed to precompile SentinelArrays [91c51154-3ec4-41a3-a24f-3f23e20d615c] to /Users/bert/.julia/compiled/v1.8/SentinelArrays/jl_HW0nHY.\n",
      "Stacktrace:\n",
      "  [1] \u001b[0m\u001b[1merror\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90ms\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4merror.jl:35\u001b[24m\u001b[39m\n",
      "  [2] \u001b[0m\u001b[1mcompilecache\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mpkg\u001b[39m::\u001b[0mBase.PkgId, \u001b[90mpath\u001b[39m::\u001b[0mString, \u001b[90minternal_stderr\u001b[39m::\u001b[0mIO, \u001b[90minternal_stdout\u001b[39m::\u001b[0mIO, \u001b[90mkeep_loaded_modules\u001b[39m::\u001b[0mBool\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1707\u001b[24m\u001b[39m\n",
      "  [3] \u001b[0m\u001b[1mcompilecache\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1651\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "  [4] \u001b[0m\u001b[1m_require\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mpkg\u001b[39m::\u001b[0mBase.PkgId\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1337\u001b[24m\u001b[39m\n",
      "  [5] \u001b[0m\u001b[1m_require_prelocked\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90muuidkey\u001b[39m::\u001b[0mBase.PkgId\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1200\u001b[24m\u001b[39m\n",
      "  [6] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1180\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "  [7] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mlock.jl:223\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      "  [8] \u001b[0m\u001b[1mrequire\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90minto\u001b[39m::\u001b[0mModule, \u001b[90mmod\u001b[39m::\u001b[0mSymbol\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1144\u001b[24m\u001b[39m\n",
      "  [9] \u001b[0m\u001b[1minclude\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90m\u001b[4mBase.jl:419\u001b[24m\u001b[39m\u001b[90m [inlined]\u001b[39m\n",
      " [10] \u001b[0m\u001b[1minclude_package_for_output\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mpkg\u001b[39m::\u001b[0mBase.PkgId, \u001b[90minput\u001b[39m::\u001b[0mString, \u001b[90mdepot_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mdl_load_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mload_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mconcrete_deps\u001b[39m::\u001b[0mVector\u001b[90m{Pair{Base.PkgId, UInt64}}\u001b[39m, \u001b[90msource\u001b[39m::\u001b[0mNothing\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90m\u001b[4mloading.jl:1554\u001b[24m\u001b[39m\n",
      " [11] top-level scope\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m\u001b[4mstdin:1\u001b[24m\u001b[39m\n",
      "in expression starting at /Users/bert/.julia/packages/CSV/Zl2ww/src/CSV.jl:1\n",
      "in expression starting at stdin:1\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "Failed to precompile CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b] to /Users/bert/.julia/compiled/v1.8/CSV/jl_SQ0k1U.",
     "output_type": "error",
     "traceback": [
      "Failed to precompile CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b] to /Users/bert/.julia/compiled/v1.8/CSV/jl_SQ0k1U.",
      "",
      "Stacktrace:",
      "  [1] error(s::String)",
      "    @ Base ./error.jl:35",
      "  [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IO, internal_stdout::IO, keep_loaded_modules::Bool)",
      "    @ Base ./loading.jl:1707",
      "  [3] compilecache",
      "    @ ./loading.jl:1651 [inlined]",
      "  [4] _require(pkg::Base.PkgId)",
      "    @ Base ./loading.jl:1337",
      "  [5] _require_prelocked(uuidkey::Base.PkgId)",
      "    @ Base ./loading.jl:1200",
      "  [6] macro expansion",
      "    @ ./loading.jl:1180 [inlined]",
      "  [7] macro expansion",
      "    @ ./lock.jl:223 [inlined]",
      "  [8] require(into::Module, mod::Symbol)",
      "    @ Base ./loading.jl:1144",
      "  [9] eval",
      "    @ ./boot.jl:368 [inlined]",
      " [10] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "using DataFrames, CSV, LinearAlgebra, PDMats, SpecialFunctions\n",
    "include(\"scripts/gmm_plot.jl\") # Holds plotting function \n",
    "old_faithful = CSV.read(\"datasets/old_faithful.csv\",DataFrame);\n",
    "X = convert(Matrix{Float64}, [old_faithful[!,1] old_faithful[!,2]]');#data matrix\n",
    "N = size(X, 2) #number of observations\n",
    "K = 6\n",
    "\n",
    "function sufficientStatistics(X,r,k::Int) #function to compute sufficient statistics\n",
    "    N_k = sum(r[k,:])\n",
    "    hat_x_k = sum([r[k,n]*X[:,n] for n in 1:N]) ./ N_k\n",
    "    S_k = sum([r[k,n]*(X[:,n]-hat_x_k)*(X[:,n]-hat_x_k)' for n in 1:N]) ./ N_k\n",
    "    return N_k, hat_x_k, S_k\n",
    "end\n",
    "\n",
    "function updateMeanPrecisionPi(m_0,β_0,W_0,ν_0,α_0,r) #variational maximisation function\n",
    "    m = Array{Float64}(undef,2,K) #mean of the clusters \n",
    "    β = Array{Float64}(undef,K) #precision scaling for Gausian distribution\n",
    "    W = Array{Float64}(undef,2,2,K) #precision prior for Wishart distributions\n",
    "    ν = Array{Float64}(undef,K) #degrees of freedom parameter for Wishart distribution\n",
    "    α = Array{Float64}(undef,K) #Dirichlet distribution parameter \n",
    "    for k=1:K\n",
    "        sst = sufficientStatistics(X,r,k)\n",
    "        α[k] = α_0[k] + sst[1]; β[k] = β_0[k] + sst[1]; ν[k] = ν_0[k] .+ sst[1]\n",
    "        m[:,k] = (1/β[k])*(β_0[k].*m_0[:,k] + sst[1].*sst[2])\n",
    "        W[:,:,k] = inv(inv(W_0[:,:,k])+sst[3]*sst[1] + ((β_0[k]*sst[1])/(β_0[k]+sst[1])).*(sst[2]-m_0[:,k])*(sst[2]-m_0[:,k])')\n",
    "    end\n",
    "    return m,β,W,ν,α\n",
    "end\n",
    "\n",
    "function updateR(Λ,m,α,ν,β) #variational expectation function\n",
    "    r = Array{Float64}(undef,K,N) #responsibilities \n",
    "    hat_π = Array{Float64}(undef,K) \n",
    "    hat_Λ = Array{Float64}(undef,K)\n",
    "    for k=1:K\n",
    "        hat_Λ[k] = 1/2*(2*log(2)+logdet(Λ[:,:,k])+digamma(ν[k]/2)+digamma((ν[k]-1)/2))\n",
    "        hat_π[k] = exp(digamma(α[k])-digamma(sum(α)))\n",
    "        for n=1:N\n",
    "           r[k,n] = hat_π[k]*exp(-hat_Λ[k]-1/β[k] - (ν[k]/2)*(X[:,n]-m[:,k])'*Λ[:,:,k]*(X[:,n]-m[:,k]))\n",
    "        end\n",
    "    end\n",
    "    for n=1:N\n",
    "        r[:,n] = r[:,n]./ sum(r[:,n]) #normalize to ensure r represents probabilities \n",
    "    end\n",
    "    return r\n",
    "end\n",
    "\n",
    "max_iter = 15\n",
    "#store the inference results in these vectors\n",
    "ν = fill!(Array{Float64}(undef,K,max_iter),3)\n",
    "β = fill!(Array{Float64}(undef,K,max_iter),1.0)\n",
    "α = fill!(Array{Float64}(undef,K,max_iter),0.01)\n",
    "R = Array{Float64}(undef,K,N,max_iter)\n",
    "M = Array{Float64}(undef,2,K,max_iter)\n",
    "Λ = Array{Float64}(undef,2,2,K,max_iter)\n",
    "clusters_vb = Array{Distribution}(undef,K,max_iter) #clusters to be plotted\n",
    "#initialize prior distribution parameters\n",
    "M[:,:,1] = [[3.0; 60.0];[4.0; 70.0];[2.0; 50.0];[2.0; 60.0];[3.0; 100.0];[4.0; 80.0]]\n",
    "for k=1:K\n",
    "    Λ[:,:,k,1] = [1.0 0;0 0.01]\n",
    "    R[k,:,1] = 1/(K)*ones(N)\n",
    "    clusters_vb[k,1] = MvNormal(M[:,k,1],PDMats.PDMat(convert(Matrix,Hermitian(inv(ν[1,1].*Λ[:,:,k,1])))))\n",
    "end\n",
    "#variational inference\n",
    "for i=1:max_iter-1\n",
    "    #variational expectation \n",
    "    R[:,:,i+1] = updateR(Λ[:,:,:,i],M[:,:,i],α[:,i],ν[:,i],β[:,i]) \n",
    "    #variational minimisation\n",
    "    M[:,:,i+1],β[:,i+1],Λ[:,:,:,i+1],ν[:,i+1],α[:,i+1] = updateMeanPrecisionPi(M[:,:,i],β[:,i],Λ[:,:,:,i],ν[:,i],α[:,i],R[:,:,i+1])\n",
    "    for k=1:K\n",
    "        clusters_vb[k,i+1] = MvNormal(M[:,k,i+1],PDMats.PDMat(convert(Matrix,Hermitian(inv(ν[k,i+1].*Λ[:,:,k,i+1])))))\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "subplot(2,3,1); plotGMM(X, clusters_vb[:,1], R[:,:,1]); title(\"Initial situation\")\n",
    "for i=2:6\n",
    "    subplot(2,3,i)\n",
    "    plotGMM(X, clusters_vb[:,i*2], R[:,:,i*2]); title(\"After $(i*2) iterations\")\n",
    "end\n",
    "PyPlot.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The generated figure looks much like Figure 10.6 in Bishop. The plots show results for Variational Bayesian\n",
    "mixture of K = 6 Gaussians applied to the Old Faithful data set, in\n",
    "which the ellipses denote the one\n",
    "standard-deviation density contours\n",
    "for each of the components, and the\n",
    "color coding of the data points reflects the \"soft\" class label assignments. Components whose expected\n",
    "mixing coefficient are numerically indistinguishable from zero are not\n",
    "plotted. Note that this procedure learns the number of classes (two), learns the class label for each observation, and learns the mean and variance for the Gaussian data distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Interesting Decompositions of the Free Energy Functional\n",
    "\n",
    "- <a id='fe-decompositions'></a> The FE functional can be decomposed in various interesting ways, making use of $p(x,z) = p(z|x)p(x) = p(x|z)p(z)$ \n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathrm{F}[q] &= \\underbrace{-\\sum_z q(z) \\log p(x,z)}_{\\text{energy}} - \\underbrace{\\sum_z q(z) \\log \\frac{1}{q(z)}}_{\\text{entropy}} \\tag{EE} \\\\\n",
    "&= \\underbrace{\\sum_z q(z) \\log \\frac{q(z)}{p(z|x)}}_{\\text{KL divergence}\\geq 0} - \\underbrace{\\log p(x)}_{\\text{log-evidence}} \\tag{DE}\\\\\n",
    "&= \\underbrace{\\sum_z q(z)\\log\\frac{q(z)}{p(z)}}_{\\text{complexity}} - \\underbrace{\\sum_z q(z) \\log p(x|z)}_{\\text{accuracy}}  \\tag{CA}\n",
    "\\end{align*}$$\n",
    "\n",
    "- These <a id=\"#fe-decompositions\">decompositions</a> are very insightful (we will revisit them later) and we will label them respectively as _energy-entropy_ (EE), _divergence-evidence_ (DE), and _complexity-accuracy_ (CA) decompositions. \n",
    "\n",
    "- In the [Bayesian Machine Learning](https://nbviewer.org/github/bertdv/BMLIP/blob/master/lessons/notebooks/Bayesian-Machine-Learning.ipynb) lecture, we discussed the CA decomposition of Bayesian model evidence to support the interpretation of evidence as a model performance criterion. Here, we recognize that FE allows a similar CA decomposition: minimizing FE increases data fit and decreases model complexity. Hence, FE is a good model performance criterion.\n",
    "\n",
    "- The CA decomposition makes use of the prior $p(z)$ and likelihood $p(x|z)$, both of which are selected by the engineer, so the FE can be evaluated with this decomposition!\n",
    "\n",
    "- The DE decomposition restates what we derived earlier, namely that the FE is an upperbound on the (negative) log-evidence. The bound is the KL-divergence between the variational posterior $q(z)$ and the (perfect) Bayesian posterior $p(z|x)$. Global minimization of FE with only data constraints drives the KL-divergence to zero and results to perfect Bayesian inference.  \n",
    "\n",
    "- The EE decomposition provides a link to the [second law of thermodynamics](https://en.wikipedia.org/wiki/Second_law_of_thermodynamics): Minimizing FE leads to entropy maximization, subject to constraints, where in this case the constraints are imposed by the postulated generative model. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Summary\n",
    "\n",
    "- Latent variable models (LVM) contain a set of unobserved variables whose size grows with the number of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- LVMs can model more complex phenomena than fully observed models, but inference in LVM models is usually not analytically solvable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The Free Energy (FE) functional transforms Bayesian inference computations (very large summations or integrals) to an optimization problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Inference by minimizing FE, also known as variational inference, is fully consistent with the \"Method of Maximum Relative Entropy\", which is by design the rational way to update beliefs from priors to posteriors when new information becomes available. Thus, FE mimimization is the \"correct\" inference procedure that generalizes Bayes rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In general, global FE minimization is an unsolved problem and finding good local minima is at the heart of current Bayesian technology research. Three simplifying constraints on the posterior $q(z)$ in the FE functional are currently popular in practical settings:\n",
    "    - mean-field assumptions\n",
    "    - assuming a parametric from for $q$\n",
    "    - EM algorithm\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   \n",
    "- These constraints often makes FE minimization implementable at the price of obtaining approximately Bayesian inference results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The back-ends of Probabilistic Programming Languages (such as [ReactiveMP.jl](https://github.com/biaslab/ReactiveMP.jl)) often contain lots of methods for automating constrained FE minimization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><a id=\"optional-slides\">OPTIONAL SLIDES</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE Minimization with Mean-field Factorization Constraints: <a id=\"CAVI\">the CAVI Approach</a> \n",
    "\n",
    "- Let's work out FE minimization with additional mean-field constraints (=full factorization) constraints:  $$q(z) = \\prod_{j=1}^m q_j(z_j)\\,.$$\n",
    "\n",
    "  - In other words, the posteriors for $z_j$ are all considered independent. This is a strong constraint but leads often to good solutions.\n",
    "  \n",
    "- Given the mean-field constraints, it is possible to derive the following expression for the <a id=\"optimal-solutions\">optimal solutions</a> $q_j^*(z_j)$, for $j=1,\\ldots,m$: \n",
    "\n",
    "\\begin{equation*} \\tag{B-10.9}\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\log q_j^*(z_j) &\\propto \\mathrm{E}_{q_{-j}^*}\\left[ \\log p(x,z) \\right]  \\\\\n",
    "  &= \\underbrace{\\sum_{z_{-j}} q_{-j}^*(z_{-j}) \\underbrace{\\log p(x,z)}_{\\text{\"field\"}}}_{\\text{\"mean field\"}} \n",
    "\\end{aligned}}\n",
    "\\end{equation*}\n",
    "\n",
    "where we defined $q_{-j}^*(z_{-j}) \\triangleq q_1^*(z_1)q_2^*(z_2)\\cdots q_{j-1}^*(z_{j-1})q_{j+1}^*(z_{j+1})\\cdots q_m^*(z_m)$.\n",
    "- **Proof** (from [Blei, 2017](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773)): We first rewrite the FE as a function of $q_j(z_j)$ only: \n",
    "  $$ F[q_j] = \\mathbb{E}_{q_{j}}\\left[ \\mathbb{E}_{q_{-j}}\\left[ \\log p(x,z_j,z_{-j})\\right]\\right] - \\mathbb{E}_{q_j}\\left[ \\log q_j(z_j)\\right] + \\mathtt{const.}\\,,$$\n",
    "  where the constant holds all terms that do not depend on $z_j$. This expression can be written as \n",
    "  $$ F[q_j] = \\sum_{z_j} q_j(z_j) \\log \\frac{q_j(z_j)}{\\exp\\left( \\mathbb{E}_{q_{-j}}\\left[ \\log p(x,z_j,z_{-j})\\right]\\right)}$$\n",
    "  which is a KL-divergence that is minimized by Eq. B-10.9.  (end proof)\n",
    "  \n",
    "- This is not yet a full solution to the FE minimization task since the solution $q_j^*(z_j)$ depends on expectations that involve other solutions $q_{i\\neq j}^*(z_{i \\neq j})$, and each of these other solutions $q_{i\\neq j}^*(z_{i \\neq j})$ depends on an expection that involves $q_j^*(z_j)$. \n",
    "- In practice, we solve this chicken-and-egg problem by an iterative approach: we first initialize all $q_j(z_j)$ (for $j=1,\\ldots,m$) to an appropriate initial distribution and then cycle through the factors in turn by solving eq.B-10.9 and update $q_{-j}^*(z_{-j})$ with the latest estimates. (See [Blei, 2017](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773), Algorithm 1, p864).  \n",
    "\n",
    "- This algorithm for approximating Bayesian inference is known **Coordinate Ascent Variational Inference** (CAVI).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"EM-Algorithm\">FE Minimization by the Expectation-Maximization (EM) Algorithm</a>\n",
    "\n",
    "- The EM algorithm is a special case of FE minimization that focusses on Maximum-Likelihood estimation for models with latent variables. \n",
    "- Consider a model $$p(x,z,\\theta)$$ with observations $x = \\{x_n\\}$, latent variables $z=\\{z_n\\}$ and parameters $\\theta$.\n",
    "\n",
    "- We can write the following FE functional for this model:\n",
    "$$\\begin{align*}\n",
    "F[q] =  \\sum_z \\sum_\\theta q(z,\\theta) \\log \\frac{q(z,\\theta)}{p(x,z,\\theta)} \n",
    "\\end{align*}$$\n",
    "\n",
    "- The EM algorithm makes the following simplifying assumptions:\n",
    "  1. The prior for the parameters is uninformative (uniform). This implies that \n",
    "  $$p(x,z,\\theta) = p(x,z|\\theta) p(\\theta) \\propto p(x,z|\\theta)$$\n",
    "  2. A factorization constraint $$q(z,\\theta) = q(z) q(\\theta)$$\n",
    "  3. The posterior for the parameters is a delta function:\n",
    "  $$q(\\theta) = \\delta(\\theta - \\hat{\\theta})$$\n",
    " \n",
    "\n",
    " \n",
    "- Basically, these three assumptions turn FE minimization into maximum likelihood estimation for the parameters $\\theta$ and the FE simplifies to \n",
    "$$\\begin{align*}\n",
    "F[q,\\theta] =  \\sum_z q(z) \\log \\frac{q(z)}{p(x,z|\\theta)} \n",
    "\\end{align*}$$\n",
    "\n",
    "- The EM algorithm minimizes this FE by iterating (iteration counter: $i$) over \n",
    "\n",
    "\\begin{equation*}\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}^{(i)}(\\theta) &= \\sum_z \\overbrace{p(z|x,\\theta^{(i-1)})}^{q^{(i)}(z)}  \\log p(x,z|\\theta) \\quad &&\\text{the E-step} \\\\\n",
    "\\theta^{(i)} &= \\arg\\max_\\theta \\mathcal{L}^{(i)}(\\theta) &&\\text{the M-step}\n",
    "\\end{aligned}}\n",
    "\\end{equation*}\n",
    "\n",
    "- These choices are optimal for the given FE functional. In order to see this, consider the two decompositions\n",
    "$$\\begin{align}\n",
    "F[q,\\theta] &= \\underbrace{-\\sum_z q(z) \\log p(x,z|\\theta)}_{\\text{energy}} - \\underbrace{\\sum_z q(z) \\log \\frac{1}{q(z)}}_{\\text{entropy}} \\tag{EE}\\\\\n",
    "  &= \\underbrace{\\sum_z q(z) \\log \\frac{q(z)}{p(z|x,\\theta)}}_{\\text{divergence}} - \\underbrace{\\log p(x|\\theta)}_{\\text{log-likelihood}}  \\tag{DE}\n",
    "\\end{align}$$\n",
    "\n",
    "- The DE decomposition shows that the FE is minimized for the choice $q(z) := p(z|x,\\theta)$. Also, for this choice, the FE equals the (negative) log-evidence (, which is this case simplifies to the log-likelihood). \n",
    "\n",
    "- The EE decomposition shows that the FE is minimized wrt $\\theta$ by minimizing the energy term. The energy term is computed in the E-step and optimized in the M-step.\n",
    "  - Note that in the EM literature, the energy term is often called the _expected complete-data log-likelihood_.)\n",
    "\n",
    "- In order to execute the EM algorithm, it is assumed that we can analytically execute the E- and M-steps. For a large set of models (including models whose distributions belong to the exponential family of distributions), this is indeed the case and hence the large popularity of the EM algorithm. \n",
    "\n",
    "- The EM algorihm imposes rather severe assumptions on the FE (basically approximating Bayesian inference by maximum likelihood estimation). Over the past few years, the rise of Probabilistic Programming languages has dramatically increased the range of models for which the parameters can by estimated autmatically by (approximate) Bayesian inference, so the popularity of EM is slowly waning. (More on this in the Probabilistic Programming lessons). \n",
    "\n",
    "- Bishop (2006) works out EM for the GMM in section 9.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example: EM-algorithm for the GMM on the Old-Faithful data set\n",
    "\n",
    "We'll perform clustering on the data set from the [illustrative example](#illustrative-example) by fitting a GMM consisting of two Gaussians using the EM algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n",
      "└ @ Base loading.jl:1342\n",
      "\u001b[91m\u001b[1mERROR: \u001b[22m\u001b[39mLoadError: LoadError: TypeError: in Type{...} expression, expected UnionAll, got Type{Parsers.Options}\n",
      "Stacktrace:\n",
      "  [1] top-level scope\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CSV/Zl2ww/src/\u001b[39m\u001b[90;4mdetection.jl:164\u001b[0m\n",
      "  [2] \u001b[0m\u001b[1minclude\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mmod\u001b[39m::\u001b[0mModule, \u001b[90m_path\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mBase.jl:386\u001b[0m\n",
      "  [3] \u001b[0m\u001b[1minclude\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mx\u001b[39m::\u001b[0mString\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[35mCSV\u001b[39m \u001b[90m~/.julia/packages/CSV/Zl2ww/src/\u001b[39m\u001b[90;4mCSV.jl:1\u001b[0m\n",
      "  [4] top-level scope\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CSV/Zl2ww/src/\u001b[39m\u001b[90;4mCSV.jl:27\u001b[0m\n",
      "  [5] \u001b[0m\u001b[1minclude\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mBase.jl:386\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "  [6] \u001b[0m\u001b[1minclude_package_for_output\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mpkg\u001b[39m::\u001b[0mBase.PkgId, \u001b[90minput\u001b[39m::\u001b[0mString, \u001b[90mdepot_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mdl_load_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mload_path\u001b[39m::\u001b[0mVector\u001b[90m{String}\u001b[39m, \u001b[90mconcrete_deps\u001b[39m::\u001b[0mVector\u001b[90m{Pair{Base.PkgId, UInt64}}\u001b[39m, \u001b[90msource\u001b[39m::\u001b[0mNothing\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90mBase\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mloading.jl:1235\u001b[0m\n",
      "  [7] top-level scope\n",
      "\u001b[90m    @ \u001b[39m\u001b[90;4mnone:1\u001b[0m\n",
      "  [8] \u001b[0m\u001b[1meval\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mboot.jl:360\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "  [9] \u001b[0m\u001b[1meval\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mx\u001b[39m::\u001b[0mExpr\u001b[0m\u001b[1m)\u001b[22m\n",
      "\u001b[90m    @ \u001b[39m\u001b[90mBase.MainInclude\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mclient.jl:446\u001b[0m\n",
      " [10] top-level scope\n",
      "\u001b[90m    @ \u001b[39m\u001b[90;4mnone:1\u001b[0m\n",
      "in expression starting at /Users/bert/.julia/packages/CSV/Zl2ww/src/detection.jl:164\n",
      "in expression starting at /Users/bert/.julia/packages/CSV/Zl2ww/src/CSV.jl:1\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "Failed to precompile CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b] to /Users/bert/.julia/compiled/v1.6/CSV/jl_zIZor3.",
     "output_type": "error",
     "traceback": [
      "Failed to precompile CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b] to /Users/bert/.julia/compiled/v1.6/CSV/jl_zIZor3.",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base ./error.jl:33",
      " [2] compilecache(pkg::Base.PkgId, path::String, internal_stderr::IJulia.IJuliaStdio{Base.PipeEndpoint}, internal_stdout::IJulia.IJuliaStdio{Base.PipeEndpoint}, ignore_loaded_modules::Bool)",
      "   @ Base ./loading.jl:1385",
      " [3] compilecache(pkg::Base.PkgId, path::String)",
      "   @ Base ./loading.jl:1329",
      " [4] _require(pkg::Base.PkgId)",
      "   @ Base ./loading.jl:1043",
      " [5] require(uuidkey::Base.PkgId)",
      "   @ Base ./loading.jl:936",
      " [6] require(into::Module, mod::Symbol)",
      "   @ Base ./loading.jl:923",
      " [7] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [8] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "using DataFrames, CSV, LinearAlgebra\n",
    "include(\"scripts/gmm_plot.jl\") # Holds plotting function \n",
    "old_faithful = CSV.read(\"datasets/old_faithful.csv\", DataFrame);\n",
    "\n",
    "X =  Array(Matrix{Float64}(old_faithful)')\n",
    "N = size(X, 2)\n",
    "\n",
    "# Initialize the GMM. We assume 2 clusters.\n",
    "clusters = [MvNormal([4.;60.], [.5 0;0 10^2]); \n",
    "            MvNormal([2.;80.], [.5 0;0 10^2])];\n",
    "π_hat = [0.5; 0.5]                    # Mixing weights\n",
    "γ = fill!(Matrix{Float64}(undef,2,N), NaN)  # Responsibilities (row per cluster)\n",
    "\n",
    "# Define functions for updating the parameters and responsibilities\n",
    "function updateResponsibilities!(X, clusters, π_hat, γ)\n",
    "    # Expectation step: update γ\n",
    "    norm = [pdf(clusters[1], X) pdf(clusters[2], X)] * π_hat\n",
    "    γ[1,:] = (π_hat[1] * pdf(clusters[1],X) ./ norm)'\n",
    "    γ[2,:] = 1 .- γ[1,:]\n",
    "end\n",
    "function updateParameters!(X, clusters, π_hat, γ)\n",
    "    # Maximization step: update π_hat and clusters using ML estimation\n",
    "    m = sum(γ, dims=2)\n",
    "    π_hat = m / N\n",
    "    μ_hat = (X * γ') ./ m'\n",
    "    for k=1:2\n",
    "        Z = (X .- μ_hat[:,k])\n",
    "        Σ_k = Symmetric(((Z .* (γ[k,:])') * Z') / m[k])\n",
    "        clusters[k] = MvNormal(μ_hat[:,k], convert(Matrix, Σ_k))\n",
    "    end\n",
    "end\n",
    "\n",
    "# Execute the algorithm: iteratively update parameters and responsibilities\n",
    "subplot(2,3,1); plotGMM(X, clusters, γ); title(\"Initial situation\")\n",
    "updateResponsibilities!(X, clusters, π_hat, γ)\n",
    "subplot(2,3,2); plotGMM(X, clusters, γ); title(\"After first E-step\")\n",
    "updateParameters!(X, clusters, π_hat, γ)\n",
    "subplot(2,3,3); plotGMM(X, clusters, γ); title(\"After first M-step\")\n",
    "iter_counter = 1\n",
    "for i=1:3\n",
    "    for j=1:i+1\n",
    "        updateResponsibilities!(X, clusters, π_hat, γ)\n",
    "        updateParameters!(X, clusters, π_hat, γ)\n",
    "        iter_counter += 1\n",
    "    end\n",
    "    subplot(2,3,3+i); \n",
    "    plotGMM(X, clusters, γ); \n",
    "    title(\"After $(iter_counter) iterations\")\n",
    "end\n",
    "PyPlot.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can step through the interactive demo yourself by running [this script](https://github.com/bertdv/AIP-5SSB0/blob/master/lessons/notebooks/scripts/interactive_em_demo.jl) in julia. You can run a script in julia by    \n",
    "`julia> include(\"path/to/script-name.jl\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Passing for Free Energy Minimization\n",
    "\n",
    "- The Sum-Product (SP) update rule implements perfect Bayesian inference. \n",
    "- Sometimes, the SP update rule is not analytically solvable. \n",
    "- Fortunately, for many well-known Bayesian approximation methods, a message passing update rule can be created, e.g. [Variational Message Passing](https://en.wikipedia.org/wiki/Variational_message_passing) (VMP) for variational inference. \n",
    "- In general, all of these message passing algorithms can be interpreted as minimization of a constrained free energy (e.g., see [Senoz et al. (2021)](https://www.mdpi.com/1099-4300/23/7/807), and hence these message passing schemes comply with [Caticha's Method of Maximum Relative Entropy](https://arxiv.org/abs/1011.0723), which, as discussed in the [variational Bayes lesson](https://nbviewer.jupyter.org/github/bertdv/BMLIP/blob/master/lessons/notebooks/Latent-Variable-Models-and-VB.ipynb) is the proper way for updating beliefs. \n",
    "- Different message passing updates rules can be combined to get a hybrid inference method in one model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Local Free Energy in a Factor Graph \n",
    "\n",
    "- Consider an edge $x_j$ in a Forney-style factor graph for a generative model $p(x) = p(x_1,x_2,\\ldots,x_N)$.\n",
    "\n",
    "\n",
    "\n",
    "- Assume that the graph structure (factorization) is specified by\n",
    "$$\n",
    "p(x) = \\prod_{a=1}^M p_a(x_a)\n",
    "$$\n",
    "where $a$ is a set of indices.\n",
    "- Also, we assume a mean-field approximation for the posterior:\n",
    "$$\n",
    "q(x) = \\prod_{i=1}^N q_i(x_i)\n",
    "$$\n",
    "and consequently a corresponding free energy functional  \n",
    "$$\\begin{align*}\n",
    "F[q] &= \\sum_x q(x) \\log \\frac{q(x)}{p(x)} \\\\\n",
    "  &= \\sum_i \\sum_{x_i} \\left(\\prod_{i=1}^N q_i(x_i)\\right) \\log \\frac{\\prod_{i=1}^N q_i(x_i)}{\\prod_{a=1}^M p_a(x_a)}\n",
    "\\end{align*}$$\n",
    "\n",
    "- With these assumptions, it can be shown that the FE evaluates to (exercise)\n",
    "$$\n",
    "F[q] = \\sum_{a=1}^M \\underbrace{\\sum_{x_a} \\left( \\prod_{j\\in N(a)} q_j(x_j)\\cdot \\left(-\\log p_a(x_a)\\right) \\right) }_{\\text{node energy }U[p_a]} - \\sum_{i=1}^N \\underbrace{\\sum_{x_i} q_i(x_i) \\log \\frac{1}{q_i(x_i)}}_{\\text{edge entropy }H[q_i]}\n",
    "$$\n",
    "\n",
    "- In words, the FE decomposes into a sum of (expected) energies for the nodes minus the entropies on the edges. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Message Passing\n",
    "\n",
    "- Let us now consider the local free energy that is associated with edge corresponding to $x_j$. \n",
    "  \n",
    "  <p style=\"text-align:center;\"><img src=\"./figures/VMP-two-nodes.png\" width=\"600\"></p>\n",
    "  \n",
    "  \n",
    "- Apparently (see previous slide), there are three contributions to the free energy for $x_j$:\n",
    "    - one entropy term for the edge $x_j$\n",
    "    - two energy terms: one for each node that attaches to $x_j$ (in the figure: nodes $p_a$ and $p_b$)\n",
    "    \n",
    "- The local free energy for $x_j$ can be written as (exercise)\n",
    "  $$\n",
    "  F[q_j] \\propto \\sum_{x_j} q(x_j) \\log \\frac{q_j(x_j)}{\\nu_a(x_j)\\cdot \\nu_b(x_j)}\n",
    "  $$\n",
    "  where\n",
    "  $$\\begin{align*} \n",
    "  \\nu_a(x_j) &\\propto \\exp\\left( \\mathbb{E}_{q_{k}}\\left[ \\log p_a(x_a)\\right]\\right) \\\\\n",
    "  \\nu_b(x_j) &\\propto \\exp\\left( \\mathbb{E}_{q_{l}}\\left[ \\log p_b(x_b)\\right]\\right) \n",
    "  \\end{align*}$$\n",
    "  and $\\mathbb{E}_{q_{k}}\\left[\\cdot\\right]$ is an expectation w.r.t. all $q(x_k)$ with $k \\in N(a)\\setminus {j}$.\n",
    "  \n",
    "- $\\nu_a(x_j)$ and $\\nu_b(x_j)$  can be locally computed in nodes $a$ and $b$ respectively and can be interpreted as colliding messages over edge $x_j$. \n",
    "  \n",
    "- Local free energy minimization is achieved by setting\n",
    "  $$\n",
    "  q_j(x_j) \\propto \\nu_a(x_j) \\cdot \\nu_b(x_j)\n",
    "  $$\n",
    "  \n",
    "- Note that message $\\nu_a(x_j)$ depends on posterior beliefs over incoming edges ($k$) for node $a$, and in turn, the message from node $a$ towards edge $x_k$ depends on the belief $q_j(x_j)$. I.o.w., direct mutual dependencies exist between posterior beliefs over edges that attach to the same node. \n",
    "  \n",
    "- These considerations lead to the [Variational Message Passing](https://en.wikipedia.org/wiki/Variational_message_passing) procedure, which is an iterative free energy minimization procedure that can be executed completely through locally computable messages.  \n",
    "\n",
    "- Procedure VMP, see [Dauwels (2007), section 3](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Dauwels-2007-on-variational-message-passing-on-factor-graphs)\n",
    "  > 1. Initialize all messages $q$ and $ν$, e.g., $q(\\cdot) \\propto 1$ and $\\nu(\\cdot) \\propto 1$. <br/>\n",
    "  > 2. Select an edge $z_k$ in the factor graph of $f(z_1,\\ldots,z_m)$.<br/>\n",
    "  > 3. Compute the two messages $\\overrightarrow{\\nu}(z_k)$ and $\\overleftarrow{\\nu}(z_k)$ by applying the following generic rule:\n",
    "  $$\n",
    "  \\overrightarrow{\\nu}(y) \\propto \\exp\\left( \\mathbb{E}_{q}\\left[ \\log g(x_1,\\dots,x_n,y)\\right] \\right) \n",
    "  $$\n",
    "  > 4. Compute the marginal $q(z_k)$\n",
    "  $$\n",
    "  q(z_k) \\propto \\overrightarrow{\\nu}(z_k) \\overleftarrow{\\nu}(z_k)\n",
    "  $$\n",
    "  and send it to the two nodes connected to the edge $x_k$.<br/>\n",
    "  > 5. Iterate 2–4 until convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Bethe Free Energy and Belief Propagation\n",
    "\n",
    "- We showed that, under mean field assumptions, the FE can be decomposed into a sum of local FE contributions for the nodes ($a$) and edges ($i$):\n",
    "$$\n",
    "F[q] = \\sum_{a=1}^M \\underbrace{\\sum_{x_a} \\left( \\prod_{j\\in N(a)} q_j(x_j)\\cdot \\left(-\\log p_a(x_a)\\right) \\right) }_{\\text{node energy }U[p_a]} - \\sum_{i=1}^N \\underbrace{\\sum_{x_i} q_i(x_i) \\log \\frac{1}{q_i(x_i)}}_{\\text{edge entropy }H[q_i]}\n",
    "$$\n",
    "\n",
    "- The mean field assumption is very strong and may lead to large inference costs ($\\mathrm{KL}(q(x),p(x|\\text{data}))$). A more relaxed assumption is to allow joint posterior beliefs over the variables that attach to a node. This idea is expressed by the Bethe Free Energy:\n",
    "$$\n",
    "F_B[q] = \\sum_{a=1}^M \\left( \\sum_{x_a} q_a(x_a) \\log \\frac{q_a(x_a)}{p_a(x_a)} \\right)  - \\sum_{i=1}^N (d_i - 1) \\sum_{x_i} q_i(x_i) \\log {q_i(x_i)}\n",
    "$$\n",
    "where $q_a(x_a)$ is the posterior joint belief over the variables $x_a$ (i.e., the set of variables that attach to node $a$), $q_i(x_i)$ is the posterior marginal belief over the variable $x_i$ and $d_i$ is the number of factor nodes that link to edge $i$. Moreover, $q_a(x_a)$ and $q_i(x_i)$ are constrained to obey the following equalities:\n",
    "$$\n",
    "  \\sum_{x_a \\backslash x_i} q_a(x_a) = q_i(x_i), ~~~ \\forall i, \\forall a \\\\\n",
    "  \\sum_{x_i} q_i(x_i) = 1, ~~~ \\forall i \\\\\n",
    "  \\sum_{x_a} q_a(x_a) = 1, ~~~ \\forall a \\\\\n",
    "$$\n",
    "\n",
    "- We form the Lagrangian by augmenting the Bethe Free Energy functional with the constraints:\n",
    "$$\n",
    "L[q] = F_B[q] + \\sum_i\\sum_{a \\in N(i)} \\lambda_{ai}(x_i) \\left(q_i(x_i) - \\sum_{x_a\\backslash x_i} q(x_a) \\right) + \\sum_{i} \\gamma_i \\left(  \\sum_{x_i}q_i(x_i) - 1\\right) + \\sum_{a}\\gamma_a \\left(  \\sum_{x_a}q_a(x_a) -1\\right)\n",
    "$$\n",
    "\n",
    "- The stationary solutions for this Lagrangian are given by\n",
    "$$\n",
    "q_a(x_a) = f_a(x_a) \\exp\\left(\\gamma_a -1 + \\sum_{i \\in N(a)} \\lambda_{ai}(x_i)\\right) \\\\ \n",
    "q_i(x_i) = \\exp\\left(1- \\gamma_i + \\sum_{a \\in N(i)} \\lambda_{ai}(x_i)\\right) ^{\\frac{1}{d_i - 1}}\n",
    "$$\n",
    "where $N(i)$ denotes the factor nodes that have $x_i$ in their arguments and $N(a)$ denotes the set of variables in the argument of $f_a$.\n",
    "\n",
    "- Stationary solutions are functions of Lagrange multipliers. This means that Lagrange multipliers need to be determined. Lagrange multipliers can be determined by plugging the stationary solutions back into the constraint specification and solving for the multipliers which ensure that the constraint is satisfied. The first constraint we consider is normalization, which yields the following identification:\n",
    "$$\n",
    "\\gamma_a = 1 - \\log \\Bigg(\\sum_{x_a}f_a(x_a)\\exp\\left(\\sum_{i \\in N(a)}\\lambda_{ai}(x_i)\\right)\\Bigg)\\\\\n",
    "\\gamma_i = 1 + (d_i-1) \\log\\Bigg(\\sum_{x_i}\\exp\\left( \\frac{1}{d_i-1}\\sum_{a \\in N(i)} \\lambda_{ai}(x_i)\\right)\\Bigg).\n",
    "$$\n",
    "\n",
    "- The functional form of the Lagrange multipliers that corresponds to the normalization constraint enforces us to obtain the Lagrange multipliers that correspond to the marginalization constraint. To do so we solve for \n",
    "$$ \\sum_{x_a \\backslash x_i} f_a(x_a) \\exp\\left(\\sum_{i \\in N(a)} \\lambda_{ai}(x_i)\\right) = \\exp\\left(\\sum_{a \\in N(i)} \\lambda_{ai}(x_i)\\right) ^{\\frac{1}{d_i - 1}} \\\\ \\exp\\left(\\lambda_{ai}(x_i)\\right)\\sum_{x_a \\backslash x_i} f_a(x_a) \\exp\\Bigg(\\sum_{\\substack{{j \\in N(a)} \\\\ j \\neq i}}\\lambda_{aj}(x_j)\\Bigg) = \\exp\\left(\\sum_{a \\in N(i)} \\lambda_{ai}(x_i)\\right) ^{\\frac{1}{d_i - 1}}\\\\ \\exp\\left(\\lambda_{ai}(x_i) + \\lambda_{ia}(x_i)\\right) = \\exp\\left(\\sum_{a \\in N(i)} \\lambda_{ai}(x_i)\\right) ^{\\frac{1}{d_i - 1}}\\,, $$ \n",
    "where we defined an auxilary function\n",
    "$$\n",
    "\\exp(\\lambda_{ia}(x_i)) \\triangleq \\sum_{x_a \\backslash x_i} f_a(x_a) \\exp\\Bigg(\\sum_{\\substack{{j \\in N(a)} \\\\ j \\neq i}}\\lambda_{aj}(x_j)\\Bigg) \\,.\n",
    "$$\n",
    "This definition is valid since it can be inverted by the relation\n",
    "$$\n",
    "\\lambda_{ia}(x_i) = \\frac{2-d_i}{d_i - 1}\\lambda_{ai}(x_i) + \\frac{1}{d_i -1}\\sum_{\\substack{c \\in N(i)\\\\c \\neq a}}\\lambda_{ci}(x_i)\n",
    "$$\n",
    "\n",
    "- In general it is not possible to solve for the Lagrange multipliers analytically and we resort to iteratively obtaining the solutions. This leads to the **Belief Propagation algorithm** where the exponentiated Lagrange multipliers (messages) are updated iteratively via \n",
    "$$ \\mu_{ia}^{(k+1)}(x_i) = \\sum_{x_a \\backslash x_i} f_a(x_a) \\prod_{\\substack{{j \\in N(a)} \\\\ j \\neq i}}\\mu^{(k)}_{aj}(x_j) \\\\ \\mu_{ai}^{(k)}(x_i) = \\prod_{\\substack{c \\in N(i) \\\\ c \\neq a}}\\mu^{(k)}_{ic}(x_i)\\,, $$ \n",
    "where $k$ denotes iteration number and the messages are defined as\n",
    "$$\n",
    "\\mu_{ia}(x_i) \\triangleq \\exp(\\lambda_{ia}(x_i))\\\\\n",
    "\\mu_{ai}(x_i) \\triangleq \\exp(\\lambda_{ai}(x_i))\\,.\n",
    "$$\n",
    "\n",
    "- For a more complete overview of message passing as Bethe Free Energy minimization, see [Senoz et al. (2021)](https://www.mdpi.com/1099-4300/23/7/807)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "footer.hidden-print {\n",
       "    display: none !important;\n",
       "}\n",
       "    \n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", read(f, String))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
