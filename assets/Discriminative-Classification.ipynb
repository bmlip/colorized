{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discriminative Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to discriminative classification models\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 213-220 \n",
    "    - [T. Minka (2005), Discriminative models, not discriminative training](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Minka-2005-Discriminative-models-not-discriminative-training.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Challenge: difficult class-conditional data distribitions\n",
    "\n",
    "Our task will be the same as in the preceding class on (generative) classification. But this time, the class-conditional data distributions look very non-Gaussian, yet the linear discriminative boundary looks easy enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate dataset {(x1,y1),...,(xN,yN)}\n",
    "# x is a 2-d feature vector [x_1;x_2]\n",
    "# y ∈ {false,true} is a binary class label\n",
    "# p(x|y) is multi-modal (mixture of uniform and Gaussian distributions)\n",
    "using PyPlot\n",
    "include(\"./scripts/lesson8_helpers.jl\")\n",
    "N = 200\n",
    "X, y = genDataset(N) # Generate data set, collect in matrix X and vector y\n",
    "X_c1 = X[:,findall(.!y)]'; X_c2 = X[:,findall(y)]' # Split X based on class label\n",
    "X_test = [3.75; 1.0] # Features of 'new' data point\n",
    "function plotDataSet()\n",
    "    plot(X_c1[:,1], X_c1[:,2], \"bx\", markersize=8)\n",
    "    plot(X_c2[:,1], X_c2[:,2], \"r+\", markersize=8, fillstyle=\"none\")\n",
    "    plot(X_test[1], X_test[2], \"ko\")   \n",
    "    xlabel(L\"x_1\"); ylabel(L\"x_2\"); \n",
    "    legend([L\"y=0\", L\"y=1\",L\"y=?\"], loc=2)\n",
    "    xlim([-2;10]); ylim([-4, 8])\n",
    "end\n",
    "plotDataSet();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Main Idea of Discriminative Classification \n",
    "\n",
    "- Again, a data set is given by  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$ with $x_n \\in \\mathbb{R}^D$ and $y_n \\in \\mathcal{C}_k$, with $k=1,\\ldots,K$.\n",
    "\n",
    "-  Sometimes, the precise assumptions of the (multinomial-Gaussian) generative model $$p(x_n,y_n\\in\\mathcal{C}_k|\\theta) =  \\pi_k \\cdot \\mathcal{N}(x_n|\\mu_k,\\Sigma)$$ clearly do not match the data distribution.\n",
    "\n",
    "- Here's an **IDEA**! Let's model the posterior $$p(y_n\\in\\mathcal{C}_k|x_n)$$  *directly*, without any assumptions on the class densities.\n",
    "\n",
    "- Of course, this implies also that we build direct models for the **discrimination boundaries** \n",
    "  $$\\log \\frac{p(y_n\\in\\mathcal{C}_k|x_n)}{p(y_n\\in\\mathcal{C}_j|x_n)} \\overset{!}{=} 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Logistic Regression\n",
    "\n",
    "- We will work this idea out for a 2-class problem. Assume a data set is given by  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$ with $x_n \\in \\mathbb{R}^D$ and $y_n \\in \\{0,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Model Specification\n",
    "\n",
    "- What model should we use for the posterior distribution $p(y_n \\in \\mathcal{C}_k|x_n)$?\n",
    "\n",
    "- In Bayesian Logistic Regression, we take inspiration from the generative approach, where the **logistic function** (softmax for multi-class problems) \"emerged\" as the posterior. Here, we **choose** the familiar logistic structure with linear discrimination bounderies for the posterior class probability\n",
    "$$\n",
    "p(y_n =1 \\,|\\, x_n, w) = \\sigma(w^T x_n) \\,.\n",
    "$$\n",
    "where $$\\sigma(a) = \\frac{1}{1+e^{-a}}$$ is the _logistic_ function.\n",
    "\n",
    "- <font color=\"red\"> Add a plot of the logistic function</font>\n",
    "\n",
    "- Adding the other class ($y_n=0$) leads to the following posterior class distribution:\n",
    "$$\\begin{align*}\n",
    "p(y_n \\,|\\, x_n, w) &=  \\sigma(w^T x_n)^{y_n} \\left(1 - \\sigma(w^T x_n)\\right)^{(1-y_n)} \\tag{B-4.89} \\\\\n",
    "  &= \\sigma\\left( (2y_n-1) w^T x_n\\right) \\\\\n",
    "  &= \\mathrm{Bernoulli}\\left(y_n \\,|\\, \\sigma(w^T x_n) \\right) \n",
    "\\end{align*}$$\n",
    "  - Note that for the 2nd equality, we have made use of the fact that $\\sigma(-a) = 1-\\sigma(a)$.\n",
    "  - (Each of these three models in B-4.89 are equivalent. We mention all three notational options since they all appear in the literature).  \n",
    "  \n",
    "- Note that in this model specification, we do not impose a Gaussian structure on the class features. In the discriminative approach, the parameters $w$ are **not** structured into $\\{\\mu,\\Sigma,\\pi \\}$. This provides discriminative approach with more flexibility than the generative approach. \n",
    "  \n",
    "- In *Bayesian* logistic regression, we add a **Gaussian prior on the weights**: \n",
    "$$\\begin{align*}\n",
    "p(w) = \\mathcal{N}(w \\,|\\, m_0, S_0) \\tag{B-4.140}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### <a id=\"#logistic-regression-posterior\">Inference</a>\n",
    "\n",
    "- The posterior for the weights follows by Bayes rule\n",
    "$$\\begin{align*}\n",
    "\\underbrace{p(w \\,|\\, D)}_{\\text{posterior}} \\propto  \\underbrace{\\prod_{n=1}^N \\sigma\\left( (2y_n-1) w^T x_n\\right)}_{\\text{likelihood}} \\cdot \\underbrace{\\mathcal{N}(w \\,|\\, m_0, S_0)}_{\\text{prior}} \\tag{B-4.142}\n",
    "\\end{align*}$$\n",
    "\n",
    "- In principle, Bayesian inference is done now. Unfortunately, the posterior is not Gaussian and the evidence $p(D)$ is also not analytically computable. (We will deal with this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Predictive distribution\n",
    "\n",
    "- For a new data point $x_\\bullet$, the predictive distribution for $y_\\bullet$ is given by \n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &= \\int p(y_\\bullet = 1 \\,|\\, x_\\bullet, w) \\, p(w\\,|\\, D) \\,\\mathrm{d}w \\\\\n",
    "  &= \\int \\sigma(w^T x_\\bullet) \\, p(w\\,|\\, D) \\,\\mathrm{d}w \\tag{B-4.145}\n",
    "\\end{align*}$$\n",
    "\n",
    "- After substitution of $p(w | D)$ from B-4.142, we have an integral that is not solvable in closed-form. \n",
    "\n",
    "- Many methods have been developed to approximate the integrals for the predictive distribution and evidence. Here, we present the **Laplace approximation**, which is one of the simplest methods with broad applicability to Bayesian calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Laplace Approximation\n",
    "\n",
    "- The central idea of the Laplace approximation is to approximate a (possibly unnormalized) distribution $f(z)$ by a Gaussian distribution $q(z)$. \n",
    "\n",
    "- Note that $\\log q(z)$ is a second order polynomial in $z$, so we will find the Gaussian by fitting a parabola to $\\log f(z)$. \n",
    "\n",
    "- <font color=\"red\"> Needs an image</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### estimation of mean \n",
    "\n",
    "- The mean ($z_0$) of $q(z)$ is placed on the mode of $\\log f(z)$, i.e., \n",
    "\n",
    "$$z_0 = \\arg\\max_z \\log f(z) \\tag{B-4.126}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### estimation of variance\n",
    "\n",
    "- Since the gradient $\\nabla \\left. f(z) \\right|_{z=z_0}$ vanishes at the mode, we can (Taylor) expand $\\log f(z)$ around $z=z_0$ as \n",
    "$$\n",
    "\\log f(z) \\approx \\log f(z_0) - \\frac{1}{2} (z-z_0)^T A (z-z_0) \\tag{B-4.131}\n",
    "$$\n",
    "where the [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix) $A$ is defined by\n",
    "$$\n",
    "A = - \\nabla \\nabla \\left. \\log f(z) \\right|_{z=z_0} \\tag{B-4.132}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Laplace approximation\n",
    "\n",
    "- After taking exponentials in eq. B-4.131, we obtain\n",
    "\n",
    "$$\n",
    "f(z) \\approx f(z_0) \\exp\\left( - \\frac{1}{2} (z-z_0)^T A (z-z_0)\\right) \n",
    "$$\n",
    "\n",
    "- We can now identify $q(z)$ as\n",
    "$$\n",
    "q(z) = \\mathcal{N}\\left( z\\,|\\,z_0, A^{-1}\\right) \\tag{B-4.134}\n",
    "$$\n",
    "with $z_0$ and $A$ defined by eqs. B-4.126 and B-4.132.\n",
    "\n",
    "\n",
    "- <font color=\"red\"> insert fig 4.14</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Logistic Regression with the Laplace Approximation\n",
    "\n",
    "- Let's get back to the challenge of computing the predictive class distribution (B-4.145) for Bayesian logistic regression. We first work out the Gaussian Laplace approximation $q(w)$ to the [posterior weight distribution](#logistic-regression-posterior) \n",
    "$$\\begin{align*}\n",
    "\\underbrace{p(w | D)}_{\\text{posterior}} \\propto  \\underbrace{\\prod_{n=1}^N \\sigma\\left( (2y_n-1) w^T x_n\\right)}_{\\text{likelihood}} \\cdot \\underbrace{\\mathcal{N}(w \\,|\\, m_0, S_0)}_{\\text{prior}} \\tag{B-4.142}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### A Gausian Laplace approximation to the weights posterior \n",
    "\n",
    "- It is straightforward to compute the gradient and Hessian of $\\log p(w | D)$:\n",
    "$$\\begin{align*}\n",
    "\\nabla_w \\log p(w | D) &= S_0^{-1}\\cdot \\left(m_0-w\\right) + \\sum_n (2y_n-1) (1-\\sigma_n) x_n \\\\\n",
    "\\nabla_w \\nabla_w \\log p(w | D) &= -S_0^{-1} - \\sum_n \\sigma_n (1-\\sigma_n) x_n x_n^T \\tag{B-4.143}\n",
    "\\end{align*}$$\n",
    "where we used shorthand $\\sigma_n$ for $\\sigma\\left( (2y_n-1) w^T x_n\\right)$. \n",
    "\n",
    "- We can use the gradient to find the mode $w_{\\text{MAP}}$ of $\\log p(w|D)$ and use the Hessian to get the variance of $q(w)$, leading to a <a id=\"Laplace-posterior-logistic-regression\">**Gaussian approximate weights posterior**</a>:\n",
    "\n",
    "$$\\begin{align*}\n",
    "q(w) &= \\mathcal{N}\\left(w\\,|\\, w_{\\text{MAP}}, S_N\\right) \\tag{B-4.144}\\\\\n",
    "S_N^{-1} &= S_0^{-1} + \\sum_n \\sigma_n (1-\\sigma_n) x_n x_n^T \\tag{B-4.143}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "##### Using the Laplace weights posterior to evaluate the predictive distribution \n",
    "\n",
    "- In the analytically unsolveable expressions for evidence and the predictive distribution (estimating the class of a new observation), we proceed with using the Laplace approximation to the weights posterior. For a new observation $x_\\bullet$, the class probability is now\n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &\\approx \\int p(y_\\bullet = 1 \\,|\\, x_\\bullet, w) \\cdot \\underbrace{q(w)}_{\\text{Gaussian}} \\,\\mathrm{d}w \\\\\n",
    "  &= \\int \\sigma(w^T x_\\bullet) \\cdot \\mathcal{N}\\left(w\\,|\\, w_{\\text{MAP}}, S_N\\right) \\,\\mathrm{d}w \\tag{B-4.145}\n",
    "\\end{align*}$$\n",
    "\n",
    "- This looks better but we need two more clever tricks to evaluate this expression. \n",
    "  1. First, note that $w$ only appears in inner products, so through substitution of $a:=w^T x_\\bullet$, the expression simplifies to an integral over the scalar $a$ (see Bishop for derivation):\n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &\\approx \\int \\sigma(a) \\, \\mathcal{N}\\left(a\\,|\\, \\mu_a, \\Sigma_a\\right) \\,\\mathrm{d}a \\tag{B-4.151}\\\\\n",
    "\\mu_a  &= w^T_{\\text{MAP}} x_\\bullet \\tag{B-4.149}\\\\\n",
    "\\Sigma_a &= x^T_\\bullet S_N x_\\bullet \\tag{B-4.150}\n",
    "\\end{align*}$$\n",
    "  1. Secondly, while the integral of the product of a logistic function with a Gaussian is not analytically solvable, the integral of the product of a Gaussian CDF (cumulative distribution function) with a Gaussian _does_ have a closed-form solution. Fortunately, \n",
    "$$\\Phi(\\lambda a) \\approx \\sigma(a)$$\n",
    "with the Gaussian CDF $\\Phi(x)= \\frac{1}{\\sqrt(2\\pi)}\\int_{-\\infty}^{x}e^{-t^2/2}\\mathrm{d}t$, $ \\lambda^2= \\pi / 8 $ and $\\sigma(a) = 1/(1+e^{-a})$. Thus, substituting $\\Phi(\\lambda a)$ with $ \\lambda^2= \\pi / 8 $ for $\\sigma(a)$ leads to \n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &\\approx \\int \\Phi(\\lambda a) \\, \\mathcal{N}\\left(a\\,|\\, \\mu_a, \\Sigma_a\\right) \\,\\mathrm{d}a \\\\ &= \\Phi\\left( \\frac{\\mu_a}{\\sqrt(\\lambda^{-2} +\\sigma_a^2)}\\right) \\tag{B-4.152}\n",
    "\\end{align*}$$\n",
    "\n",
    "- We now have an approximate but **closed-form expression for the predictive class distribution for a new observation** with a Bayesian logistic regression model.  \n",
    "\n",
    "- Note that, by [Eq.B-4.143](#Laplace-posterior-logistic-regression), the variance $S_N$ (and consequently $\\sigma_a^2$) for the weight vector depends on the distribution of the training set. Large uncertainty about the weights (in areas with little training data and uninformative prior variance $S_0$) takes the posterior class probability eq. B-4.152 closer to $0.5$. Does that make sense?\n",
    "\n",
    "- Apparently, the Laplace approximation leads to a closed-form solutions for Bayesian logistic regression (although admittedly, the derivation is no walk in the park). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  2. ML Estimation for Discriminative Classification\n",
    " \n",
    "- TODO TODO ### <font color=\"red\">Derive ML frmo Bayesian approach</font>\n",
    "\n",
    "-  The conditional log-likelihood for discriminative classification is \n",
    "\n",
    "     $$\n",
    "    \\mathrm{L}(\\theta) = \\log \\prod_n \\prod_k {p(\\mathcal{C}_k|x_n,\\theta)}^{y_{nk}} \n",
    "     $$\n",
    "\n",
    "     \n",
    "- Computing the gradient $\\nabla_{\\theta_k} \\mathrm{L}(\\theta)$ (NB: revised text) leads to (for proof, see next slide) \n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta_k} \\mathrm{L}(\\theta) = \\sum_n \\Big( \\underbrace{y_{nk}}_{\\text{target}} - \\underbrace{\\frac{e^{\\theta_k^T x_n}}{ \\sum_j e^{\\theta_j^T x_n}}}_{\\text{prediction}} \\Big)\\cdot x_n \n",
    "$$\n",
    "\n",
    "  \n",
    "- Compare this to the gradient for _linear_ regression:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathrm{L}(\\theta) =  \\sum_n \\left(y_n - \\theta^T x_n \\right)  x_n\n",
    "$$\n",
    "\n",
    "- In both cases\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathrm{L} =  \\sum_n \\left( \\text{target}_n - \\text{prediction}_n \\right) \\cdot \\text{input}_n \n",
    "$$\n",
    "\n",
    "- The parameter vector $\\theta$ for logistic regression can be estimated through iterative gradient-based adaptation. E.g. (with iteration index $i$),\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}^{(i+1)} =  \\hat{\\theta}^{(i)} + \\eta \\cdot \\left. \\nabla_\\theta   \\mathrm{L}(\\theta)  \\right|_{\\theta = \\hat{\\theta}^{(i)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "Let us perform ML estimation of $w$ on the data set from the introduction. To allow an offset in the discrimination boundary, we add a constant 1 to the feature vector $x$. We only have to specify the (negative) log-likelihood and the gradient w.r.t. $w$. Then, we use an off-the-shelf optimisation library to minimize the negative log-likelihood.\n",
    "\n",
    "We plot the resulting maximum likelihood discrimination boundary. For comparison we also plot the ML discrimination boundary obtained from the [code example in the generative Gaussian classifier lesson](https://nbviewer.jupyter.org/github/bertdv/BMLIP/blob/master/lessons/notebooks/classification/Generative-Classification.ipynb#code-generative-classification-example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: \"\\\" is not a unary operator",
     "output_type": "error",
     "traceback": [
      "syntax: \"\\\" is not a unary operator",
      ""
     ]
    }
   ],
   "source": [
    "using Optim # Optimization library\n",
    "\n",
    "y_1 = zeros(length(y))# class 1 indicator vector\n",
    "y_1[findall(y)] .= 1\n",
    "X_ext = vcat(X, ones(1, length(y))) # Extend X with a row of ones to allow an offset in the discrimination boundary\n",
    "\n",
    "# Implement negative log-likelihood function\n",
    "function negative_log_likelihood(θ::Vector)\n",
    "    # Return negative log-likelihood: -L(θ)\n",
    "    p_1 = 1.0 ./ (1.0 .+ exp.(-X_ext' * θ))   # P(C1|X,θ)\n",
    "    return -sum(log.( (y_1 .* p_1) + ((1 .- y_1).*(1 .- p_1))) ) # negative log-likelihood\n",
    "end\n",
    "\n",
    "# Use Optim.jl optimiser to minimize the negative log-likelihood function w.r.t. θ\n",
    "results = optimize(negative_log_likelihood, zeros(3), LBFGS())\n",
    "θ = results.minimizer\n",
    "\n",
    "# Plot the data set and ML discrimination boundary\n",
    "plotDataSet()\n",
    "p_1(x) = 1.0 ./ (1.0 .+ exp(-([x;1.]' * θ)))\n",
    "boundary(x1) = -1 ./ θ[2] * (θ[1]*x1 .+ θ[3])\n",
    "plot([-2.;10.], boundary([-2.; 10.]), \"k-\");\n",
    "# # Also fit the generative Gaussian model from lesson 7 and plot the resulting discrimination boundary for comparison\n",
    "generative_boundary = buildGenerativeDiscriminationBoundary(X, y)\n",
    "plot([-2.;10.], generative_boundary([-2;10]), \"k:\");\n",
    "legend([L\"y=0\";L\"y=1\";L\"y=?\";\"Discr. boundary\";\"Gen. boundary\"], loc=3);\n",
    "\n",
    "Given $\\hat{\\theta}$, we can classify a new input $x_\\bullet = [3.75, 1.0]^T$:\n",
    "\n",
    "x_test = [3.75;1.0]\n",
    "println(\"P(C1|x•,θ) = $(p_1(x_test))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The generative model gives a bad result because the feature distribution of one class is clearly non-Gaussian: the model does not fit the data well. \n",
    "\n",
    "- The discriminative approach does not suffer from this problem because it makes no assumptions about the feature distribition $p(x|y)$, it just estimates the conditional class distribution $p(y|x)$ directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap Classification\n",
    "\n",
    "<table>\n",
    "<tr> <td></td><td style=\"text-align:center\"><b>Generative</b></td> <td style=\"text-align:center\"><b>Discriminative</b></td> </tr> \n",
    "\n",
    "<tr> <td>1</td><td>Like <b>density estimation</b>, model joint prob.\n",
    "$$p(\\mathcal{C}_k) p(x|\\mathcal{C}_k) = \\pi_k \\mathcal{N}(\\mu_k,\\Sigma)$$</td> <td>Like (linear) <b>regression</b>, model conditional\n",
    "$$p(\\mathcal{C}_k|x,\\theta)$$</td> </tr>\n",
    "\n",
    "<tr> <td>2</td><td>Leads to <b>softmax</b> posterior class probability\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) = e^{\\theta_k^T x}/Z$$\n",
    "with <b>structured</b> $\\theta$</td> <td> <b>Choose</b> also softmax posterior class probability\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) = e^{\\theta_k^T x}/Z$$\n",
    "but now with 'free' $\\theta$</td> </tr>\n",
    "\n",
    "<tr> <td>3</td><td>For Gaussian $p(x|\\mathcal{C}_k)$ and multinomial priors,\n",
    "$$\\hat \\theta_k  = \\left[ {\\begin{array}{c}\n",
    "   { - \\frac{1}{2} \\mu_k^T \\sigma^{-1} \\mu_k  + \\log \\pi_k}  \\\\\n",
    "   {\\sigma^{-1} \\mu_k }  \\\\\n",
    "\\end{array}} \\right]$$\n",
    "<b>in one shot</b>.</td> <td>Find $\\hat\\theta_k$ through gradient-based adaptation\n",
    "$$\\nabla_{\\theta_k}\\mathrm{L}(\\theta) = \\sum_n \\Big( y_{nk} - \\frac{e^{\\theta_k^T x_n}}{\\sum_{k^\\prime} e^{\\theta_{k^\\prime}^T x_n}} \\Big)\\, x_n$$ </td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### OPTIONAL MATERIALS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Application - Classify a new input\n",
    "\n",
    "-  Discriminative model-based prediction for a new input $x_\\bullet$ is easy, namely substitute the ML estimate in the model to get\n",
    "\n",
    "$$\n",
    "p(\\mathcal{C}_k |\\, x_\\bullet,\\hat\\theta) = \\frac{ \\mathrm{exp}\\left( \\hat \\theta_k^T x_\\bullet \\right) }{ \\sum_{k^\\prime} \\mathrm{exp}\\left(\\hat \\theta_{k^\\prime}^T x_\\bullet \\right)} \n",
    "  \\propto \\mathrm{exp}\\left(\\hat \\theta_k^T x_\\bullet\\right) \n",
    "$$\n",
    "\n",
    "-  The contours of equal probability (**discriminant boundaries**) are lines (hyperplanes) in feature space given by\n",
    "$$\n",
    "\\log \\frac{{p(\\mathcal{C}_k|x,\\hat \\theta )}}{{p(\\mathcal{C}_j|x,\\hat \\theta )}} = \\left( \\hat{\\theta}_{k} - \\hat{\\theta}_j\\right) ^T x = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  2. <span style=\"color:red\">(OPTIONAL)</span> Proof of Derivative of Log-likelihood for  Discriminative Classification\n",
    "\n",
    "\n",
    "- The Log-likelihood is $\n",
    "    \\mathrm{L}(\\theta) = \\log \\prod_n \\prod_k {\\underbrace{p(\\mathcal{C}_k|x_n,\\theta)}_{p_{nk}}}^{y_{nk}} = \\sum_{n,k} y_{nk} \\log p_{nk}$\n",
    "\n",
    "     \n",
    "- Use the fact that the softmax $\\phi_k \\equiv e^{a_k} / {\\sum_j e^{a_j}}$ has analytical derivative:\n",
    "\n",
    "$$ \\begin{align*}\n",
    " \\frac{\\partial \\phi_k}{\\partial a_j} &= \\frac{(\\sum_j e^{a_j})e^{a_k}\\delta_{kj}-e^{a_j}e^{a_k}}{(\\sum_j e^{a_j})^2} = \\frac{e^{a_k}}{\\sum_j e^{a_j}}\\delta_{kj} - \\frac{e^{a_j}}{\\sum_j e^{a_j}} \\frac{e^{a_k}}{\\sum_j e^{a_j}}\\\\\n",
    "     &= \\phi_k \\cdot(\\delta_{kj}-\\phi_j)\n",
    " \\end{align*}$$\n",
    "\n",
    "<!---\n",
    "%    -  Again we try to minimize the cross-entropy ($\\sum_{nk} y_{nk} \\log \\frac{y_{nk}}{p_{nk}}$) between the data `targets' $t_{nk}$ and the model outputs $p_{nk}$.\n",
    "--->\n",
    "\n",
    " -  Take the derivative of $\\mathrm{L}(\\theta)$ (or: how to spend a hour ...)\n",
    "$$\\begin{align*} \n",
    "\\nabla_{\\theta_j} \\mathrm{L}(\\theta) &= \\sum_{n,k} \\frac{\\partial \\mathrm{L}_{nk}}{\\partial p_{nk}} \\cdot\\frac{\\partial p_{nk}}{\\partial a_{nj}}\\cdot\\frac{\\partial a_{nj}}{\\partial \\theta_j} \\\\\n",
    "  &= \\sum_{n,k} \\frac{y_{nk}}{p_{nk}} \\cdot p_{nk} (\\delta_{kj}-p_{nj}) \\cdot x_n \\\\\n",
    "  &= \\sum_n \\Big( y_{nj} (1-p_{nj}) -\\sum_{k\\neq j} y_{nk} p_{nj} \\Big) \\cdot x_n \\\\\n",
    "  &= \\sum_n \\left( y_{nj} - p_{nj} \\right)\\cdot x_n \\\\\n",
    "  &= \\sum_n \\Big( \\underbrace{y_{nj}}_{\\text{target}} - \\underbrace{\\frac{e^{\\theta_j^T x_n}}{\\sum_{j^\\prime} e^{\\theta_{j^\\prime}^T x_n}}}_{\\text{prediction}} \\Big)\\cdot x_n \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Evidence Estimation with the Laplace Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "footer.hidden-print {\n",
       "    display: none !important;\n",
       "}\n",
       "    \n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", read(f,String))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
