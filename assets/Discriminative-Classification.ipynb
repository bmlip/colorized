{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discriminative Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to discriminative classification models\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 213 - 217 (Laplace approximation)  \n",
    "    - Bishop pp. 217 - 220 (Bayesian logistic regression) \n",
    "    - [T. Minka (2005), Discriminative models, not discriminative training](https://github.com/bertdv/BMLIP/blob/master/lessons/notebooks/files/Minka-2005-Discriminative-models-not-discriminative-training.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Challenge: difficult class-conditional data distribitions\n",
    "\n",
    "Our task will be the same as in the preceding class on (generative) classification. But this time, the class-conditional data distributions look very non-Gaussian, yet the linear discriminative boundary looks easy enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate dataset {(x1,y1),...,(xN,yN)}\n",
    "# x is a 2-d feature vector [x_1;x_2]\n",
    "# y ∈ {false,true} is a binary class label\n",
    "# p(x|y) is multi-modal (mixture of uniform and Gaussian distributions)\n",
    "using PyPlot\n",
    "include(\"./scripts/lesson8_helpers.jl\")\n",
    "N = 200\n",
    "X, y = genDataset(N) # Generate data set, collect in matrix X and vector y\n",
    "X_c1 = X[:,findall(.!y)]'; X_c2 = X[:,findall(y)]' # Split X based on class label\n",
    "X_test = [3.75; 1.0] # Features of 'new' data point\n",
    "function plotDataSet()\n",
    "    plot(X_c1[:,1], X_c1[:,2], \"bx\", markersize=8)\n",
    "    plot(X_c2[:,1], X_c2[:,2], \"r+\", markersize=8, fillstyle=\"none\")\n",
    "    plot(X_test[1], X_test[2], \"ko\")   \n",
    "    xlabel(L\"x_1\"); ylabel(L\"x_2\"); \n",
    "    legend([L\"y=0\", L\"y=1\",L\"y=?\"], loc=2)\n",
    "    xlim([-2;10]); ylim([-4, 8])\n",
    "end\n",
    "plotDataSet();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Main Idea of Discriminative Classification \n",
    "\n",
    "- Again, a data set is given by  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$ with $x_n \\in \\mathbb{R}^D$ and $y_n \\in \\mathcal{C}_k$, with $k=1,\\ldots,K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  Sometimes, the precise assumptions of the (multinomial-Gaussian) generative model $$p(x_n,y_n\\in\\mathcal{C}_k|\\theta) =  \\pi_k \\cdot \\mathcal{N}(x_n|\\mu_k,\\Sigma)$$ clearly do not match the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here's an **IDEA**! Let's model the posterior $$p(y_n\\in\\mathcal{C}_k|x_n)$$  *directly*, without any assumptions on the class densities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Of course, this implies also that we build direct models for the **discrimination boundaries** \n",
    "  $$\\log \\frac{p(y_n\\in\\mathcal{C}_k|x_n)}{p(y_n\\in\\mathcal{C}_j|x_n)} \\overset{!}{=} 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Logistic Regression\n",
    "\n",
    "- We will work this idea out for a 2-class problem. Assume a data set is given by  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$ with $x_n \\in \\mathbb{R}^D$ and $y_n \\in \\{0,1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Model Specification\n",
    "\n",
    "- What model should we use for the posterior distribution $p(y_n \\in \\mathcal{C}_k|x_n)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In Bayesian Logistic Regression, we take inspiration from the generative approach, where the **logistic function** (softmax for multi-class problems) \"emerged\" as the posterior. Here, we **choose** the familiar logistic structure with linear discrimination bounderies for the posterior class probability\n",
    "$$\n",
    "p(y_n =1 \\,|\\, x_n, w) = \\sigma(w^T x_n) \\,.\n",
    "$$\n",
    "where $$\\sigma(a) = \\frac{1}{1+e^{-a}}$$ is the _logistic_ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src=\"./figures/Figure4.9.png\" width=\"500px\">\n",
    "\n",
    "- (Bishop fig.4.9). The logistic function $\\sigma(a) = 1/(1+e^{-a})$ (red), together with the scaled probit function $\\Phi(\\lambda a)$, for $\\lambda^2=\\pi/8$ (see later in [Laplace approximation](#gaussian-cdf)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Adding the other class ($y_n=0$) leads to the following posterior class distribution:\n",
    "$$\\begin{align*}\n",
    "p(y_n \\,|\\, x_n, w) &=  \\sigma(w^T x_n)^{y_n} \\left(1 - \\sigma(w^T x_n)\\right)^{(1-y_n)} \\tag{B-4.89} \\\\\n",
    "  &= \\sigma\\left( (2y_n-1) w^T x_n\\right) \\\\\n",
    "  &= \\mathrm{Bernoulli}\\left(y_n \\,|\\, \\sigma(w^T x_n) \\right) \n",
    "\\end{align*}$$\n",
    "  - Note that for the 2nd equality, we have made use of the fact that $\\sigma(-a) = 1-\\sigma(a)$.\n",
    "  - (Each of these three models in B-4.89 are **equivalent**. We mention all three notational options since they all appear in the literature).  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "- Note that in this model specification, we do not impose a Gaussian structure on the class features. In the discriminative approach, the parameters $w$ are **not** structured into $\\{\\mu,\\Sigma,\\pi \\}$. This provides discriminative approach with more flexibility than the generative approach. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In *Bayesian* logistic regression, we add a **Gaussian prior on the weights**: \n",
    "$$\\begin{align*}\n",
    "p(w) = \\mathcal{N}(w \\,|\\, m_0, S_0) \\tag{B-4.140}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### <a id=\"#logistic-regression-posterior\">Inference</a>\n",
    "\n",
    "- The posterior for the weights follows by Bayes rule\n",
    "$$\\begin{align*}\n",
    "\\underbrace{p(w \\,|\\, D)}_{\\text{posterior}} \\propto  \\underbrace{\\mathcal{N}(w \\,|\\, m_0, S_0)}_{\\text{prior}} \\cdot \\underbrace{\\prod_{n=1}^N \\sigma\\left( (2y_n-1) w^T x_n\\right)}_{\\text{likelihood}} \\tag{B-4.142}\n",
    "\\end{align*}$$\n",
    "\n",
    "- In principle, Bayesian inference is done now. Unfortunately, the posterior is not Gaussian and the evidence $p(D)$ is also not analytically computable. (We will deal with this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Predictive distribution\n",
    "\n",
    "- For a new data point $x_\\bullet$, the predictive distribution for $y_\\bullet$ is given by \n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &= \\int p(y_\\bullet = 1 \\,|\\, x_\\bullet, w) \\, p(w\\,|\\, D) \\,\\mathrm{d}w \\\\\n",
    "  &= \\int \\sigma(w^T x_\\bullet) \\, p(w\\,|\\, D) \\,\\mathrm{d}w \\tag{B-4.145}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- After substitution of $p(w | D)$ from B-4.142, we have an integral that is not solvable in closed-form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Many methods have been developed to approximate the integrals for the predictive distribution and evidence. Here, we present the **Laplace approximation**, which is one of the simplest methods with broad applicability to Bayesian calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Laplace Approximation\n",
    "\n",
    "- The central idea of the Laplace approximation is to approximate a (possibly unnormalized) distribution $f(z)$ by a Gaussian distribution $q(z)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that $\\log q(z)$ is a second-order polynomial in $z$, so we will find the Gaussian by fitting a parabola to $\\log f(z)$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### estimation of mean \n",
    "\n",
    "- The mean ($z_0$) of $q(z)$ is placed on the mode of $\\log f(z)$, i.e., \n",
    "\n",
    "$$z_0 = \\arg\\max_z \\log f(z) \\tag{B-4.126}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### estimation of variance\n",
    "\n",
    "- Since the gradient $\\nabla \\left. f(z) \\right|_{z=z_0}$ vanishes at the mode, we can (Taylor) expand $\\log f(z)$ around $z=z_0$ as \n",
    "$$\n",
    "\\log f(z) \\approx \\log f(z_0) - \\frac{1}{2} (z-z_0)^T A (z-z_0) \\tag{B-4.131}\n",
    "$$\n",
    "where the [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix) $A$ is defined by\n",
    "$$\n",
    "A = - \\nabla \\nabla \\left. \\log f(z) \\right|_{z=z_0} \\tag{B-4.132}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Laplace approximation\n",
    "\n",
    "- After taking exponentials in eq. B-4.131, we obtain\n",
    "\n",
    "$$\n",
    "f(z) \\approx f(z_0) \\exp\\left( - \\frac{1}{2} (z-z_0)^T A (z-z_0)\\right) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can now identify $q(z)$ as\n",
    "$$\n",
    "q(z) = \\mathcal{N}\\left( z\\,|\\,z_0, A^{-1}\\right) \\tag{B-4.134}\n",
    "$$\n",
    "with $z_0$ and $A$ defined by eqs. B-4.126 and B-4.132."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Example \n",
    "\n",
    "<img src=\"./figures/Figure4.14.png\" width=\"600px\">\n",
    "\n",
    "- (Bishop fig.4.14). Laplace approximation to the distribution $p(z)\\propto \\exp(-z^2/2)\\sigma(20z+4)$, where $\\sigma(z)=1/(1+e^{-z})$. Left plot shows Laplace approximation centered on the mode of $p(z)$ and right plot shows negative logarithms of corresponding curves.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Logistic Regression with the Laplace Approximation\n",
    "\n",
    "- Let's get back to the challenge of computing the predictive class distribution (B-4.145) for Bayesian logistic regression. We first work out the Gaussian Laplace approximation $q(w)$ to the [posterior weight distribution](#logistic-regression-posterior) \n",
    "$$\\begin{align*}\n",
    "\\underbrace{p(w | D)}_{\\text{posterior}} \\propto  \\underbrace{\\mathcal{N}(w \\,|\\, m_0, S_0)}_{\\text{prior}} \\cdot \\underbrace{\\prod_{n=1}^N \\sigma\\left( (2y_n-1) w^T x_n\\right)}_{\\text{likelihood}}  \\tag{B-4.142}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### A Gausian Laplace approximation to the weights posterior \n",
    "\n",
    "- It is straightforward to compute the gradient and Hessian of $\\log p(w | D)$:\n",
    "$$\\begin{align*}\n",
    "\\nabla_w \\log p(w | D) &= S_0^{-1}\\cdot \\left(m_0-w\\right) + \\sum_n (2y_n-1) (1-\\sigma_n) x_n \\\\\n",
    "\\nabla_w \\nabla_w \\log p(w | D) &= -S_0^{-1} - \\sum_n \\sigma_n (1-\\sigma_n) x_n x_n^T \\tag{B-4.143}\n",
    "\\end{align*}$$\n",
    "where we used shorthand $\\sigma_n$ for $\\sigma\\left( (2y_n-1) w^T x_n\\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can use the gradient to find the mode $w_{\\text{MAP}}$ of $\\log p(w|D)$ and use the Hessian to get the variance of $q(w)$, leading to a <a id=\"Laplace-posterior-logistic-regression\">**Gaussian approximate weights posterior**</a>:\n",
    "\n",
    "$$\\begin{align*}\n",
    "q(w) &= \\mathcal{N}\\left(w\\,|\\, w_{\\text{MAP}}, S_N\\right) \\tag{B-4.144}\\\\\n",
    "S_N^{-1} &= S_0^{-1} + \\sum_n \\sigma_n (1-\\sigma_n) x_n x_n^T \\tag{B-4.143}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Using the Laplace weights posterior to evaluate the predictive distribution \n",
    "\n",
    "- In the analytically unsolveable expressions for evidence and the predictive distribution (estimating the class of a new observation), we proceed with using the Laplace approximation to the weights posterior. For a new observation $x_\\bullet$, the class probability is now\n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &\\approx \\int p(y_\\bullet = 1 \\,|\\, x_\\bullet, w) \\cdot \\underbrace{q(w)}_{\\text{Gaussian}} \\,\\mathrm{d}w \\\\\n",
    "  &= \\int \\sigma(w^T x_\\bullet) \\cdot \\mathcal{N}\\left(w\\,|\\, w_{\\text{MAP}}, S_N\\right) \\,\\mathrm{d}w \\tag{B-4.145}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- This looks better but we need two more clever tricks to evaluate this expression. \n",
    "  1. First, note that $w$ only appears in inner products, so through substitution of $a:=w^T x_\\bullet$, the expression simplifies to an integral over the scalar $a$ (see Bishop for derivation):\n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &\\approx \\int \\sigma(a) \\, \\mathcal{N}\\left(a\\,|\\, \\mu_a, \\Sigma_a\\right) \\,\\mathrm{d}a \\tag{B-4.151}\\\\\n",
    "\\mu_a  &= w^T_{\\text{MAP}} x_\\bullet \\tag{B-4.149}\\\\\n",
    "\\Sigma_a &= x^T_\\bullet S_N x_\\bullet \\tag{B-4.150}\n",
    "\\end{align*}$$\n",
    "  1. Secondly, while the integral of the product of a logistic function with a Gaussian is not analytically solvable, the integral of the product of a Gaussian CDF (cumulative distribution function) with a Gaussian _does_ have a closed-form solution. Fortunately, \n",
    "$$\\Phi(\\lambda a) \\approx \\sigma(a)$$\n",
    "with the <a id=\"gaussian-cdf\">Gaussian</a> CDF $\\Phi(x)= \\frac{1}{\\sqrt(2\\pi)}\\int_{-\\infty}^{x}e^{-t^2/2}\\mathrm{d}t$, $ \\lambda^2= \\pi / 8 $ and $\\sigma(a) = 1/(1+e^{-a})$. Thus, substituting $\\Phi(\\lambda a)$ with $ \\lambda^2= \\pi / 8 $ for $\\sigma(a)$ leads to \n",
    "\n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &\\approx \\int \\Phi(\\lambda a) \\, \\mathcal{N}\\left(a\\,|\\, \\mu_a, \\Sigma_a\\right) \\,\\mathrm{d}a \\\\ &= \\Phi\\left( \\frac{\\mu_a}{\\sqrt(\\lambda^{-2} +\\sigma_a^2)}\\right) \\tag{B-4.152}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We now have an approximate but **closed-form expression for the predictive class distribution for a new observation** with a Bayesian logistic regression model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that, by [Eq.B-4.143](#Laplace-posterior-logistic-regression), the variance $S_N$ (and consequently $\\sigma_a^2$) for the weight vector depends on the distribution of the training set. Large uncertainty about the weights (in areas with little training data and uninformative prior variance $S_0$) takes the posterior class probability eq. B-4.152 closer to $0.5$. Does that make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Apparently, the Laplace approximation leads to a closed-form solutions for Bayesian logistic regression (although admittedly, the derivation is no walk in the park). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "Let us perform ML estimation of $w$ on the data set from the introduction. To allow an offset in the discrimination boundary, we add a constant 1 to the feature vector $x$. We only have to specify the (negative) log-likelihood and the gradient w.r.t. $w$. Then, we use an off-the-shelf optimisation library to minimize the negative log-likelihood.\n",
    "\n",
    "We plot the resulting maximum likelihood discrimination boundary. For comparison we also plot the ML discrimination boundary obtained from the [code example in the generative Gaussian classifier lesson](https://nbviewer.jupyter.org/github/bertdv/BMLIP/blob/master/lessons/notebooks/classification/Generative-Classification.ipynb#code-generative-classification-example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: \"\\\" is not a unary operator",
     "output_type": "error",
     "traceback": [
      "syntax: \"\\\" is not a unary operator",
      ""
     ]
    }
   ],
   "source": [
    "using Optim # Optimization library\n",
    "\n",
    "y_1 = zeros(length(y))# class 1 indicator vector\n",
    "y_1[findall(y)] .= 1\n",
    "X_ext = vcat(X, ones(1, length(y))) # Extend X with a row of ones to allow an offset in the discrimination boundary\n",
    "\n",
    "# Implement negative log-likelihood function\n",
    "function negative_log_likelihood(θ::Vector)\n",
    "    # Return negative log-likelihood: -L(θ)\n",
    "    p_1 = 1.0 ./ (1.0 .+ exp.(-X_ext' * θ))   # P(C1|X,θ)\n",
    "    return -sum(log.( (y_1 .* p_1) + ((1 .- y_1).*(1 .- p_1))) ) # negative log-likelihood\n",
    "end\n",
    "\n",
    "# Use Optim.jl optimiser to minimize the negative log-likelihood function w.r.t. θ\n",
    "results = optimize(negative_log_likelihood, zeros(3), LBFGS())\n",
    "θ = results.minimizer\n",
    "\n",
    "# Plot the data set and ML discrimination boundary\n",
    "plotDataSet()\n",
    "p_1(x) = 1.0 ./ (1.0 .+ exp(-([x;1.]' * θ)))\n",
    "boundary(x1) = -1 ./ θ[2] * (θ[1]*x1 .+ θ[3])\n",
    "plot([-2.;10.], boundary([-2.; 10.]), \"k-\");\n",
    "# # Also fit the generative Gaussian model from lesson 7 and plot the resulting discrimination boundary for comparison\n",
    "generative_boundary = buildGenerativeDiscriminationBoundary(X, y)\n",
    "plot([-2.;10.], generative_boundary([-2;10]), \"k:\");\n",
    "legend([L\"y=0\";L\"y=1\";L\"y=?\";\"Discr. boundary\";\"Gen. boundary\"], loc=3);\n",
    "\n",
    "Given $\\hat{\\theta}$, we can classify a new input $x_\\bullet = [3.75, 1.0]^T$:\n",
    "\n",
    "x_test = [3.75;1.0]\n",
    "println(\"P(C1|x•,θ) = $(p_1(x_test))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The generative model gives a bad result because the feature distribution of one class is clearly non-Gaussian: the model does not fit the data well. \n",
    "\n",
    "- The discriminative approach does not suffer from this problem because it makes no assumptions about the feature distribition $p(x|y)$, it just estimates the conditional class distribution $p(y|x)$ directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap Classification\n",
    "\n",
    "<table>\n",
    "<tr> <td></td><td style=\"text-align:center\"><b>Generative</b></td> <td style=\"text-align:center\"><b>Discriminative</b></td> </tr> \n",
    "\n",
    "<tr> <td>1</td><td>Like <b>density estimation</b>, model joint prob.\n",
    "$$p(\\mathcal{C}_k) p(x|\\mathcal{C}_k) = \\pi_k \\mathcal{N}(\\mu_k,\\Sigma)$$</td> <td>Like (linear) <b>regression</b>, model conditional\n",
    "$$p(\\mathcal{C}_k|x,\\theta)$$</td> </tr>\n",
    "\n",
    "<tr> <td>2</td><td>Leads to <b>softmax</b> posterior class probability\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) = e^{\\theta_k^T x}/Z$$\n",
    "with <b>structured</b> $\\theta$</td> <td> <b>Choose</b> also softmax posterior class probability\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) = e^{\\theta_k^T x}/Z$$\n",
    "but now with 'free' $\\theta$</td> </tr>\n",
    "\n",
    "<tr> <td>3</td><td>For Gaussian $p(x|\\mathcal{C}_k)$ and multinomial priors,\n",
    "$$\\hat \\theta_k  = \\left[ {\\begin{array}{c}\n",
    "   { - \\frac{1}{2} \\mu_k^T \\sigma^{-1} \\mu_k  + \\log \\pi_k}  \\\\\n",
    "   {\\sigma^{-1} \\mu_k }  \\\\\n",
    "\\end{array}} \\right]$$\n",
    "<b>in one shot</b>.</td> <td>Find $\\hat\\theta_k$ through gradient-based adaptation\n",
    "$$\\nabla_{\\theta_k}\\mathrm{L}(\\theta) = \\sum_n \\Big( y_{nk} - \\frac{e^{\\theta_k^T x_n}}{\\sum_{k^\\prime} e^{\\theta_{k^\\prime}^T x_n}} \\Big)\\, x_n$$ </td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> OPTIONAL SLIDES </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  2. ML Estimation for Discriminative Classification\n",
    " \n",
    "- TODO TODO ### <font color=\"red\">Derive ML frmo Bayesian approach</font>\n",
    "\n",
    "-  The conditional log-likelihood for discriminative classification is \n",
    "\n",
    "     $$\n",
    "    \\mathrm{L}(\\theta) = \\log \\prod_n \\prod_k {p(\\mathcal{C}_k|x_n,\\theta)}^{y_{nk}} \n",
    "     $$\n",
    "\n",
    "     \n",
    "- Computing the gradient $\\nabla_{\\theta_k} \\mathrm{L}(\\theta)$ leads to (for proof, see next slide) \n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta_k} \\mathrm{L}(\\theta) = \\sum_n \\Big( \\underbrace{y_{nk}}_{\\text{target}} - \\underbrace{\\frac{e^{\\theta_k^T x_n}}{ \\sum_j e^{\\theta_j^T x_n}}}_{\\text{prediction}} \\Big)\\cdot x_n \n",
    "$$\n",
    "\n",
    "  \n",
    "- Compare this to the gradient for _linear_ regression:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathrm{L}(\\theta) =  \\sum_n \\left(y_n - \\theta^T x_n \\right)  x_n\n",
    "$$\n",
    "\n",
    "- In both cases\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathrm{L} =  \\sum_n \\left( \\text{target}_n - \\text{prediction}_n \\right) \\cdot \\text{input}_n \n",
    "$$\n",
    "\n",
    "- The parameter vector $\\theta$ for logistic regression can be estimated through iterative gradient-based adaptation. E.g. (with iteration index $i$),\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}^{(i+1)} =  \\hat{\\theta}^{(i)} + \\eta \\cdot \\left. \\nabla_\\theta   \\mathrm{L}(\\theta)  \\right|_{\\theta = \\hat{\\theta}^{(i)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Proof of Derivative of Log-likelihood for  Discriminative Classification\n",
    "\n",
    "\n",
    "- The Log-likelihood is $\n",
    "    \\mathrm{L}(\\theta) = \\log \\prod_n \\prod_k {\\underbrace{p(\\mathcal{C}_k|x_n,\\theta)}_{p_{nk}}}^{y_{nk}} = \\sum_{n,k} y_{nk} \\log p_{nk}$\n",
    "\n",
    "     \n",
    "- Use the fact that the softmax $\\phi_k \\equiv e^{a_k} / {\\sum_j e^{a_j}}$ has analytical derivative:\n",
    "\n",
    "$$ \\begin{align*}\n",
    " \\frac{\\partial \\phi_k}{\\partial a_j} &= \\frac{(\\sum_j e^{a_j})e^{a_k}\\delta_{kj}-e^{a_j}e^{a_k}}{(\\sum_j e^{a_j})^2} = \\frac{e^{a_k}}{\\sum_j e^{a_j}}\\delta_{kj} - \\frac{e^{a_j}}{\\sum_j e^{a_j}} \\frac{e^{a_k}}{\\sum_j e^{a_j}}\\\\\n",
    "     &= \\phi_k \\cdot(\\delta_{kj}-\\phi_j)\n",
    " \\end{align*}$$\n",
    "\n",
    "<!---\n",
    "%    -  Again we try to minimize the cross-entropy ($\\sum_{nk} y_{nk} \\log \\frac{y_{nk}}{p_{nk}}$) between the data `targets' $t_{nk}$ and the model outputs $p_{nk}$.\n",
    "--->\n",
    "\n",
    " -  Take the derivative of $\\mathrm{L}(\\theta)$ (or: how to spend a hour ...)\n",
    "$$\\begin{align*} \n",
    "\\nabla_{\\theta_j} \\mathrm{L}(\\theta) &= \\sum_{n,k} \\frac{\\partial \\mathrm{L}_{nk}}{\\partial p_{nk}} \\cdot\\frac{\\partial p_{nk}}{\\partial a_{nj}}\\cdot\\frac{\\partial a_{nj}}{\\partial \\theta_j} \\\\\n",
    "  &= \\sum_{n,k} \\frac{y_{nk}}{p_{nk}} \\cdot p_{nk} (\\delta_{kj}-p_{nj}) \\cdot x_n \\\\\n",
    "  &= \\sum_n \\Big( y_{nj} (1-p_{nj}) -\\sum_{k\\neq j} y_{nk} p_{nj} \\Big) \\cdot x_n \\\\\n",
    "  &= \\sum_n \\left( y_{nj} - p_{nj} \\right)\\cdot x_n \\\\\n",
    "  &= \\sum_n \\Big( \\underbrace{y_{nj}}_{\\text{target}} - \\underbrace{\\frac{e^{\\theta_j^T x_n}}{\\sum_{j^\\prime} e^{\\theta_{j^\\prime}^T x_n}}}_{\\text{prediction}} \\Big)\\cdot x_n \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "footer.hidden-print {\n",
       "    display: none !important;\n",
       "}\n",
       "    \n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", read(f,String))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
