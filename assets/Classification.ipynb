{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to linear generative classification with multinomial-Gaussian generative model\n",
    "  \n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 196-202     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example Problem: an apple or a peach?\n",
    "\n",
    "<span class=\"exercise\">You're given numerical values for the skin features roughness and color for 200 pieces of fruit, where for each piece of fruit you also know if it is an apple or a peach. Now you receive the roughness and color values for a new piece of fruit but you don't get its class label (apple or peach). What is the probability that the new piece is an apple?</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package Distributions not found in current path:\n- Run `import Pkg; Pkg.add(\"Distributions\")` to install the Distributions package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package Distributions not found in current path:\n- Run `import Pkg; Pkg.add(\"Distributions\")` to install the Distributions package.\n",
      "",
      "Stacktrace:",
      " [1] require(::Module, ::Symbol) at ./loading.jl:876",
      " [2] top-level scope at In[5]:1"
     ]
    }
   ],
   "source": [
    "using Distributions, PyPlot\n",
    "N = 250; p_apple = 0.7; Σ = [0.2 0.1; 0.1 0.3]\n",
    "p_given_apple = MvNormal([1.0, 1.0], Σ) # p(X|y=apple)\n",
    "p_given_peach = MvNormal([1.7, 2.5], Σ) # p(X|y=peach)\n",
    "X = Matrix{Float64}(undef,2,N); y = Vector{Bool}(undef,N) # true corresponds to apple\n",
    "for n=1:N\n",
    "    y[n] = (rand() < p_apple) # Apple or peach?\n",
    "    X[:,n] = y[n] ? rand(p_given_apple) : rand(p_given_peach) # Sample features\n",
    "end\n",
    "X_apples = X[:,findall(y)]'; X_peaches = X[:,findall(.!y)]' # Sort features on class\n",
    "x_test = [2.3; 1.5] # Features of 'new' data point\n",
    "\n",
    "function plot_fruit_dataset()\n",
    "    # Plot the data set and x_test\n",
    "    plot(X_apples[:,1], X_apples[:,2], \"r+\")   # apples\n",
    "    plot(X_peaches[:,1], X_peaches[:,2], \"bx\") # peaches\n",
    "    plot(x_test[1], x_test[2], \"ko\")           # 'new' unlabelled data point\n",
    "    legend([\"Apples\"; \"Peaches\"; \"Apple or peach?\"], loc=2)\n",
    "    xlabel(L\"x_1\"); ylabel(L\"x_2\"); xlim([-1,3]); ylim([-1,4])\n",
    "end\n",
    "plot_fruit_dataset();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generative Classification Problem Statement\n",
    "\n",
    "- Given is a data set  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$\n",
    "  - inputs $x_n \\in \\mathbb{R}^D$ are called **features**.\n",
    "  - outputs $y_n \\in \\mathcal{C}_k$, with $k=1,\\ldots,K$; The **discrete** targets $\\mathcal{C}_k$ are called **classes**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We will again use the 1-of-$K$ notation for the discrete classes. Define the binary **class selection variable**\n",
    "$$\n",
    "y_{nk} = \\begin{cases} 1 & \\text{if  } \\, y_n \\in \\mathcal{C}_k\\\\\n",
    "0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "  - (Hence, the notations $y_{nk}=1$ and $y_n \\in \\mathcal{C}_k$ mean the same thing.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  The plan for generative classification: build a model for the joint pdf $p(x,y)= p(x|y)p(y)$ and use Bayes to infer the posterior class probabilities \n",
    "\n",
    "$$\n",
    "p(y|x) = \\frac{p(x|y) p(y)}{\\sum_{y} p(x|y) p(y)} \\propto p(x|y)\\,p(y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  1 - Model specification \n",
    "\n",
    "##### Likelihood\n",
    "\n",
    "- Assume Gaussian **class-conditional distributions** with **constant covariance matrix** across the classes,\n",
    " $$\n",
    " p(x_n|\\mathcal{C}_{k}) = \\mathcal{N}(x_n|\\mu_k,\\Sigma)\n",
    " $$\n",
    "with notational shorthand: $\\mathcal{C}_{k} \\triangleq (y_n \\in \\mathcal{C}_{k})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Prior\n",
    "\n",
    "- We use a categorical distribution for the class labels $y_{nk}$: \n",
    "$$p(\\mathcal{C}_{k}) = \\pi_k$$\n",
    "\n",
    "- This leads to\n",
    "$$\n",
    " p(x_n,\\mathcal{C}_{k}) =  \\pi_k \\cdot \\mathcal{N}(x_n|\\mu_k,\\Sigma)\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The log-likelihood for the full data set is then\n",
    "$$\\begin{align*}\n",
    "\\log\\, &p(D|\\theta) \\stackrel{\\text{IID}}{=} \\sum_n \\log p(x_n,\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{K} \\,|\\,\\theta) \\\\\n",
    "  &= \\sum_n \\log \\prod_k p(x_n,\\mathcal{C}_{k}|\\theta)^{y_{nk}} \\;\\;\\text{(use 1-of-K coding)} \\\\\n",
    "  &=  \\sum_{n,k} y_{nk} \\log p(x_n,\\mathcal{C}_{k}|\\theta) \\\\\n",
    "   &=  \\sum_{n,k} y_{nk}  \\log\\mathcal{N}(x_n|\\mu_k,\\Sigma)  +  \\sum_{n,k} y_{nk} \\log \\pi_k \\\\\n",
    "   &=  \\sum_{n,k} y_{nk} \\underbrace{ \\log\\mathcal{N}(x_n|\\mu_k,\\Sigma) }_{ \\text{see Gaussian est.} } + \\underbrace{ \\sum_k m_k \\log \\pi_k }_{ \\text{see multinomial est.} } \n",
    "\\end{align*}$$\n",
    "where we used $m_k \\triangleq \\sum_n y_{nk}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As usual, the rest (inference for parameters and model prediction) through straight probability theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2 -  Parameter Inference for Classification\n",
    "\n",
    "- We'll do ML estimation for $\\theta = \\{ \\pi_k, \\mu_k, \\Sigma \\}$ from data $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  Recall (from the previous slide) the log-likelihood (LLH)\n",
    "\n",
    "$$\n",
    "\\log\\, p(D|\\theta) =  \\sum_{n,k} y_{nk} \\underbrace{ \\log\\mathcal{N}(x_n|\\mu_k,\\Sigma) }_{ \\text{Gaussian} } + \\underbrace{ \\sum_k m_k \\log \\pi_k }_{ \\text{multinomial} } \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Maximization of the LLH breaks down into\n",
    "  -  **Gaussian density estimation** for parameters $\\mu_k, \\Sigma$, since the first term contains exactly the LLH for MVG density estimation (see lesson on Density Est., Eq.1) \n",
    "  - **Multinomial density estimation** for class priors $\\pi_k$, since the second term holds exactly the LLH for multinomial density estimation (see lesson on Density Estimation, Eq.2). \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " - The ML for multinomial class prior (we've done this before!)\n",
    "$$\\begin{align*}   \n",
    "\\hat \\pi_k = m_k/N \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now group the data into separate classes and do MVG ML estimation for class-conditional parameters (we've done this before as well):\n",
    "$$\\begin{align*}\n",
    " \\hat \\mu_k &= \\frac{ \\sum_n y_{nk} x_n} { \\sum_n y_{nk} } = \\frac{1}{m_k} \\sum_n y_{nk} x_n \\\\\n",
    " \\hat \\Sigma  &= \\frac{1}{N} \\sum_{n,k} y_{nk} (x_n-\\hat \\mu_k)(x_n-\\hat \\mu_k)^T \\\\\n",
    "  &= \\sum_k \\hat \\pi_k \\cdot \\underbrace{ \\left( \\frac{1}{m_k} \\sum_{n} y_{nk} (x_n-\\hat \\mu_k)(x_n-\\hat \\mu_k)^T  \\right) }_{ \\text{class-cond. variance} } \\\\\n",
    "  &= \\sum_k \\hat \\pi_k \\cdot \\hat \\Sigma_k\n",
    "\\end{align*}$$\n",
    "where $\\hat \\pi_k$, $\\hat{\\mu}_k$ and $\\hat{\\Sigma}_k$ are the sample proportion, sample mean and sample variance for the $k$th class, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Note that the binary class selection variable $y_{nk}$ groups data from the same class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  3 - Application: Class prediction for new Data\n",
    "\n",
    "-  Let's apply the trained model: given a 'new' input $x_\\bullet$, use Bayes rule to get posterior class probability\n",
    "$$\\begin{align*}\n",
    " p(\\mathcal{C}_k|x_\\bullet,\\hat{\\theta} ) &\\propto p(\\mathcal{C}_k) \\,p(x_\\bullet|\\mathcal{C}_k) \\\\\n",
    "  &\\propto \\hat{\\pi}_k \\exp \\left\\{ { - {\\frac{1}{2}}(x_\\bullet - \\hat{\\mu}_k )^T \\hat{\\Sigma}^{ - 1} (x_\\bullet - \\hat{\\mu}_k )} \\right\\}\\\\\n",
    "  &\\propto \\exp \\left\\{ {\\hat{\\mu}_k^T \\hat{\\Sigma}^{-1} x_\\bullet - {\\frac{1}{2}}\\hat{\\mu}_k^T \\hat{\\Sigma}^{ - 1} \\hat{\\mu}_k  + \\log \\hat{\\pi}_k } \\right\\}  \\\\\n",
    "  &=  \\exp\\{\\beta_k^T x + \\gamma_k\\}\n",
    "\\end{align*}$$\n",
    "where \n",
    "$$\\begin{align*}\n",
    "\\beta_k &= \\hat{\\Sigma}^{-1} \\hat{\\mu}_k \\\\\n",
    "\\gamma_k &= - \\frac{1}{2} \\hat{\\mu}_k^T \\hat{\\Sigma}^{-1} \\hat{\\mu}_k  + \\log \\hat{\\pi}_k \\,.\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The class posterior function $$\\phi(a_k) \\triangleq \\frac{\\exp(a_k)}{\\sum_{k^\\prime}\\exp(a_{k^\\prime})}$$ is called a **softmax** function. Note that the softmax function is per definition properly normalized in the sense that $\\sum_k \\phi(a_k) = 1$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Discrimination Boundaries\n",
    "\n",
    "-  The class log-posterior $\\log p(\\mathcal{C}_k|x) \\propto \\beta_k^T x + \\gamma_k$ is a linear function of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  Thus, the contours of equal probability (**discriminant functions**) are lines (hyperplanes) in feature space\"\n",
    "$$\n",
    "\\log \\frac{{p(\\mathcal{C}_k|x,\\theta )}}{{p(\\mathcal{C}_j|x,\\theta )}} = \\beta_{kj}^T x + \\gamma_{kj} = 0\n",
    "$$\n",
    "where we defined $\\beta_{kj} \\triangleq \\beta_k - \\beta_j$ and similarly for $\\gamma_{kj}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  (homework). What happens if we had not assumed class-independent variances $\\Sigma_k=\\Sigma$? Are the discrimination functions still linear? quadratic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  How to classify a new input $x_\\bullet$? The Bayesian answer is a posterior distribution $ p(\\mathcal{C}_k|x_\\bullet)$. If you must choose, then the class with maximum posterior class probability\n",
    "$$\\begin{align*}\n",
    "k^* &= \\arg\\max_k p(\\mathcal{C}_k|x_\\bullet) \\\\\n",
    "  &= \\arg\\max_k \\left( \\beta _k^T x_\\bullet + \\gamma_k \\right)\n",
    "\\end{align*}$$\n",
    "is an appealing decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "We'll apply the above results to solve the \"apple or peach\" example problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Figure(PyObject <Figure size 640x480 with 1 Axes>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(apple|x=x∙) = 0.7842698441755891\n"
     ]
    }
   ],
   "source": [
    "# Make sure you run the data-generating code cell first\n",
    "\n",
    "# Multinomial (in this case binomial) density estimation\n",
    "p_apple_est = sum(y.==true) / length(y)\n",
    "π_hat = [p_apple_est; 1-p_apple_est]\n",
    "\n",
    "# Estimate class-conditional multivariate Gaussian densities\n",
    "d1 = fit_mle(FullNormal, X_apples')  # MLE density estimation d1 = N(μ₁, Σ₁)\n",
    "d2 = fit_mle(FullNormal, X_peaches') # MLE density estimation d2 = N(μ₂, Σ₂)\n",
    "Σ = π_hat[1]*cov(d1) + π_hat[2]*cov(d2) # Combine Σ₁ and Σ₂ into Σ\n",
    "conditionals = [MvNormal(mean(d1), Σ); MvNormal(mean(d2), Σ)] # p(x|C)\n",
    "\n",
    "# Calculate posterior class probability of x∙ (prediction)\n",
    "function predict_class(k, X) # calculate p(Ck|X)\n",
    "    norm = π_hat[1]*pdf(conditionals[1],X) + π_hat[2]*pdf(conditionals[2],X)\n",
    "    return π_hat[k]*pdf(conditionals[k], X) ./ norm\n",
    "end\n",
    "println(\"p(apple|x=x∙) = $(predict_class(1,x_test))\")\n",
    "\n",
    "# Discrimination boundary of the posterior (p(apple|x;D) = p(peach|x;D) = 0.5)\n",
    "β(k) = inv(Σ)*mean(conditionals[k])\n",
    "γ(k) = -0.5 * mean(conditionals[k])' * inv(Σ) * mean(conditionals[k]) + log(π_hat[k])\n",
    "function discriminant_x2(x1)\n",
    "    # Solve discriminant equation for x2\n",
    "    β12 = β(1) .- β(2)\n",
    "    γ12 = (γ(1) .- γ(2))[1,1]\n",
    "    return -1*(β12[1]*x1 .+ γ12) ./ β12[2]\n",
    "end\n",
    "\n",
    "plot_fruit_dataset() # Plot dataset\n",
    "x1 = range(-1,length=10,stop=3)\n",
    "plot(x1, discriminant_x2(x1), \"k-\") # Plot discrimination boundary\n",
    "fill_between(x1, -1, discriminant_x2(x1), color=\"r\", alpha=0.2)\n",
    "fill_between(x1, discriminant_x2(x1), 4, color=\"b\", alpha=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Recap Generative Classification\n",
    "\n",
    "- Model speccification:  $p(x,\\mathcal{C}_k|\\,\\theta) = \\pi_k \\cdot \\mathcal{N}(x|\\mu_k,\\Sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the class-conditional distributions are Gaussian with equal covariance matrices across classes ($\\Sigma_k = \\Sigma$), then\n",
    "    the discriminant functions are hyperplanes in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ML estimation for $\\{\\pi_k,\\mu_k,\\Sigma\\}$ breaks down to simple density estimation for Gaussian and multinomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Posterior class probability is a softmax function\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) \\propto \\exp\\{\\beta_k^T x + \\gamma_k\\}$$\n",
    "where $\\beta _k= \\Sigma^{-1} \\mu_k$ and $\\gamma_k=- \\frac{1}{2} \\mu_k^T \\Sigma^{-1} \\mu_k  + \\log \\pi_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discriminative Classification\n",
    "\n",
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to discriminative classification models\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 203-206 \n",
    "    - [T. Minka (2005), Discriminative models, not discriminative training](./files/Minka-2005 -Discriminative-models-not-discriminative-training.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayesian Logistic Regression\n",
    "\n",
    "- A data set is given by  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$ with $x_n \\in \\mathbb{R}^D$ and $y_n \\in \\{0,1\\}$.\n",
    "\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "- The likelihood model is given by\n",
    "$$\\begin{align*}\n",
    "p(y_n \\,|\\, x_n, w) &= \\mathrm{Bernoulli}\\left(y_n \\,|\\, \\sigma(w^T x_n) \\right) \\\\\n",
    "  &=  \\sigma(w^T x_n)^{y_n} \\left(1 - \\sigma(w^T x_n)\\right)^{(1-y_n)} \\tag{B-4.89} \\\\\n",
    "  &= \\sigma\\left( (2y_n-1) w^T x_n\\right)\n",
    "\\end{align*}$$\n",
    "where $$\\sigma(a) = 1/(1+e^{-a})$$ is the _logistic_ function. Note that for the 3rd line, we have made use of the fact that $\\sigma(-a) = 1-\\sigma(a)$.\n",
    "  - (Each of these three models are equivalent. We mention all three notational options since they all appear in the literature).  \n",
    "  \n",
    "- <font color=\"red\"> Add a plot of the logistic function</font>\n",
    "  \n",
    "- We will consider a Gaussian prior on the weights: \n",
    "  $$\\begin{align*}\n",
    "p(w) = \\mathcal{N}(w \\,|\\, m_0, S_0) \\tag{B-4.140}\n",
    "\\end{align*}$$\n",
    "\n",
    "### Inference\n",
    "\n",
    "- The posterior for the weights follows by Bayes rule\n",
    "$$\\begin{align*}\n",
    "p(w \\mid D) \\propto \\mathcal{N}(w \\,|\\, m_0, S_0) \\cdot \\prod_{n=1}^N \\sigma\\left( (2y_n-1) w^T x_n\\right) \\tag{B-4.142}\n",
    "\\end{align*}$$\n",
    "\n",
    "- In principle, Bayesian inference is done now. Unfortunately, the posterior is not Gaussian and the evidence $p(D)$ is also not analytically computable. \n",
    "\n",
    "### Predictive distribution\n",
    "\n",
    "- For a new data point $x_\\bullet$, the predictive distribution for $y_\\bullet$ is given by \n",
    "$$\\begin{align*}\n",
    "p(y_\\bullet = 1 \\mid x_\\bullet, D) &= \\int p(y_\\bullet = 1 \\,|\\, x_\\bullet, w) \\, p(w\\,|\\, D) \\,\\mathrm{d}w \\\\\n",
    "  &= \\int \\sigma(w^T x_\\bullet) \\, p(w\\,|\\, D) \\,\\mathrm{d}w \\tag{B-4.145}\n",
    "\\end{align*}$$\n",
    "\n",
    "- After substitution of $p(w | D)$ from B-4.142, we have an integral that is not solvable in closed-form. \n",
    "\n",
    "- Many methods have been developed to approximate the integrals for the predictive distribution and evidence. Here, we present the **Laplace approximation**, which is one of the simplest methods with broad applicability to Bayesian calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Laplace Approximation\n",
    "\n",
    "- The central idea of the Laplace approximation is to approximate a (possibly unnormalized) distribution $f(z)$ by a Gaussian distribution $q(z)$. \n",
    "\n",
    "- Note that $\\log q(z)$ is a second order polynomial in $z$, so we will find the Gaussian by fitting a parabola to $\\log f(z)$. \n",
    "\n",
    "\n",
    "##### estimation of mean \n",
    "\n",
    "- The mean ($z_0$) of q(z) is placed on the mode of $\\log f(z)$, i.e., \n",
    "\n",
    "$$z_0 = \\arg\\max_z \\log f(z) \\tag{B-4.126}$$ \n",
    "  \n",
    "##### estimation of variance\n",
    "\n",
    "- Since the gradient $\\nabla \\left. f(z) \\right|_{z=z_0}$ vanishes at the mode, we can (Taylor) expand $\\log f(z)$ around $z=z_0$ as \n",
    "$$\n",
    "\\log f(z) \\approx \\log f(z_0) - \\frac{1}{2} (z-z_0)^T A (z-z_0) \\tag{B-4.131}\n",
    "$$\n",
    "where the [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix) $A$ is defined by\n",
    "$$\n",
    "A = - \\nabla \\nabla \\left. \\log f(z) \\right|_{z=z_0} \\tag{B-4.132}\n",
    "$$\n",
    "\n",
    "##### Laplace approximation\n",
    "\n",
    "- After taking exponentials in eq. B-4.131, we obtain\n",
    "\n",
    "$$\n",
    "f(z) \\approx f(z_0) \\exp\\left( - \\frac{1}{2} (z-z_0)^T A (z-z_0)\\right) \n",
    "$$\n",
    "\n",
    "- We can now identify $q(z)$ as\n",
    "$$\n",
    "q(z) = \\mathcal{N}\\left( z\\,|\\,z_0, A^{-1}\\right) \\tag{B-4.134}\n",
    "$$\n",
    "with $z_0$ and $A$ defined by eqs. B-4.126 and B-4.132.\n",
    "- insert fig 4.14\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Evidence Estimation with the Laplace Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bayesian Logistic Regression worked out with the Laplace Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Problem: difficult class-conditional data distribitions\n",
    "\n",
    "Our task will be the same as in the preceding class on (generative) classification. But this time, the class-conditional data distributions look very non-Gaussian, yet the linear discriminative boundary looks easy enough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package PyPlot not found in current path:\n- Run `import Pkg; Pkg.add(\"PyPlot\")` to install the PyPlot package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package PyPlot not found in current path:\n- Run `import Pkg; Pkg.add(\"PyPlot\")` to install the PyPlot package.\n",
      "",
      "Stacktrace:",
      " [1] require(::Module, ::Symbol) at ./loading.jl:876",
      " [2] top-level scope at In[1]:1"
     ]
    }
   ],
   "source": [
    "# Generate dataset {(x1,y1),...,(xN,yN)}\n",
    "# x is a 2-d feature vector [x_1;x_2]\n",
    "# y ∈ {false,true} is a binary class label\n",
    "# p(x|y) is multi-modal (mixture of uniform and Gaussian distributions)\n",
    "using PyPlot\n",
    "include(\"scripts/lesson8_helpers.jl\")\n",
    "N = 200\n",
    "X, y = genDataset(N) # Generate data set, collect in matrix X and vector y\n",
    "X_c1 = X[:,findall(.!y)]'; X_c2 = X[:,findall(y)]' # Split X based on class label\n",
    "X_test = [3.75; 1.0] # Features of 'new' data point\n",
    "function plotDataSet()\n",
    "    plot(X_c1[:,1], X_c1[:,2], \"bx\", markersize=8)\n",
    "    plot(X_c2[:,1], X_c2[:,2], \"r+\", markersize=8, fillstyle=\"none\")\n",
    "    plot(X_test[1], X_test[2], \"ko\")   \n",
    "    xlabel(L\"x_1\"); ylabel(L\"x_2\"); legend([L\"y=0\", L\"y=1\",L\"y=?\"], loc=2)\n",
    "    xlim([-2;10]); ylim([-4, 8])\n",
    "end\n",
    "plotDataSet();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Main Idea of Discriminative Classification \n",
    "\n",
    "- Again, a data set is given by  $D = \\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$ with $x_n \\in \\mathbb{R}^D$ and $y_n \\in \\mathcal{C}_k$, with $k=1,\\ldots,K$.\n",
    "\n",
    "-  Sometimes, the precise assumptions of the (multinomial-Gaussian) generative model $$p(x_n,\\mathcal{C}_k|\\theta) =  \\pi_k \\cdot \\mathcal{N}(x_n|\\mu_k,\\Sigma)$$ clearly do not match the data distribution.\n",
    "\n",
    "- Here's an **IDEA**! Let's model the posterior $$p(\\mathcal{C}_k|x_n)$$  _directly_, without any assumptions on the class densities.\n",
    "\n",
    "- Of course, this implies also that we build direct models for the **discrimination boundaries** \n",
    "  $$\\log \\frac{p(\\mathcal{C}_k|x_n)}{p(\\mathcal{C}_j|x_n)} \\overset{!}{=} 0$$\n",
    "\n",
    "### 1. Model Specification \n",
    "\n",
    "- <span style=\"color:blue\">[Q.]</span> What model should we use for $p(\\mathcal{C}_k|x_n)$?\n",
    "\n",
    "-   <span style=\"color:blue\">[A.]</span> Get inspiration from the generative approach: choose the familiar softmax structure **with linear discrimination bounderies** for the posterior class probability\n",
    "$$\n",
    "p(\\mathcal{C}_k|x_n,\\theta) = \\frac{e^{\\theta_k^T x_n}}{\\sum_j e^{\\theta_j^T x_n}}\n",
    "$$\n",
    "but **do not impose a Gaussian structure on the class features**.\n",
    "\n",
    "- $\\Rightarrow$ There are **two key differences** between the discriminative and generative approach: \n",
    "  1. In the discriminative approach, the parameters $\\theta_k$ are **not** structured into $\\{\\mu_k,\\Sigma,\\pi_k \\}$. This provides discriminative approach with more flexibility.\n",
    "  2. ML learning for the discriminative approach by optimization of _conditional_ likelihood $\\prod_n p(y_n|x_n,\\theta)$ rather than _joint_ likelihood $\\prod_n p(y_n,x_n|\\theta)$.\n",
    "\n",
    " ###  2. ML Estimation for Discriminative Classification\n",
    " \n",
    "\n",
    "-  The conditional log-likelihood for discriminative classification is \n",
    "\n",
    "     $$\n",
    "    \\mathrm{L}(\\theta) = \\log \\prod_n \\prod_k {p(\\mathcal{C}_k|x_n,\\theta)}^{y_{nk}} \n",
    "     $$\n",
    "\n",
    "     \n",
    "- Computing the gradient $\\nabla_{\\theta_k} \\mathrm{L}(\\theta)$ (NB: revised text) leads to (for proof, see next slide) \n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta_k} \\mathrm{L}(\\theta) = \\sum_n \\Big( \\underbrace{y_{nk}}_{\\text{target}} - \\underbrace{\\frac{e^{\\theta_k^T x_n}}{ \\sum_j e^{\\theta_j^T x_n}}}_{\\text{prediction}} \\Big)\\cdot x_n \n",
    "$$\n",
    "\n",
    "  \n",
    "- Compare this to the gradient for _linear_ regression:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathrm{L}(\\theta) =  \\sum_n \\left(y_n - \\theta^T x_n \\right)  x_n\n",
    "$$\n",
    "\n",
    "- In both cases\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathrm{L} =  \\sum_n \\left( \\text{target}_n - \\text{prediction}_n \\right) \\cdot \\text{input}_n \n",
    "$$\n",
    "\n",
    "- The parameter vector $\\theta$ for logistic regression can be estimated through iterative gradient-based adaptation. E.g. (with iteration index $i$),\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}^{(i+1)} =  \\hat{\\theta}^{(i)} + \\eta \\cdot \\left. \\nabla_\\theta   \\mathrm{L}(\\theta)  \\right|_{\\theta = \\hat{\\theta}^{(i)}}\n",
    "$$\n",
    "\n",
    " ###  2. <span style=\"color:red\">(OPTIONAL)</span> Proof of Derivative of Log-likelihood for  Discriminative Classification\n",
    "\n",
    "\n",
    "- The Log-likelihood is $\n",
    "    \\mathrm{L}(\\theta) = \\log \\prod_n \\prod_k {\\underbrace{p(\\mathcal{C}_k|x_n,\\theta)}_{p_{nk}}}^{y_{nk}} = \\sum_{n,k} y_{nk} \\log p_{nk}$\n",
    "\n",
    "     \n",
    "- Use the fact that the softmax $\\phi_k \\equiv e^{a_k} / {\\sum_j e^{a_j}}$ has analytical derivative:\n",
    "\n",
    "$$ \\begin{align*}\n",
    " \\frac{\\partial \\phi_k}{\\partial a_j} &= \\frac{(\\sum_j e^{a_j})e^{a_k}\\delta_{kj}-e^{a_j}e^{a_k}}{(\\sum_j e^{a_j})^2} = \\frac{e^{a_k}}{\\sum_j e^{a_j}}\\delta_{kj} - \\frac{e^{a_j}}{\\sum_j e^{a_j}} \\frac{e^{a_k}}{\\sum_j e^{a_j}}\\\\\n",
    "     &= \\phi_k \\cdot(\\delta_{kj}-\\phi_j)\n",
    " \\end{align*}$$\n",
    "\n",
    "<!---\n",
    "%    -  Again we try to minimize the cross-entropy ($\\sum_{nk} y_{nk} \\log \\frac{y_{nk}}{p_{nk}}$) between the data `targets' $t_{nk}$ and the model outputs $p_{nk}$.\n",
    "--->\n",
    "\n",
    " -  Take the derivative of $\\mathrm{L}(\\theta)$ (or: how to spend a hour ...)\n",
    "$$\\begin{align*} \n",
    "\\nabla_{\\theta_j} \\mathrm{L}(\\theta) &= \\sum_{n,k} \\frac{\\partial \\mathrm{L}_{nk}}{\\partial p_{nk}} \\cdot\\frac{\\partial p_{nk}}{\\partial a_{nj}}\\cdot\\frac{\\partial a_{nj}}{\\partial \\theta_j} \\\\\n",
    "  &= \\sum_{n,k} \\frac{y_{nk}}{p_{nk}} \\cdot p_{nk} (\\delta_{kj}-p_{nj}) \\cdot x_n \\\\\n",
    "  &= \\sum_n \\Big( y_{nj} (1-p_{nj}) -\\sum_{k\\neq j} y_{nk} p_{nj} \\Big) \\cdot x_n \\\\\n",
    "  &= \\sum_n \\left( y_{nj} - p_{nj} \\right)\\cdot x_n \\\\\n",
    "  &= \\sum_n \\Big( \\underbrace{y_{nj}}_{\\text{target}} - \\underbrace{\\frac{e^{\\theta_j^T x_n}}{\\sum_{j^\\prime} e^{\\theta_{j^\\prime}^T x_n}}}_{\\text{prediction}} \\Big)\\cdot x_n \n",
    "\\end{align*}$$\n",
    "  \n",
    "\n",
    "### 3. Application - Classify a new input\n",
    "\n",
    "-  Discriminative model-based prediction for a new input $x_\\bullet$ is easy, namely substitute the ML estimate in the model to get\n",
    "\n",
    "$$\n",
    "p(\\mathcal{C}_k |\\, x_\\bullet,\\hat\\theta) = \\frac{ \\mathrm{exp}\\left( \\hat \\theta_k^T x_\\bullet \\right) }{ \\sum_{k^\\prime} \\mathrm{exp}\\left(\\hat \\theta_{k^\\prime}^T x_\\bullet \\right)} \n",
    "  \\propto \\mathrm{exp}\\left(\\hat \\theta_k^T x_\\bullet\\right) \n",
    "$$\n",
    "\n",
    "-  The contours of equal probability (**discriminant boundaries**) are lines (hyperplanes) in feature space given by\n",
    "$$\n",
    "\\log \\frac{{p(\\mathcal{C}_k|x,\\hat \\theta )}}{{p(\\mathcal{C}_j|x,\\hat \\theta )}} = \\left( \\hat{\\theta}_{k} - \\hat{\\theta}_j\\right) ^T x = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "Let us perform ML estimation of $\\theta$ on the data set from the introduction. To allow an offset in the discrimination boundary, we add a constant 1 to the feature vector $x$. We only have to specify the (negative) log-likelihood and the gradient w.r.t. $\\theta$. Then, we use an off-the-shelf optimisation library to minimize the negative log-likelihood.\n",
    "\n",
    "We plot the resulting maximum likelihood discrimination boundary. For comparison we also plot the ML discrimination boundary obtained from the generative Gaussian classifier from lesson 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ArgumentError",
     "evalue": "ArgumentError: Package Optim not found in current path:\n- Run `import Pkg; Pkg.add(\"Optim\")` to install the Optim package.\n",
     "output_type": "error",
     "traceback": [
      "ArgumentError: Package Optim not found in current path:\n- Run `import Pkg; Pkg.add(\"Optim\")` to install the Optim package.\n",
      "",
      "Stacktrace:",
      " [1] require(::Module, ::Symbol) at ./loading.jl:876",
      " [2] top-level scope at In[4]:1"
     ]
    }
   ],
   "source": [
    "using Optim # Optimization library\n",
    "\n",
    "y_1 = zeros(length(y))# class 1 indicator vector\n",
    "y_1[findall(y)] .= 1\n",
    "X_ext = vcat(X, ones(1, length(y))) # Extend X with a row of ones to allow an offset in the discrimination boundary\n",
    "\n",
    "# Implement negative log-likelihood function\n",
    "function negative_log_likelihood(θ::Vector)\n",
    "    # Return negative log-likelihood: -L(θ)\n",
    "    p_1 = 1.0 ./ (1.0 .+ exp.(-X_ext' * θ))   # P(C1|X,θ)\n",
    "    return -sum(log.( (y_1 .* p_1) + ((1 .- y_1).*(1 .- p_1))) ) # negative log-likelihood\n",
    "end\n",
    "\n",
    "# Use Optim.jl optimiser to minimize the negative log-likelihood function w.r.t. θ\n",
    "results = optimize(negative_log_likelihood, zeros(3), LBFGS())\n",
    "θ = results.minimizer\n",
    "\n",
    "# Plot the data set and ML discrimination boundary\n",
    "plotDataSet()\n",
    "p_1(x) = 1.0 ./ (1.0 .+ exp(-([x;1.]' * θ)))\n",
    "boundary(x1) = -1 ./ θ[2] * (θ[1]*x1 .+ θ[3])\n",
    "plot([-2.;10.], boundary([-2.; 10.]), \"k-\");\n",
    "# # Also fit the generative Gaussian model from lesson 7 and plot the resulting discrimination boundary for comparison\n",
    "generative_boundary = buildGenerativeDiscriminationBoundary(X, y)\n",
    "plot([-2.;10.], generative_boundary([-2;10]), \"k:\");\n",
    "legend([L\"y=0\";L\"y=1\";L\"y=?\";\"Discr. boundary\";\"Gen. boundary\"], loc=3);\n",
    "\n",
    "Given $\\hat{\\theta}$, we can classify a new input $x_\\bullet = [3.75, 1.0]^T$:\n",
    "\n",
    "x_test = [3.75;1.0]\n",
    "println(\"P(C1|x•,θ) = $(p_1(x_test))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The generative model gives a bad result because the feature distribution of one class is clearly non-Gaussian: the model does not fit the data well. \n",
    "\n",
    "- The discriminative approach does not suffer from this problem because it makes no assumptions about the feature distribition $p(x|y)$, it just estimates the conditional class distribution $p(y|x)$ directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap Classification\n",
    "\n",
    "<table>\n",
    "<tr> <td></td><td style=\"text-align:center\"><b>Generative</b></td> <td style=\"text-align:center\"><b>Discriminative</b></td> </tr> \n",
    "\n",
    "<tr> <td>1</td><td>Like <b>density estimation</b>, model joint prob.\n",
    "$$p(\\mathcal{C}_k) p(x|\\mathcal{C}_k) = \\pi_k \\mathcal{N}(\\mu_k,\\Sigma)$$</td> <td>Like (linear) <b>regression</b>, model conditional\n",
    "$$p(\\mathcal{C}_k|x,\\theta)$$</td> </tr>\n",
    "\n",
    "<tr> <td>2</td><td>Leads to <b>softmax</b> posterior class probability\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) = e^{\\theta_k^T x}/Z$$\n",
    "with <b>structured</b> $\\theta$</td> <td> <b>Choose</b> also softmax posterior class probability\n",
    "$$ p(\\mathcal{C}_k|x,\\theta ) = e^{\\theta_k^T x}/Z$$\n",
    "but now with 'free' $\\theta$</td> </tr>\n",
    "\n",
    "<tr> <td>3</td><td>For Gaussian $p(x|\\mathcal{C}_k)$ and multinomial priors,\n",
    "$$\\hat \\theta_k  = \\left[ {\\begin{array}{c}\n",
    "   { - \\frac{1}{2} \\mu_k^T \\sigma^{-1} \\mu_k  + \\log \\pi_k}  \\\\\n",
    "   {\\sigma^{-1} \\mu_k }  \\\\\n",
    "\\end{array}} \\right]$$\n",
    "<b>in one shot</b>.</td> <td>Find $\\hat\\theta_k$ through gradient-based adaptation\n",
    "$$\\nabla_{\\theta_k}\\mathrm{L}(\\theta) = \\sum_n \\Big( y_{nk} - \\frac{e^{\\theta_k^T x_n}}{\\sum_{k^\\prime} e^{\\theta_{k^\\prime}^T x_n}} \\Big)\\, x_n$$ </td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "footer.hidden-print {\n",
       "    display: none !important;\n",
       "}\n",
       "    \n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", read(f,String))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
