{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering with Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preliminaries\n",
    "\n",
    "- Goal \n",
    "  - Introduction to the Expectation-Maximization (EM) Algorithm with application to Gaussian Mixture Models (GMM)\n",
    "- Materials        \n",
    "  - Mandatory\n",
    "    - These lecture notes\n",
    "  - Optional\n",
    "    - Bishop pp. 430-439 for Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Limitations of Simple IID Gaussian Models\n",
    "\n",
    "Sofar, model inference was solved analytically, but we used strong assumptions:\n",
    "\n",
    "- IID sampling, $p(D) = \\prod_n p(x_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Simple Gaussian (or multinomial) PDFs, $p(x_n) = \\mathcal{N}(x_n|\\mu,\\Sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Some limitations of Simple Gaussian Models with IID Sampling:\n",
    "  - What if the PDF is **multi-modal** (or is just not Gaussian in any other way)?\n",
    "  - Covariance matrix $\\Sigma$ has $D(D+1)/2$ parameters.\n",
    "    - This quickly becomes **a very large number** for increasing dimension $D$.\n",
    "  - Temporal signals are often **not IID**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Towards More Flexible Models\n",
    "\n",
    "-  What if the PDF is multi-modal (or is just not Gaussian in any other way)?\n",
    "  -   **Discrete latent** variable models (a.k.a. **mixture** models). We'll cover this case in this lesson.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  \n",
    "-  Covariance matrix $\\Sigma$ has $D(D+1)/2$ parameters. This quickly becomes very large for increasing dimension $D$.\n",
    "  -  **Continuous latent** variable models (a.k.a. **dimensionality reduction** models). Covered in [lesson 11](http://nbviewer.ipython.org/github/bertdv/AIP-5SSB0/blob/master/lessons/notebooks/11_Continuous-Latent-Variable-Models-PCA-and-FA.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  Temporal signals are often not IID.\n",
    "  -  Introduce **Markov dependencies** and **latent state** variable models. This will be covered in [lesson 13](http://nbviewer.jupyter.org/github/bertdv/AIP-5SSB0/blob/master/lessons/notebooks/13_Dynamic-Latent-Variable-Models.ipynb).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Illustrative Example\n",
    "\n",
    "- You're now asked to build a density model for a data set ([Old Faithful](https://en.wikipedia.org/wiki/Old_Faithful), Bishop pg. 681) that clearly is not distributed as a single Gaussian:\n",
    "\n",
    "<img src=\"./figures/fig-Bishop-A5-Old-Faithfull.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Unobserved Classes\n",
    "\n",
    "Consider again a set of observed data $D=\\{x_1,\\dotsc,x_N\\}$\n",
    "\n",
    "- This time we suspect that there are unobserved class labels that would help explain (or predict) the data, e.g.,\n",
    "  - the observed data are the color of living things; the unobserved classes are animals and plants.\n",
    "  - observed are wheel sizes; unobserved categories are trucks and personal cars.\n",
    "  - observed is an audio signal; unobserved classes include speech, music, traffic noise, etc.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   \n",
    "- Classification problems with unobserved classes are called **Clustering** problems. The learning algorithm needs to **discover the classes from the observed data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  The Gaussian Mixture Model (GMM)\n",
    " \n",
    "- If the categories were observed as well, these data could be nicely modeled by the previously discussed [generative classification framework](http://nbviewer.jupyter.org/github/bertdv/AIP-5SSB0/blob/master/lessons/07_generative_classification/Generative-Classification.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let us use **equivalent model assumptions to linear generative classification**: \n",
    "$$\\begin{align*} p(x_n,\\mathcal{C}_k) &=  \\overbrace{p(\\mathcal{C}_k)}^{\\text{class prior}} \\, \\overbrace{p(x_n|\\mathcal{C}_k)}^{\\text{likelihood}}  \n",
    "= \\pi_k \\,\\mathcal{N}\\left(x_n|\\mu_k,\\Sigma_k \\right) \\end{align*}$$\n",
    "where, as previously, we use notational shorthand $\\mathcal{C}_k \\triangleq (z_{nk}=1)$ and a _hidden_ $1$-of-$K$ selector variable $z_{nk}$ with each observation $x_n$ as \n",
    "$$\n",
    "z_{nk} = \\begin{cases} 1 & \\text{if  } \\, z_n \\in \\mathcal{C}_k\\\\\n",
    "0 & \\text{otherwise} \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Since the class labels are not observed, the probability for observations in a GMM is obtained through _marginalization_ over the classes (this is different in classification problems, where the classes are observed):\n",
    "$$\\begin{align*}\n",
    "p(x_n) = \\boxed{\\sum_k \\pi_k \\mathcal{N}\\left(x_n|\\mu_k,\\Sigma_k \\right)}\n",
    "\\end{align*}$$\n",
    "  - The class priors $p(\\mathcal{C}_k) =\\pi_k$ are often called _mixture coefficients_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Gaussian Mixture Models\n",
    "\n",
    "- GMMs are very popular models. They have decent computational properties and are **universal approximators of densities** (as long as there are enough Gaussians of course)\n",
    "\n",
    "<img src=\"./figures/fig-ZoubinG-GMM-universal-approximation.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Inference: Log-Likelihood for GMM\n",
    "\n",
    "- The log-likelihood for observed data $D=\\{x_1,\\dotsc,x_N\\}$,\n",
    "$$\\begin{align*}\n",
    "\\log p(D|\\theta) &\\stackrel{\\text{IID}}{=} \\sum_n \\log p(x_n|\\theta)\n",
    "  = \\sum_n \\log \\left( \\sum_{k=1}^K p(\\mathcal{C}_k) \\, p(x_n|\\mathcal{C}_k)  \\right) \\\\\n",
    "  &= \\sum_n \\log \\left( \\sum_k \\pi_k\\mathcal{N}(x_n|\\mu_k,\\Sigma_k) \\right)\n",
    "\\end{align*}$$\n",
    "... and now the log-of-sum cannot be further simplified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Compare this to the log-likelihood for $D=\\{(x_1,y_1),\\dotsc,(x_N,y_N)\\}$ in [(generative) classification](http://nbviewer.ipython.org/github/bertdv/AIP-5SSB0/blob/master/lessons/notebooks/07_Generative-Classification.ipynb): \n",
    "$$\\begin{align*}\n",
    "\\log p(D|\\theta) &= \\sum_n \\log \\prod_k p(x_n,\\mathcal{C}_{k}|\\theta)^{y_{nk}} \n",
    " =  \\sum_{n,k} y_{nk} \\log p(x_n,\\mathcal{C}_{k}|\\theta) \\\\\n",
    "   &=  \\sum_k m_k \\log \\pi_k + \\sum_{n,k} y_{nk} \\log \\mathcal{N}(x_n|\\mu_k,\\Sigma)\n",
    "\\end{align*}$$\n",
    "which led to easy Gaussian and multinomial parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fortunately GMMs can be trained by maximum likelihood using an efficient algorithm: Expectation-Maximization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Introducing a Soft Class Indicator\n",
    "\n",
    "Let's introduce a $1$-of-$K$ selector variable $z_{nk}$ to represent the _unobserved_ classes $\\mathcal{C}_k$ by \n",
    "$$\n",
    "z_{nk} = \\begin{cases} 1 & \\text{if $z_n$ in class $\\mathcal{C}_k$}\\\\\n",
    "        0 & \\text{otherwise} \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We don't _observe_ the class labels $z_{nk}$, but we can compute the posterior probability for the class labels, given the observation $x_n$:\n",
    "$$\\begin{align*}\n",
    "p(\\mathcal{C}_k | x_n ) &= \\frac{p(x_n|\\mathcal{C}_k)p(\\mathcal{C}_k)}{\\sum_{k^\\prime} p(x_n|\\mathcal{C}_{k^\\prime})p(\\mathcal{C}_{k^\\prime})} \n",
    "  = \\frac{\\pi_k \\mathcal{N}(x_n | \\mu_k,\\Sigma_k)}{\\sum_{k^\\prime} \\pi_{k^\\prime} \\mathcal{N}(x_n | \\mu_{k^\\prime},\\Sigma_{k^\\prime})}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The $\\gamma_{nk} \\triangleq p(\\mathcal{C}_k | x_n ) = p(z_{nk}=1 | x_n )$ are also called **responsibilities**. Note that $0 \\leq \\gamma_{nk} \\leq 1$ by definition and that they can be evaluated (i.e., we can compute it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The responsibilities $\\gamma_{nk}$ are soft class indicators and they play the same role in clustering as class selection variables ($y_{nk}$) do in classification problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  ML estimation for Clustering: The Expectation-Maximization (EM) Algorithm Idea \n",
    "\n",
    "- <span class=\"emphasis\"><b>IDEA</b>: Let's apply the (generative) classification formulas and substitute the reponsibilities $\\gamma_{nk}$ wherever the formulas use the binary class indicators $y_{nk}$</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  Try parameter updates (same as for [generative Gaussian classification](http://nbviewer.ipython.org/github/bertdv/AIP-5SSB0/blob/master/lessons/notebooks/07_Generative-Classification.ipynb)):\n",
    "$$\\begin{align*}\n",
    "\\hat \\pi_k &= \\frac{m_k}{N}\\,,\\quad \\text{where }m_k = \\sum_n \\gamma_{nk} \\\\\n",
    "\\hat \\mu_k &= \\frac{1}{m_k} \\sum_n \\gamma_{nk} x_n \\\\\n",
    "\\hat \\Sigma_k  &= \\frac{1}{m_k} \\sum_{n} \\gamma_{nk} (x_n-\\hat \\mu_k)(x_n-\\hat \\mu_k)^T\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  But wait ..., the responsibilities $\\gamma_{nk}=\\frac{\\pi_k \\mathcal{N}(x_n|\\mu_k,\\Sigma_k)}{\\sum_j \\pi_j \\mathcal{N}(x_n|\\mu_j,\\Sigma_j)}$ are a function of the model parameters $\\{\\pi,\\mu,\\Sigma\\}$ and the parameter updates depend on the responsibilities ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "-  **Solution**: Iterate updating between $\\gamma_{nk}$ and the parameters $\\{\\pi,\\mu,\\Sigma\\}$. This procedure is called the **Expectation-Maximization (EM)** algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### CODE EXAMPLE\n",
    "\n",
    "We'll perform clustering on the data set from the illustrative example by fitting a GMM consisting of two Gaussians using the EM algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mPyError (ccall(@pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, arg, kw)) <type 'exceptions.ValueError'>\nValueError(u'c of shape (271,) not acceptable as a color sequence for x with size 271, y with size 271',)\n  File \"/Users/bert/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/matplotlib/pyplot.py\", line 3378, in scatter\n    edgecolors=edgecolors, data=data, **kwargs)\n  File \"/Users/bert/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/matplotlib/__init__.py\", line 1717, in inner\n    return func(ax, *args, **kwargs)\n  File \"/Users/bert/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/matplotlib/axes/_axes.py\", line 3988, in scatter\n    raise ValueError(msg.format(c.shape, x.size, y.size))\n\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mPyError (ccall(@pysym(:PyObject_Call), PyPtr, (PyPtr, PyPtr, PyPtr), o, arg, kw)) <type 'exceptions.ValueError'>\nValueError(u'c of shape (271,) not acceptable as a color sequence for x with size 271, y with size 271',)\n  File \"/Users/bert/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/matplotlib/pyplot.py\", line 3378, in scatter\n    edgecolors=edgecolors, data=data, **kwargs)\n  File \"/Users/bert/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/matplotlib/__init__.py\", line 1717, in inner\n    return func(ax, *args, **kwargs)\n  File \"/Users/bert/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/matplotlib/axes/_axes.py\", line 3988, in scatter\n    raise ValueError(msg.format(c.shape, x.size, y.size))\n\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mpyerr_check\u001b[22m\u001b[22m at \u001b[1m/Users/bert/.julia/v0.6/PyCall/src/exception.jl:56\u001b[22m\u001b[22m [inlined]",
      " [2] \u001b[1mpyerr_check\u001b[22m\u001b[22m at \u001b[1m/Users/bert/.julia/v0.6/PyCall/src/exception.jl:61\u001b[22m\u001b[22m [inlined]",
      " [3] \u001b[1mmacro expansion\u001b[22m\u001b[22m at \u001b[1m/Users/bert/.julia/v0.6/PyCall/src/exception.jl:81\u001b[22m\u001b[22m [inlined]",
      " [4] \u001b[1m#_pycall#67\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::PyCall.PyObject, ::RowVector{Float64,Array{Float64,1}}, ::Vararg{RowVector{Float64,Array{Float64,1}},N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/bert/.julia/v0.6/PyCall/src/PyCall.jl:658\u001b[22m\u001b[22m",
      " [5] \u001b[1m(::PyCall.#kw##_pycall)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::PyCall.#_pycall, ::PyCall.PyObject, ::RowVector{Float64,Array{Float64,1}}, ::Vararg{RowVector{Float64,Array{Float64,1}},N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [6] \u001b[1m#pycall#71\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::PyCall.PyObject, ::Type{PyCall.PyAny}, ::RowVector{Float64,Array{Float64,1}}, ::Vararg{RowVector{Float64,Array{Float64,1}},N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/bert/.julia/v0.6/PyCall/src/PyCall.jl:675\u001b[22m\u001b[22m",
      " [7] \u001b[1m(::PyCall.#kw##pycall)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::PyCall.#pycall, ::PyCall.PyObject, ::Type{PyCall.PyAny}, ::RowVector{Float64,Array{Float64,1}}, ::Vararg{RowVector{Float64,Array{Float64,1}},N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [8] \u001b[1m#scatter#99\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::Function, ::RowVector{Float64,Array{Float64,1}}, ::Vararg{RowVector{Float64,Array{Float64,1}},N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/bert/.julia/v0.6/PyPlot/src/PyPlot.jl:172\u001b[22m\u001b[22m",
      " [9] \u001b[1m(::PyPlot.#kw##scatter)\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Any,1}, ::PyPlot.#scatter, ::RowVector{Float64,Array{Float64,1}}, ::Vararg{RowVector{Float64,Array{Float64,1}},N} where N\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./<missing>:0\u001b[22m\u001b[22m",
      " [10] \u001b[1mplotGMM\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}, ::Array{Distributions.MvNormal{Float64,PDMats.PDMat{Float64,Array{Float64,2}},Array{Float64,1}},1}, ::Array{Float64,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m/Users/bert/github/bertdv/AIP-5SSB0/lessons/notebooks/scripts/gmm_plot.jl:30\u001b[22m\u001b[22m",
      " [11] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "using DataFrames\n",
    "include(\"scripts/gmm_plot.jl\") # Holds plotting function \n",
    "old_faithful = readtable(\"datasets/old_faithful.csv\")\n",
    "X = convert(Matrix{Float64}, [old_faithful[1] old_faithful[2]]')\n",
    "N = size(X, 2)\n",
    "\n",
    "# Initialize the GMM. We assume 2 clusters.\n",
    "clusters = [MvNormal([4.;60.], diagm([.5;10^2])); \n",
    "            MvNormal([2.;80.], diagm([.5;10^2]))]\n",
    "π_hat = [0.5; 0.5]                    # Mixing weights\n",
    "γ = fill!(Matrix{Float64}(2,N), NaN)  # Responsibilities (row per cluster)\n",
    "\n",
    "# Define functions for updating the parameters and responsibilities\n",
    "function updateResponsibilities!(X, clusters, π_hat, γ)\n",
    "    # Expectation step: update γ\n",
    "    norm = [pdf(clusters[1], X) pdf(clusters[2], X)] * π_hat\n",
    "    γ[1,:] = (π_hat[1] * pdf(clusters[1],X) ./ norm)'\n",
    "    γ[2,:] = 1 - γ[1,:]\n",
    "end\n",
    "function updateParameters!(X, clusters, π_hat, γ)\n",
    "    # Maximization step: update π_hat and clusters using ML estimation\n",
    "    m = sum(γ, 2)\n",
    "    π_hat = m / N\n",
    "    μ_hat = (X * γ') ./ m'\n",
    "    for k=1:2\n",
    "        Z = (X .- μ_hat[:,k])\n",
    "        Σ_k = Hermitian(((Z .* (γ[k,:])') * Z') / m[k])\n",
    "        clusters[k] = MvNormal(μ_hat[:,k], convert(Matrix, Σ_k))\n",
    "    end\n",
    "end\n",
    "\n",
    "# Execute the algorithm: iteratively update parameters and responsibilities\n",
    "subplot(2,3,1); plotGMM(X, clusters, γ); title(\"Initial situation\")\n",
    "updateResponsibilities!(X, clusters, π_hat, γ)\n",
    "subplot(2,3,2); plotGMM(X, clusters, γ); title(\"After first E-step\")\n",
    "updateParameters!(X, clusters, π_hat, γ)\n",
    "subplot(2,3,3); plotGMM(X, clusters, γ); title(\"After first M-step\")\n",
    "iter_counter = 1\n",
    "for i=1:3\n",
    "    for j=1:i+1\n",
    "        updateResponsibilities!(X, clusters, π_hat, γ)\n",
    "        updateParameters!(X, clusters, π_hat, γ)\n",
    "        iter_counter += 1\n",
    "    end\n",
    "    subplot(2,3,3+i); plotGMM(X, clusters, γ); \n",
    "    title(\"After $(iter_counter) iterations\")\n",
    "end\n",
    "PyPlot.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that you can step through the interactive demo yourself by running [this script](https://github.com/bertdv/AIP-5SSB0/blob/master/lessons/notebooks/scripts/interactive_em_demo.jl) in julia. You can run a script in julia by    \n",
    "`julia> include(\"path/to/script-name.jl\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering vs. (Generative) Classification\n",
    "\n",
    "<table>\n",
    "<tr> <td></td><td style=\"text-align:center\"> <b>Classification</b></td> <td style=\"text-align:center\"><b>Clustering</b></td> </tr> \n",
    "\n",
    "<tr> <td>1</td><td>Class label $y_n$ is observed</td> <td>Class label $z_n$ is latent</td> </tr>\n",
    "\n",
    "<tr> <td>2</td><td>log-likelihood <b>conditions</b> on observed class<br />$$\\propto \\sum_{nk} y_{nk} \\log \\mathcal{N}(x_n|\\mu_k,\\Sigma_k)$$</td> <td> log-likelihood <b>marginalizes</b> over latent classes<br />$$\\propto \\sum_{n}\\log \\sum_k \\pi_k \\mathcal{N}(x_n|\\mu_k,\\Sigma_k)$$</td> </tr>\n",
    "\n",
    "<tr> <td>3</td><td>'Hard' class selector<br />$$\n",
    "y_{nk} = \\begin{cases} 1 & \\text{if $y_n$ in class $\\mathcal{C}_k$}\\\\\n",
    "        0 & \\text{otherwise} \\end{cases}\n",
    "$$</td> <td>'Soft' class responsibility<br />$$\\gamma_{nk} = p(\\mathcal{C}_k|x_n)$$</td> </tr>\n",
    "\n",
    "<tr> <td>4</td>\n",
    "<td>Estimation:<BR /> \n",
    "$$\\begin{align*}\n",
    "\\hat{\\pi}_k &= \\frac{1}{N}\\sum_n y_{nk} \\\\\n",
    "\\hat{\\mu}_k &= \\frac{\\sum_n y_{nk} x_n}{\\sum_n y_{nk}} \\\\\n",
    "\\hat{\\Sigma}_k &= \\frac{\\sum_n y_{nk} (x_n-\\hat\\mu_k)(x_n-\\hat\\mu_k)^T}{\\sum_n y_{nk}}\n",
    "\\end{align*}$$\n",
    "</td> \n",
    "<td>Estimation (1 update of M-step!)<BR />\n",
    "$$\\begin{align*}\n",
    "\\hat{\\pi}_k &= \\frac{1}{N}\\sum_n \\gamma_{nk} \\\\\n",
    "\\hat{\\mu}_k &= \\frac{\\sum_n \\gamma_{nk} x_n}{\\sum_n \\gamma_{nk}} \\\\\n",
    "\\hat{\\Sigma}_k &= \\frac{\\sum_n \\gamma_{nk} (x_n-\\hat\\mu_k)(x_n-\\hat\\mu_k)^T}{\\sum_n \\gamma_{nk}}\n",
    "\\end{align*}$$\n",
    "</td>\n",
    "</tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---\n",
    "The cell below loads the style file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!--\n",
       "This HTML file contains custom styles and some javascript.\n",
       "Include it a Jupyter notebook for improved rendering.\n",
       "-->\n",
       "\n",
       "<!-- Fonts -->\n",
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Nixie+One' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<!-- Custom style -->\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       "#notebook_panel { /* main background */\n",
       "    background: rgb(245,245,245);\n",
       "}\n",
       "\n",
       "div.container {\n",
       "    min-width: 960px;\n",
       "}\n",
       "\n",
       "div #notebook { /* centre the content */\n",
       "    background: #fff; /* white background for content */\n",
       "    margin: auto;\n",
       "    padding-left: 0em;\n",
       "}\n",
       "\n",
       "#notebook li { /* More space between bullet points */\n",
       "    margin-top:0.8em;\n",
       "}\n",
       "\n",
       "/* draw border around running cells */\n",
       "div.cell.border-box-sizing.code_cell.running {\n",
       "    border: 1px solid #111;\n",
       "}\n",
       "\n",
       "/* Put a solid color box around each cell and its output, visually linking them*/\n",
       "div.cell.code_cell {\n",
       "    background-color: rgb(256,256,256);\n",
       "    border-radius: 0px;\n",
       "    padding: 0.5em;\n",
       "    margin-left:1em;\n",
       "    margin-top: 1em;\n",
       "}\n",
       "\n",
       "div.text_cell_render{\n",
       "    font-family: 'Alegreya Sans' sans-serif;\n",
       "    line-height: 140%;\n",
       "    font-size: 125%;\n",
       "    font-weight: 400;\n",
       "    width:800px;\n",
       "    margin-left:auto;\n",
       "    margin-right:auto;\n",
       "}\n",
       "\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-style:regular;\n",
       "    font-weight: 400;\n",
       "    font-size: 45pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.5em;\n",
       "    margin-top: 0.5em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 30pt;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0,51,102);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.3em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Nixie One', serif;\n",
       "    margin-top:16px;\n",
       "    font-size: 22pt;\n",
       "    font-weight: 600;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: regular;\n",
       "    color: rgb(102,102,0);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {    /*Use this for captions*/\n",
       "    font-family: 'Nixie One', serif;\n",
       "    font-size: 14pt;\n",
       "    text-align: center;\n",
       "    margin-top: 0em;\n",
       "    margin-bottom: 2em;\n",
       "    font-style: regular;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {  /*Use this for small titles*/\n",
       "    font-family: 'Nixie One', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 16pt;\n",
       "    color: rgb(163,0,0);\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.8em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 { /*use this for copyright note*/\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 9pt;\n",
       "    line-height: 100%;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".CodeMirror{\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 90%;\n",
       "}\n",
       "\n",
       ".boxed { /* draw a border around a piece of text */\n",
       "  border: 1px solid blue ;\n",
       "}\n",
       "\n",
       "h4#CODE-EXAMPLE,\n",
       "h4#END-OF-CODE-EXAMPLE {\n",
       "    margin: 10px 0;\n",
       "    padding: 10px;\n",
       "    background-color: #d0f9ca !important;\n",
       "    border-top: #849f81 1px solid;\n",
       "    border-bottom: #849f81 1px solid;\n",
       "}\n",
       "\n",
       ".emphasis {\n",
       "    color: red;\n",
       "}\n",
       "\n",
       ".exercise {\n",
       "    color: green;\n",
       "}\n",
       "\n",
       ".proof {\n",
       "    color: blue;\n",
       "}\n",
       "\n",
       "code {\n",
       "  padding: 2px 4px !important;\n",
       "  font-size: 90% !important;\n",
       "  color: #222 !important;\n",
       "  background-color: #efefef !important;\n",
       "  border-radius: 2px !important;\n",
       "}\n",
       "\n",
       "/* This removes the actual style cells from the notebooks, but no in print mode\n",
       "   as they will be removed through some other method */\n",
       "@media not print {\n",
       "  .cell:nth-last-child(-n+2) {\n",
       "    display: none;\n",
       "  }\n",
       "}\n",
       "\n",
       "</style>\n",
       "\n",
       "<!-- MathJax styling -->\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"],\n",
       "                           equationNumbers: { autoNumber: \"AMS\", useLabelIds: true}\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "open(\"../../styles/aipstyle.html\") do f\n",
    "    display(\"text/html\", readstring(f))\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.6.1",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
